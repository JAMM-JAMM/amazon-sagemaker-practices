{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import psycopg2\n",
    "import base64\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import sagemaker\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'redshift-deepar-nyctaxi-demo-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-988889742134/redshift-deepar-nyctaxi-demo-notebook/output\n"
     ]
    }
   ],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "print(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting...\n",
      "Connection info retrieved from Secrets manager\n",
      "connected...running query...\n",
      "Query execution complete\n"
     ]
    }
   ],
   "source": [
    "redshift_iam_role = 'arn:aws:iam::413094830157:role/deepar-demo-RedshiftRole-2R1VM0KJVXMX' #replace the IAM role with yours\n",
    "#uses session manager name to return connection and credential information\n",
    "def connection_info(db):\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager'\n",
    "    )\n",
    "\n",
    "    get_secret_value_response = client.get_secret_value(SecretId=db)\n",
    "\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        secret = json.loads(get_secret_value_response['SecretString'])\n",
    "    else:\n",
    "        secret = json.loads(base64.b64decode(get_secret_value_response['SecretBinary']))\n",
    "        \n",
    "    return secret\n",
    "\n",
    "\n",
    "#creates a connection to the cluster\n",
    "def get_connection(db,db_creds):\n",
    "\n",
    "    con_params = connection_info(db_creds)\n",
    "    print(\"Connection info retrieved from Secrets manager\")\n",
    "    rs_conn=psycopg2.connect(dbname=db, host=con_params['host'], port=con_params['port'], user=con_params['username'], password=con_params['password'])\n",
    "    cur = rs_conn.cursor()\n",
    "    cur.execute(\"set statement_timeout = 1200000\")\n",
    "    return cur\n",
    "\n",
    "#Close the connection to the cluster\n",
    "def close_cursor(cursor):\n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "#submits a query to the cluster\n",
    "def run_command(cursor, statement):\n",
    "    res = cursor.execute(statement)\n",
    "    return res\n",
    "\n",
    "#db = args['db']\n",
    "db='nyctaxi'\n",
    "db_creds = 'nyctaxisecret'\n",
    "\n",
    "\n",
    "#get database connection\n",
    "print('connecting...')\n",
    "#con = rs_common.get_connection(db,db_creds)\n",
    "cursor = get_connection(db,db_creds)\n",
    "\n",
    "redshift_unload_path = s3_output_path + '/unload/'\n",
    "\n",
    "#run each sql statement\n",
    "print(\"connected...running query...\")\n",
    "results = []\n",
    "query_str = \"unload('select coalesce(v1.pickup_timestamp_norm, v2.pickup_timestamp_norm) as pickup_timestamp_norm \\\n",
    ", coalesce(v1.vendor_1, 0) as vendor_1 \\\n",
    ", coalesce(v2.vendor_2, 0) as vendor_2 \\\n",
    "from \\\n",
    "(select case when extract(minute from lpep_dropoff_datetime) between 0 and 14 then dateadd(minute, 0, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 15 and 29 then dateadd(minute, 15, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 30 and 44 then dateadd(minute, 30, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 45 and 59 then dateadd(minute, 45, date_trunc(''hour'', lpep_dropoff_datetime)) end as pickup_timestamp_norm \\\n",
    ", count(1) as vendor_1 \\\n",
    "from taxischema.nyc_greentaxi \\\n",
    "where vendorid = 1 group by 1) v1 \\\n",
    "full outer join \\\n",
    "(select case when extract(minute from lpep_dropoff_datetime) between 0 and 14 then dateadd(minute, 0, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 15 and 29 then dateadd(minute, 15, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 30 and 44 then dateadd(minute, 30, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 45 and 59 then dateadd(minute, 45, date_trunc(''hour'', lpep_dropoff_datetime)) end as pickup_timestamp_norm \\\n",
    ", count(1)  as vendor_2 \\\n",
    "from taxischema.nyc_greentaxi \\\n",
    "where vendorid = 2 group by 1) v2 on v1.pickup_timestamp_norm = v2.pickup_timestamp_norm order by pickup_timestamp_norm ;') to '\" \\\n",
    "+ redshift_unload_path + \"' iam_role '\" + redshift_iam_role + \"' format as CSV header ALLOWOVERWRITE GZIP\"\n",
    "\n",
    "result = run_command(cursor, query_str)\n",
    "\n",
    "print(\"Query execution complete\")\n",
    "\n",
    "close_cursor(cursor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Contents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d50556365c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_df_from_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredshift_unload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-d50556365c9c>\u001b[0m in \u001b[0;36mload_df_from_s3\u001b[0;34m(s3_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdatafiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_objects_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Contents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprefix_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3fs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS3FileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Contents'"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "redshift_unload_path = s3_output_path + '/unload/'\n",
    "\n",
    "def load_df_from_s3(s3_path):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    prefix = '/'.join(split[3:])\n",
    "\n",
    "       \n",
    "    datafiles = s3.list_objects_v2(Bucket=bucket, Prefix = prefix)['Contents']\n",
    "    prefix_df = []\n",
    "    fs = s3fs.S3FileSystem()\n",
    "\n",
    "    for file in datafiles[0:]:\n",
    "        key = file['Key']\n",
    "        with fs.open('s3://'+ bucket + '/' + key) as f:\n",
    "            df = pd.read_csv(f, compression='gzip', index_col=0, parse_dates=True, decimal=',', sep=',')\n",
    "        \n",
    "        prefix_df.append(df)\n",
    "        print(\"File retrieved %s\" %key)\n",
    "        \n",
    "    return pd.concat(prefix_df)\n",
    "\n",
    "pd_df = load_df_from_s3(redshift_unload_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = pd_df.shape[1]\n",
    "data_trip = pd_df.resample('2H').sum() / 8\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_trip.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 2), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 2):\n",
    "    timeseries[i].loc[\"2018-10-01\":\"2019-12-31\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")    \n",
    "    axx[i].set_ylabel(\"Ride count\")   \n",
    "    axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 2), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 2):\n",
    "    timeseries[i].loc[\"2019-01-01\":\"2019-08-01\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")    \n",
    "    axx[i].set_ylabel(\"Ride count\")   \n",
    "    axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = '2H'\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify here the portion of the data that is used for training: the model sees data from 2019-01-01 to 2019-04-01 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2019-01-01 00:00:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2019-04-01 00:00:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training][:-1].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we will consider time series extending beyond the training range: these will be used for computing test scores, by using the trained model to forecast their trailing 7 days, and comparing predictions with actual values.\n",
    "To evaluate our model performance on more than one week, we generate test data that extends to 1, 2, 3, 4 weeks beyond the training range. This way we perform *rolling evaluation* of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "idx =  []\n",
    "print(len(pd.date_range(start_dataset, end_training )))\n",
    "period_range = len(pd.date_range(start_dataset, end_training))\n",
    "      \n",
    "for i in range(1, num_test_windows + 1) :\n",
    "    idx.append(pd.date_range(start_dataset, periods = period_range + i * prediction_length, freq=freq))\n",
    "\n",
    "test_data = [\n",
    "    {   \n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[idx[k]].tolist()\n",
    "    }\n",
    "    for k in range(0, num_test_windows)\n",
    "    for ts in timeseries\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.85 ms, sys: 187 µs, total: 3.04 ms\n",
      "Wall time: 2.86 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File s3://sagemaker-us-west-2-413094830157/s3://sagemaker-us-west-2-413094830157/redshift-deepar-nyctaxi-demo-notebook/data/train/train.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "File s3://sagemaker-us-west-2-413094830157/s3://sagemaker-us-west-2-413094830157/redshift-deepar-nyctaxi-demo-notebook/data/test/test.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "CPU times: user 25.5 ms, sys: 3.54 ms, total: 29 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2019-01-01 00:00:00\", \"target\": [42.625, 36.625, 17.5, 10.375, 6.875, 7.875, 15.5, 26.5, ...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='redshift-deepar-nyctaxi-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use Sagemaker Automated Model Tuning to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the test data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last prediction_length points of each time-series in the test set and comparing this to the actual value of the time-series.\n",
    "\n",
    "Note: the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-22 18:13:27 Starting - Starting the training job...\n",
      "2020-06-22 18:13:29 Starting - Launching requested ML instances......\n",
      "2020-06-22 18:14:37 Starting - Preparing the instances for training...\n",
      "2020-06-22 18:15:20 Downloading - Downloading input data......\n",
      "2020-06-22 18:16:20 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'84', u'epochs': u'400', u'time_freq': u'2H', u'context_length': u'84', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'84', u'time_freq': u'2H', u'context_length': u'84', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Real time series\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] number of time series: 2\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] number of observations: 2160\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] mean target length: 1080\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] min/mean/max target: 1.25/78.727025463/352.125\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] mean abs(target): 78.727025463\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Small number of time series. Doing 320 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Real time series\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] number of time series: 20\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] number of observations: 30860\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] mean target length: 1543\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] min/mean/max target: 0.875/75.2997043098/352.125\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] mean abs(target): 75.2997043098\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] nvidia-smi took: 0.0252161026001 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:23 INFO 140052404180800] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 654.9229621887207, \"sum\": 654.9229621887207, \"min\": 654.9229621887207}}, \"EndTime\": 1592849784.516191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849783.860403}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:24 INFO 140052404180800] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 1535.2609157562256, \"sum\": 1535.2609157562256, \"min\": 1535.2609157562256}}, \"EndTime\": 1592849785.395795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849784.516272}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:26 INFO 140052404180800] Epoch[0] Batch[0] avg_epoch_loss=5.458694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=5.45869445801\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:27 INFO 140052404180800] Epoch[0] Batch[5] avg_epoch_loss=5.210703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=5.21070305506\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:27 INFO 140052404180800] Epoch[0] Batch [5]#011Speed: 314.68 samples/sec#011loss=5.210703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] Epoch[0] Batch[10] avg_epoch_loss=5.129491\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=5.03203630447\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] Epoch[0] Batch [10]#011Speed: 247.59 samples/sec#011loss=5.032036\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 3039.355993270874, \"sum\": 3039.355993270874, \"min\": 3039.355993270874}}, \"EndTime\": 1592849788.435356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849785.395884}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=218.458062753 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=0, train loss <loss>=5.1294908957\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:28 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_4ed53d0f-81b2-45b8-98c7-6038d092ca81-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 102.37503051757812, \"sum\": 102.37503051757812, \"min\": 102.37503051757812}}, \"EndTime\": 1592849788.538348, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849788.435448}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:29 INFO 140052404180800] Epoch[1] Batch[0] avg_epoch_loss=5.141357\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=5.14135694504\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] Epoch[1] Batch[5] avg_epoch_loss=4.842142\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=4.84214218458\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] Epoch[1] Batch [5]#011Speed: 339.92 samples/sec#011loss=4.842142\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] Epoch[1] Batch[10] avg_epoch_loss=4.811948\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=4.7757142067\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] Epoch[1] Batch [10]#011Speed: 326.41 samples/sec#011loss=4.775714\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2458.446979522705, \"sum\": 2458.446979522705, \"min\": 2458.446979522705}}, \"EndTime\": 1592849790.996937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849788.538428}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.924756682 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=1, train loss <loss>=4.81194764918\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:30 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:31 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_f0d50d94-f61e-4b8e-bed4-326dea9d1dcf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 72.27587699890137, \"sum\": 72.27587699890137, \"min\": 72.27587699890137}}, \"EndTime\": 1592849791.069786, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849790.997008}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:31 INFO 140052404180800] Epoch[2] Batch[0] avg_epoch_loss=4.597450\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=4.59744977951\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:32 INFO 140052404180800] Epoch[2] Batch[5] avg_epoch_loss=4.598525\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=4.59852464994\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:32 INFO 140052404180800] Epoch[2] Batch [5]#011Speed: 337.58 samples/sec#011loss=4.598525\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:33 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2197.728157043457, \"sum\": 2197.728157043457, \"min\": 2197.728157043457}}, \"EndTime\": 1592849793.267651, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849791.069858}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:33 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.008987201 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:33 INFO 140052404180800] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=2, train loss <loss>=4.51850700378\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:33 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:33 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_771659a1-d98e-4271-aa56-788e6c090f2b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 82.03887939453125, \"sum\": 82.03887939453125, \"min\": 82.03887939453125}}, \"EndTime\": 1592849793.350295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849793.267732}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:33 INFO 140052404180800] Epoch[3] Batch[0] avg_epoch_loss=4.464411\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=4.4644112587\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:34 INFO 140052404180800] Epoch[3] Batch[5] avg_epoch_loss=4.433608\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=4.43360797564\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:34 INFO 140052404180800] Epoch[3] Batch [5]#011Speed: 340.00 samples/sec#011loss=4.433608\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:35 INFO 140052404180800] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2179.4300079345703, \"sum\": 2179.4300079345703, \"min\": 2179.4300079345703}}, \"EndTime\": 1592849795.529864, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849793.350373}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:35 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=292.722040259 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:35 INFO 140052404180800] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=3, train loss <loss>=4.41016769409\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:35 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:35 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_ab79a65f-5ec3-4a8e-86e8-419995330ded-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 71.65312767028809, \"sum\": 71.65312767028809, \"min\": 71.65312767028809}}, \"EndTime\": 1592849795.602079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849795.529942}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:36 INFO 140052404180800] Epoch[4] Batch[0] avg_epoch_loss=4.335592\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.3355922699\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] Epoch[4] Batch[5] avg_epoch_loss=4.350752\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=4.35075171789\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] Epoch[4] Batch [5]#011Speed: 329.84 samples/sec#011loss=4.350752\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] Epoch[4] Batch[10] avg_epoch_loss=4.218913\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=4.06070561409\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] Epoch[4] Batch [10]#011Speed: 335.35 samples/sec#011loss=4.060706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2386.5370750427246, \"sum\": 2386.5370750427246, \"min\": 2386.5370750427246}}, \"EndTime\": 1592849797.988742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849795.60215}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.091512595 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=4, train loss <loss>=4.2189125798\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:37 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:38 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_b4749e4c-6829-46ba-8d71-66b246255797-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 71.7020034790039, \"sum\": 71.7020034790039, \"min\": 71.7020034790039}}, \"EndTime\": 1592849798.061032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849797.988819}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:38 INFO 140052404180800] Epoch[5] Batch[0] avg_epoch_loss=4.278634\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=4.27863359451\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:39 INFO 140052404180800] Epoch[5] Batch[5] avg_epoch_loss=4.203747\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=4.2037473917\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:39 INFO 140052404180800] Epoch[5] Batch [5]#011Speed: 335.75 samples/sec#011loss=4.203747\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] Epoch[5] Batch[10] avg_epoch_loss=4.256047\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=4.31880607605\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] Epoch[5] Batch [10]#011Speed: 335.77 samples/sec#011loss=4.318806\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2367.799997329712, \"sum\": 2367.799997329712, \"min\": 2367.799997329712}}, \"EndTime\": 1592849800.428968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849798.061106}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.547465976 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=5, train loss <loss>=4.25604679368\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] Epoch[6] Batch[0] avg_epoch_loss=4.182508\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=4.18250846863\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:41 INFO 140052404180800] Epoch[6] Batch[5] avg_epoch_loss=4.020746\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=4.02074611187\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:41 INFO 140052404180800] Epoch[6] Batch [5]#011Speed: 329.37 samples/sec#011loss=4.020746\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:42 INFO 140052404180800] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2181.205987930298, \"sum\": 2181.205987930298, \"min\": 2181.205987930298}}, \"EndTime\": 1592849802.61068, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849800.429043}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:42 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.560436224 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:42 INFO 140052404180800] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.91628088951\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:42 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:42 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_ed4abdb1-87de-44b3-9738-df7239432028-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 65.97304344177246, \"sum\": 65.97304344177246, \"min\": 65.97304344177246}}, \"EndTime\": 1592849802.677284, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849802.610766}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:43 INFO 140052404180800] Epoch[7] Batch[0] avg_epoch_loss=4.190830\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=4.19083023071\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:44 INFO 140052404180800] Epoch[7] Batch[5] avg_epoch_loss=4.169580\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.16957982381\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:44 INFO 140052404180800] Epoch[7] Batch [5]#011Speed: 335.88 samples/sec#011loss=4.169580\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:44 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2190.4208660125732, \"sum\": 2190.4208660125732, \"min\": 2190.4208660125732}}, \"EndTime\": 1592849804.867848, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849802.67736}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:44 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.971279895 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:44 INFO 140052404180800] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=7, train loss <loss>=4.14663190842\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:44 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:45 INFO 140052404180800] Epoch[8] Batch[0] avg_epoch_loss=4.186996\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=4.18699645996\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:46 INFO 140052404180800] Epoch[8] Batch[5] avg_epoch_loss=4.023741\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=4.02374144395\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:46 INFO 140052404180800] Epoch[8] Batch [5]#011Speed: 326.28 samples/sec#011loss=4.023741\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:47 INFO 140052404180800] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2210.318088531494, \"sum\": 2210.318088531494, \"min\": 2210.318088531494}}, \"EndTime\": 1592849807.078713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849804.867915}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:47 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.563129738 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:47 INFO 140052404180800] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=8, train loss <loss>=4.05617673397\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:47 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:47 INFO 140052404180800] Epoch[9] Batch[0] avg_epoch_loss=4.094200\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=4.09419965744\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:48 INFO 140052404180800] Epoch[9] Batch[5] avg_epoch_loss=4.052826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=4.05282608668\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:48 INFO 140052404180800] Epoch[9] Batch [5]#011Speed: 333.02 samples/sec#011loss=4.052826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] Epoch[9] Batch[10] avg_epoch_loss=3.994419\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=3.92432971001\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] Epoch[9] Batch [10]#011Speed: 332.99 samples/sec#011loss=3.924330\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2375.5860328674316, \"sum\": 2375.5860328674316, \"min\": 2375.5860328674316}}, \"EndTime\": 1592849809.454796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849807.078771}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.655188059 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.99441864274\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] Epoch[10] Batch[0] avg_epoch_loss=3.905169\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.905169487\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:50 INFO 140052404180800] Epoch[10] Batch[5] avg_epoch_loss=4.038042\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=4.03804151217\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:50 INFO 140052404180800] Epoch[10] Batch [5]#011Speed: 341.22 samples/sec#011loss=4.038042\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:51 INFO 140052404180800] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2209.512948989868, \"sum\": 2209.512948989868, \"min\": 2209.512948989868}}, \"EndTime\": 1592849811.66482, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849809.45487}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:51 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.11548292 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:51 INFO 140052404180800] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.9682806015\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:51 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:52 INFO 140052404180800] Epoch[11] Batch[0] avg_epoch_loss=3.782631\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=3.78263139725\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] Epoch[11] Batch[5] avg_epoch_loss=3.820955\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=3.82095472018\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] Epoch[11] Batch [5]#011Speed: 338.27 samples/sec#011loss=3.820955\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2214.2889499664307, \"sum\": 2214.2889499664307, \"min\": 2214.2889499664307}}, \"EndTime\": 1592849813.879638, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849811.664901}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.758243841 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=11, train loss <loss>=3.90361287594\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:53 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_e2f3c9c6-1c74-4be9-9d36-8fc029d6df30-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.27214431762695, \"sum\": 61.27214431762695, \"min\": 61.27214431762695}}, \"EndTime\": 1592849813.941527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849813.87972}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:54 INFO 140052404180800] Epoch[12] Batch[0] avg_epoch_loss=3.947787\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=3.9477865696\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:55 INFO 140052404180800] Epoch[12] Batch[5] avg_epoch_loss=3.926444\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=3.92644441128\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:55 INFO 140052404180800] Epoch[12] Batch [5]#011Speed: 338.71 samples/sec#011loss=3.926444\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] Epoch[12] Batch[10] avg_epoch_loss=3.872620\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=3.80803132057\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] Epoch[12] Batch [10]#011Speed: 332.20 samples/sec#011loss=3.808031\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2368.7238693237305, \"sum\": 2368.7238693237305, \"min\": 2368.7238693237305}}, \"EndTime\": 1592849816.310395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849813.941605}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.551724294 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=12, train loss <loss>=3.87262027914\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_d8d07fa8-10a8-420c-ae75-401d450d3dd1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.59603118896484, \"sum\": 66.59603118896484, \"min\": 66.59603118896484}}, \"EndTime\": 1592849816.377606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849816.310472}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] Epoch[13] Batch[0] avg_epoch_loss=3.951371\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.95137143135\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:57 INFO 140052404180800] Epoch[13] Batch[5] avg_epoch_loss=3.930278\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=3.93027818203\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:57 INFO 140052404180800] Epoch[13] Batch [5]#011Speed: 333.56 samples/sec#011loss=3.930278\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:58 INFO 140052404180800] Epoch[13] Batch[10] avg_epoch_loss=3.910695\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=3.8871948719\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:58 INFO 140052404180800] Epoch[13] Batch [10]#011Speed: 333.94 samples/sec#011loss=3.887195\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:58 INFO 140052404180800] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2394.0799236297607, \"sum\": 2394.0799236297607, \"min\": 2394.0799236297607}}, \"EndTime\": 1592849818.771828, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849816.37768}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:58 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.184605767 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:58 INFO 140052404180800] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=13, train loss <loss>=3.91069485924\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:58 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:59 INFO 140052404180800] Epoch[14] Batch[0] avg_epoch_loss=3.952569\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:16:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.95256924629\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] Epoch[14] Batch[5] avg_epoch_loss=3.897550\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=3.8975499471\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] Epoch[14] Batch [5]#011Speed: 341.51 samples/sec#011loss=3.897550\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2156.501054763794, \"sum\": 2156.501054763794, \"min\": 2156.501054763794}}, \"EndTime\": 1592849820.928838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849818.771908}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=295.369602632 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.85745751858\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:00 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_6c6b436c-7b08-40ec-92a7-62980ea8a734-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 64.62383270263672, \"sum\": 64.62383270263672, \"min\": 64.62383270263672}}, \"EndTime\": 1592849820.994071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849820.92892}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:01 INFO 140052404180800] Epoch[15] Batch[0] avg_epoch_loss=3.997235\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.99723482132\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:02 INFO 140052404180800] Epoch[15] Batch[5] avg_epoch_loss=3.867290\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=3.86729025841\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:02 INFO 140052404180800] Epoch[15] Batch [5]#011Speed: 321.20 samples/sec#011loss=3.867290\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:03 INFO 140052404180800] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2231.388807296753, \"sum\": 2231.388807296753, \"min\": 2231.388807296753}}, \"EndTime\": 1592849823.225601, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849820.994146}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:03 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.461490383 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:03 INFO 140052404180800] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=15, train loss <loss>=3.91569888592\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:03 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:03 INFO 140052404180800] Epoch[16] Batch[0] avg_epoch_loss=3.777464\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=3.77746391296\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:04 INFO 140052404180800] Epoch[16] Batch[5] avg_epoch_loss=3.840251\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.84025120735\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:04 INFO 140052404180800] Epoch[16] Batch [5]#011Speed: 337.73 samples/sec#011loss=3.840251\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] Epoch[16] Batch[10] avg_epoch_loss=3.813963\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=3.78241615295\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] Epoch[16] Batch [10]#011Speed: 335.84 samples/sec#011loss=3.782416\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2400.6729125976562, \"sum\": 2400.6729125976562, \"min\": 2400.6729125976562}}, \"EndTime\": 1592849825.626805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849823.225682}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.990921063 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.81396254626\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:05 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_803e80cd-4fee-411f-b85e-4c976ed9d921-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.892892837524414, \"sum\": 59.892892837524414, \"min\": 59.892892837524414}}, \"EndTime\": 1592849825.68722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849825.62687}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:06 INFO 140052404180800] Epoch[17] Batch[0] avg_epoch_loss=3.949033\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=3.94903302193\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:07 INFO 140052404180800] Epoch[17] Batch[5] avg_epoch_loss=3.842369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=3.84236911933\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:07 INFO 140052404180800] Epoch[17] Batch [5]#011Speed: 334.09 samples/sec#011loss=3.842369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] Epoch[17] Batch[10] avg_epoch_loss=3.775387\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=3.69500851631\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] Epoch[17] Batch [10]#011Speed: 329.89 samples/sec#011loss=3.695009\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2365.838050842285, \"sum\": 2365.838050842285, \"min\": 2365.838050842285}}, \"EndTime\": 1592849828.053184, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849825.687289}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.1533985 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=17, train loss <loss>=3.77538702705\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_4e03781f-d720-4019-84eb-7952a00e5741-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.944007873535156, \"sum\": 61.944007873535156, \"min\": 61.944007873535156}}, \"EndTime\": 1592849828.115745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849828.053261}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] Epoch[18] Batch[0] avg_epoch_loss=3.862373\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.86237311363\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:09 INFO 140052404180800] Epoch[18] Batch[5] avg_epoch_loss=3.871734\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.8717337052\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:09 INFO 140052404180800] Epoch[18] Batch [5]#011Speed: 334.48 samples/sec#011loss=3.871734\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:10 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2169.710159301758, \"sum\": 2169.710159301758, \"min\": 2169.710159301758}}, \"EndTime\": 1592849830.28581, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849828.116048}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:10 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=293.572729246 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:10 INFO 140052404180800] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=18, train loss <loss>=3.85474712849\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:10 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:10 INFO 140052404180800] Epoch[19] Batch[0] avg_epoch_loss=3.987933\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.98793292046\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:11 INFO 140052404180800] Epoch[19] Batch[5] avg_epoch_loss=3.832134\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=3.83213416735\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:11 INFO 140052404180800] Epoch[19] Batch [5]#011Speed: 336.20 samples/sec#011loss=3.832134\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:12 INFO 140052404180800] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2199.333906173706, \"sum\": 2199.333906173706, \"min\": 2199.333906173706}}, \"EndTime\": 1592849832.485666, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849830.285888}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:12 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.07166746 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:12 INFO 140052404180800] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.86477873325\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:12 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:12 INFO 140052404180800] Epoch[20] Batch[0] avg_epoch_loss=3.835580\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=3.83558011055\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:13 INFO 140052404180800] Epoch[20] Batch[5] avg_epoch_loss=3.819264\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=3.81926417351\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:13 INFO 140052404180800] Epoch[20] Batch [5]#011Speed: 336.94 samples/sec#011loss=3.819264\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:14 INFO 140052404180800] Epoch[20] Batch[10] avg_epoch_loss=3.874778\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=3.94139561653\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:14 INFO 140052404180800] Epoch[20] Batch [10]#011Speed: 335.40 samples/sec#011loss=3.941396\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:14 INFO 140052404180800] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2384.0079307556152, \"sum\": 2384.0079307556152, \"min\": 2384.0079307556152}}, \"EndTime\": 1592849834.870177, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849832.485745}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:14 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.865346522 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:14 INFO 140052404180800] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=20, train loss <loss>=3.87477846579\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:14 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:15 INFO 140052404180800] Epoch[21] Batch[0] avg_epoch_loss=3.803502\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=3.80350160599\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:16 INFO 140052404180800] Epoch[21] Batch[5] avg_epoch_loss=3.806454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=3.80645422141\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:16 INFO 140052404180800] Epoch[21] Batch [5]#011Speed: 338.89 samples/sec#011loss=3.806454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] Epoch[21] Batch[10] avg_epoch_loss=3.862122\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=3.92892389297\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] Epoch[21] Batch [10]#011Speed: 331.80 samples/sec#011loss=3.928924\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2399.4219303131104, \"sum\": 2399.4219303131104, \"min\": 2399.4219303131104}}, \"EndTime\": 1592849837.270093, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849834.870252}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.722568046 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=21, train loss <loss>=3.86212225394\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] Epoch[22] Batch[0] avg_epoch_loss=3.796024\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=3.79602432251\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:18 INFO 140052404180800] Epoch[22] Batch[5] avg_epoch_loss=3.791821\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=3.79182096322\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:18 INFO 140052404180800] Epoch[22] Batch [5]#011Speed: 332.48 samples/sec#011loss=3.791821\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:19 INFO 140052404180800] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2213.7291431427, \"sum\": 2213.7291431427, \"min\": 2213.7291431427}}, \"EndTime\": 1592849839.484304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849837.270168}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:19 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.894832537 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:19 INFO 140052404180800] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.75565311909\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:19 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:19 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_105c0385-3e64-4838-b479-7a52f30d41d3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 97.92709350585938, \"sum\": 97.92709350585938, \"min\": 97.92709350585938}}, \"EndTime\": 1592849839.582924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849839.484376}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:20 INFO 140052404180800] Epoch[23] Batch[0] avg_epoch_loss=3.666286\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.66628551483\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] Epoch[23] Batch[5] avg_epoch_loss=3.760957\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=3.76095656554\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] Epoch[23] Batch [5]#011Speed: 335.17 samples/sec#011loss=3.760957\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2217.663049697876, \"sum\": 2217.663049697876, \"min\": 2217.663049697876}}, \"EndTime\": 1592849841.80073, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849839.583002}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.911165446 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=23, train loss <loss>=3.74479827881\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:21 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_f10ba547-7a5a-45d5-9a7d-7a56a9a365f2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 100.99101066589355, \"sum\": 100.99101066589355, \"min\": 100.99101066589355}}, \"EndTime\": 1592849841.902292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849841.800811}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:22 INFO 140052404180800] Epoch[24] Batch[0] avg_epoch_loss=3.606732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=3.60673165321\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:23 INFO 140052404180800] Epoch[24] Batch[5] avg_epoch_loss=3.816836\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.81683623791\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:23 INFO 140052404180800] Epoch[24] Batch [5]#011Speed: 335.02 samples/sec#011loss=3.816836\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] Epoch[24] Batch[10] avg_epoch_loss=3.887009\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=3.97121620178\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] Epoch[24] Batch [10]#011Speed: 330.54 samples/sec#011loss=3.971216\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2384.650945663452, \"sum\": 2384.650945663452, \"min\": 2384.650945663452}}, \"EndTime\": 1592849844.287084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849841.902371}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=268.788898725 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=24, train loss <loss>=3.88700894876\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] Epoch[25] Batch[0] avg_epoch_loss=3.821186\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=3.82118606567\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:25 INFO 140052404180800] Epoch[25] Batch[5] avg_epoch_loss=3.756389\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=3.75638858477\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:25 INFO 140052404180800] Epoch[25] Batch [5]#011Speed: 337.51 samples/sec#011loss=3.756389\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:26 INFO 140052404180800] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2186.29789352417, \"sum\": 2186.29789352417, \"min\": 2186.29789352417}}, \"EndTime\": 1592849846.473936, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849844.287167}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:26 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.972403435 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:26 INFO 140052404180800] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=25, train loss <loss>=3.75815851688\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:26 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:26 INFO 140052404180800] Epoch[26] Batch[0] avg_epoch_loss=3.762023\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=3.76202344894\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:27 INFO 140052404180800] Epoch[26] Batch[5] avg_epoch_loss=3.759788\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=3.75978823503\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:27 INFO 140052404180800] Epoch[26] Batch [5]#011Speed: 334.59 samples/sec#011loss=3.759788\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:28 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2218.9278602600098, \"sum\": 2218.9278602600098, \"min\": 2218.9278602600098}}, \"EndTime\": 1592849848.693455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849846.474016}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:28 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.059632603 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:28 INFO 140052404180800] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=26, train loss <loss>=3.77103703022\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:28 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:29 INFO 140052404180800] Epoch[27] Batch[0] avg_epoch_loss=3.836967\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.83696699142\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:30 INFO 140052404180800] Epoch[27] Batch[5] avg_epoch_loss=3.775058\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=3.77505795161\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:30 INFO 140052404180800] Epoch[27] Batch [5]#011Speed: 335.60 samples/sec#011loss=3.775058\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:30 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2197.9610919952393, \"sum\": 2197.9610919952393, \"min\": 2197.9610919952393}}, \"EndTime\": 1592849850.892001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849848.693547}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:30 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.798392631 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:30 INFO 140052404180800] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=27, train loss <loss>=3.77793574333\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:30 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:31 INFO 140052404180800] Epoch[28] Batch[0] avg_epoch_loss=3.688894\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=3.68889427185\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:32 INFO 140052404180800] Epoch[28] Batch[5] avg_epoch_loss=3.800075\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=3.80007521311\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:32 INFO 140052404180800] Epoch[28] Batch [5]#011Speed: 335.15 samples/sec#011loss=3.800075\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:33 INFO 140052404180800] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2203.5601139068604, \"sum\": 2203.5601139068604, \"min\": 2203.5601139068604}}, \"EndTime\": 1592849853.096224, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849850.892083}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:33 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.431760162 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:33 INFO 140052404180800] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=28, train loss <loss>=3.80537788868\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:33 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:33 INFO 140052404180800] Epoch[29] Batch[0] avg_epoch_loss=3.997508\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.99750804901\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:34 INFO 140052404180800] Epoch[29] Batch[5] avg_epoch_loss=3.816893\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.81689298153\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:34 INFO 140052404180800] Epoch[29] Batch [5]#011Speed: 336.37 samples/sec#011loss=3.816893\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:35 INFO 140052404180800] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2223.2959270477295, \"sum\": 2223.2959270477295, \"min\": 2223.2959270477295}}, \"EndTime\": 1592849855.320058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849853.096305}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:35 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=265.810218928 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:35 INFO 140052404180800] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.73514127731\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:35 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:35 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_d0306e89-30ae-42c2-90a3-d562338880a4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.16604804992676, \"sum\": 61.16604804992676, \"min\": 61.16604804992676}}, \"EndTime\": 1592849855.381848, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849855.320119}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:35 INFO 140052404180800] Epoch[30] Batch[0] avg_epoch_loss=3.707536\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.70753622055\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:36 INFO 140052404180800] Epoch[30] Batch[5] avg_epoch_loss=3.794012\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=3.79401151339\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:36 INFO 140052404180800] Epoch[30] Batch [5]#011Speed: 332.58 samples/sec#011loss=3.794012\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] Epoch[30] Batch[10] avg_epoch_loss=3.722218\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=3.63606581688\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] Epoch[30] Batch [10]#011Speed: 333.83 samples/sec#011loss=3.636066\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2376.1088848114014, \"sum\": 2376.1088848114014, \"min\": 2376.1088848114014}}, \"EndTime\": 1592849857.758099, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849855.381921}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.222480674 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=30, train loss <loss>=3.72221801498\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:37 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_c84c2d8e-2e15-44ea-aabe-c5c46eb82e5a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.67613983154297, \"sum\": 66.67613983154297, \"min\": 66.67613983154297}}, \"EndTime\": 1592849857.825379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849857.758174}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:38 INFO 140052404180800] Epoch[31] Batch[0] avg_epoch_loss=3.748431\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=3.74843072891\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:39 INFO 140052404180800] Epoch[31] Batch[5] avg_epoch_loss=3.793691\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=3.79369099935\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:39 INFO 140052404180800] Epoch[31] Batch [5]#011Speed: 335.52 samples/sec#011loss=3.793691\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:39 INFO 140052404180800] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2167.5429344177246, \"sum\": 2167.5429344177246, \"min\": 2167.5429344177246}}, \"EndTime\": 1592849859.993076, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849857.825457}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:39 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.716932521 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:39 INFO 140052404180800] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=31, train loss <loss>=3.77003185749\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:39 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:40 INFO 140052404180800] Epoch[32] Batch[0] avg_epoch_loss=3.660171\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=3.66017055511\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:41 INFO 140052404180800] Epoch[32] Batch[5] avg_epoch_loss=3.717398\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=3.71739840508\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:41 INFO 140052404180800] Epoch[32] Batch [5]#011Speed: 337.82 samples/sec#011loss=3.717398\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] Epoch[32] Batch[10] avg_epoch_loss=3.828553\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=3.96193814278\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] Epoch[32] Batch [10]#011Speed: 325.68 samples/sec#011loss=3.961938\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2386.624813079834, \"sum\": 2386.624813079834, \"min\": 2386.624813079834}}, \"EndTime\": 1592849862.380234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849859.993135}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.824546283 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=32, train loss <loss>=3.8285528313\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] Epoch[33] Batch[0] avg_epoch_loss=3.883005\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=3.88300490379\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:43 INFO 140052404180800] Epoch[33] Batch[5] avg_epoch_loss=3.690977\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.69097661972\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:43 INFO 140052404180800] Epoch[33] Batch [5]#011Speed: 334.63 samples/sec#011loss=3.690977\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:44 INFO 140052404180800] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2209.968090057373, \"sum\": 2209.968090057373, \"min\": 2209.968090057373}}, \"EndTime\": 1592849864.590712, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849862.38031}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:44 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.266939163 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:44 INFO 140052404180800] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=33, train loss <loss>=3.752628088\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:44 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:45 INFO 140052404180800] Epoch[34] Batch[0] avg_epoch_loss=3.597410\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=3.59741020203\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:45 INFO 140052404180800] Epoch[34] Batch[5] avg_epoch_loss=3.729903\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=3.72990266482\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:45 INFO 140052404180800] Epoch[34] Batch [5]#011Speed: 339.78 samples/sec#011loss=3.729903\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:46 INFO 140052404180800] Epoch[34] Batch[10] avg_epoch_loss=3.818981\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=3.92587542534\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:46 INFO 140052404180800] Epoch[34] Batch [10]#011Speed: 331.82 samples/sec#011loss=3.925875\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:46 INFO 140052404180800] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2371.0341453552246, \"sum\": 2371.0341453552246, \"min\": 2371.0341453552246}}, \"EndTime\": 1592849866.962297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849864.590794}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:46 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.237850169 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:46 INFO 140052404180800] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.81898119233\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:46 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:47 INFO 140052404180800] Epoch[35] Batch[0] avg_epoch_loss=3.793643\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=3.79364323616\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:48 INFO 140052404180800] Epoch[35] Batch[5] avg_epoch_loss=3.721471\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=3.72147055467\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:48 INFO 140052404180800] Epoch[35] Batch [5]#011Speed: 333.83 samples/sec#011loss=3.721471\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:49 INFO 140052404180800] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2188.1580352783203, \"sum\": 2188.1580352783203, \"min\": 2188.1580352783203}}, \"EndTime\": 1592849869.150949, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849866.962372}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:49 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=292.467415817 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:49 INFO 140052404180800] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=35, train loss <loss>=3.72959179878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:49 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:49 INFO 140052404180800] Epoch[36] Batch[0] avg_epoch_loss=3.638329\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=3.63832879066\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:50 INFO 140052404180800] Epoch[36] Batch[5] avg_epoch_loss=3.757454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=3.75745395819\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:50 INFO 140052404180800] Epoch[36] Batch [5]#011Speed: 333.14 samples/sec#011loss=3.757454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:51 INFO 140052404180800] Epoch[36] Batch[10] avg_epoch_loss=3.804122\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=3.86012296677\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:51 INFO 140052404180800] Epoch[36] Batch [10]#011Speed: 332.33 samples/sec#011loss=3.860123\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:51 INFO 140052404180800] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2395.8630561828613, \"sum\": 2395.8630561828613, \"min\": 2395.8630561828613}}, \"EndTime\": 1592849871.547335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849869.151033}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:51 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.12405584 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:51 INFO 140052404180800] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=36, train loss <loss>=3.80412168936\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:51 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:52 INFO 140052404180800] Epoch[37] Batch[0] avg_epoch_loss=3.642731\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=3.64273118973\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] Epoch[37] Batch[5] avg_epoch_loss=3.629546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.62954624494\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] Epoch[37] Batch [5]#011Speed: 339.31 samples/sec#011loss=3.629546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2284.615993499756, \"sum\": 2284.615993499756, \"min\": 2284.615993499756}}, \"EndTime\": 1592849873.832422, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849871.547404}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.992392887 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=37, train loss <loss>=3.68520047665\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:53 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_accfb18c-dfd3-427a-8dff-063cf21953f3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 87.58902549743652, \"sum\": 87.58902549743652, \"min\": 87.58902549743652}}, \"EndTime\": 1592849873.920615, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849873.832505}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:54 INFO 140052404180800] Epoch[38] Batch[0] avg_epoch_loss=3.714223\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=3.7142226696\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:55 INFO 140052404180800] Epoch[38] Batch[5] avg_epoch_loss=3.730520\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=3.73051953316\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:55 INFO 140052404180800] Epoch[38] Batch [5]#011Speed: 338.74 samples/sec#011loss=3.730520\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:56 INFO 140052404180800] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2172.5330352783203, \"sum\": 2172.5330352783203, \"min\": 2172.5330352783203}}, \"EndTime\": 1592849876.093279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849873.920684}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:56 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.381419334 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:56 INFO 140052404180800] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=38, train loss <loss>=3.69108982086\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:56 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:56 INFO 140052404180800] Epoch[39] Batch[0] avg_epoch_loss=3.808115\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=3.80811524391\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:57 INFO 140052404180800] Epoch[39] Batch[5] avg_epoch_loss=3.750938\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=3.75093789895\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:57 INFO 140052404180800] Epoch[39] Batch [5]#011Speed: 334.83 samples/sec#011loss=3.750938\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:58 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2198.681116104126, \"sum\": 2198.681116104126, \"min\": 2198.681116104126}}, \"EndTime\": 1592849878.292562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849876.093363}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:58 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.700487636 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:58 INFO 140052404180800] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.75112082958\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:58 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:58 INFO 140052404180800] Epoch[40] Batch[0] avg_epoch_loss=3.594758\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=3.59475803375\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:59 INFO 140052404180800] Epoch[40] Batch[5] avg_epoch_loss=3.729575\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=3.72957452138\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:17:59 INFO 140052404180800] Epoch[40] Batch [5]#011Speed: 336.14 samples/sec#011loss=3.729575\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:00 INFO 140052404180800] Epoch[40] Batch[10] avg_epoch_loss=3.719430\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=3.7072573185\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:00 INFO 140052404180800] Epoch[40] Batch [10]#011Speed: 338.12 samples/sec#011loss=3.707257\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:00 INFO 140052404180800] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2413.6481285095215, \"sum\": 2413.6481285095215, \"min\": 2413.6481285095215}}, \"EndTime\": 1592849880.706764, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849878.292645}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:00 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.261018552 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:00 INFO 140052404180800] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=40, train loss <loss>=3.71943033825\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:00 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:01 INFO 140052404180800] Epoch[41] Batch[0] avg_epoch_loss=3.677632\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=3.67763209343\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:02 INFO 140052404180800] Epoch[41] Batch[5] avg_epoch_loss=3.766984\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=3.76698350906\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:02 INFO 140052404180800] Epoch[41] Batch [5]#011Speed: 328.36 samples/sec#011loss=3.766984\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:02 INFO 140052404180800] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2213.5438919067383, \"sum\": 2213.5438919067383, \"min\": 2213.5438919067383}}, \"EndTime\": 1592849882.920803, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849880.70684}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:02 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.662059838 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:02 INFO 140052404180800] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=41, train loss <loss>=3.74474902153\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:02 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:03 INFO 140052404180800] Epoch[42] Batch[0] avg_epoch_loss=3.624941\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=3.62494087219\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:04 INFO 140052404180800] Epoch[42] Batch[5] avg_epoch_loss=3.739335\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=3.73933454355\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:04 INFO 140052404180800] Epoch[42] Batch [5]#011Speed: 338.05 samples/sec#011loss=3.739335\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] Epoch[42] Batch[10] avg_epoch_loss=3.641183\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=3.52340111732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] Epoch[42] Batch [10]#011Speed: 339.00 samples/sec#011loss=3.523401\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2391.335964202881, \"sum\": 2391.335964202881, \"min\": 2391.335964202881}}, \"EndTime\": 1592849885.312713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849882.920884}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=268.45638389 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=42, train loss <loss>=3.64118298617\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_37c43d01-9396-4919-8a31-382da2cdea80-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 68.03417205810547, \"sum\": 68.03417205810547, \"min\": 68.03417205810547}}, \"EndTime\": 1592849885.381294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849885.312789}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] Epoch[43] Batch[0] avg_epoch_loss=3.943561\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=3.94356107712\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:06 INFO 140052404180800] Epoch[43] Batch[5] avg_epoch_loss=3.722609\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=3.72260856628\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:06 INFO 140052404180800] Epoch[43] Batch [5]#011Speed: 339.61 samples/sec#011loss=3.722609\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:07 INFO 140052404180800] Epoch[43] Batch[10] avg_epoch_loss=3.681834\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=3.63290343285\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:07 INFO 140052404180800] Epoch[43] Batch [10]#011Speed: 325.55 samples/sec#011loss=3.632903\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:07 INFO 140052404180800] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2386.406898498535, \"sum\": 2386.406898498535, \"min\": 2386.406898498535}}, \"EndTime\": 1592849887.767813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849885.381345}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:07 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.486571241 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:07 INFO 140052404180800] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=43, train loss <loss>=3.68183350563\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:07 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:08 INFO 140052404180800] Epoch[44] Batch[0] avg_epoch_loss=3.828907\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=3.82890701294\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:09 INFO 140052404180800] Epoch[44] Batch[5] avg_epoch_loss=3.744423\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=3.74442287286\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:09 INFO 140052404180800] Epoch[44] Batch [5]#011Speed: 339.76 samples/sec#011loss=3.744423\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:09 INFO 140052404180800] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2205.6260108947754, \"sum\": 2205.6260108947754, \"min\": 2205.6260108947754}}, \"EndTime\": 1592849889.973931, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849887.767889}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:09 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.537806613 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:09 INFO 140052404180800] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=44, train loss <loss>=3.74789507389\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:09 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:10 INFO 140052404180800] Epoch[45] Batch[0] avg_epoch_loss=3.717712\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=3.71771168709\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:11 INFO 140052404180800] Epoch[45] Batch[5] avg_epoch_loss=3.709667\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=3.70966684818\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:11 INFO 140052404180800] Epoch[45] Batch [5]#011Speed: 341.13 samples/sec#011loss=3.709667\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:12 INFO 140052404180800] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2168.100118637085, \"sum\": 2168.100118637085, \"min\": 2168.100118637085}}, \"EndTime\": 1592849892.142559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849889.974011}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:12 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=295.172863342 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:12 INFO 140052404180800] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=45, train loss <loss>=3.70278794765\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:12 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:12 INFO 140052404180800] Epoch[46] Batch[0] avg_epoch_loss=3.595768\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=3.59576797485\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:13 INFO 140052404180800] Epoch[46] Batch[5] avg_epoch_loss=3.659978\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=3.65997815132\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:13 INFO 140052404180800] Epoch[46] Batch [5]#011Speed: 334.95 samples/sec#011loss=3.659978\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:14 INFO 140052404180800] Epoch[46] Batch[10] avg_epoch_loss=3.743089\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=3.84282140732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:14 INFO 140052404180800] Epoch[46] Batch [10]#011Speed: 337.51 samples/sec#011loss=3.842821\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:14 INFO 140052404180800] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2397.6359367370605, \"sum\": 2397.6359367370605, \"min\": 2397.6359367370605}}, \"EndTime\": 1592849894.540734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849892.142643}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:14 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.172994737 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:14 INFO 140052404180800] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=46, train loss <loss>=3.74308872223\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:14 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:15 INFO 140052404180800] Epoch[47] Batch[0] avg_epoch_loss=3.531893\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=3.53189325333\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:15 INFO 140052404180800] Epoch[47] Batch[5] avg_epoch_loss=3.649532\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=3.6495316426\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:15 INFO 140052404180800] Epoch[47] Batch [5]#011Speed: 340.20 samples/sec#011loss=3.649532\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:16 INFO 140052404180800] Epoch[47] Batch[10] avg_epoch_loss=3.729127\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=3.82464089394\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:16 INFO 140052404180800] Epoch[47] Batch [10]#011Speed: 338.69 samples/sec#011loss=3.824641\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:16 INFO 140052404180800] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2354.302167892456, \"sum\": 2354.302167892456, \"min\": 2354.302167892456}}, \"EndTime\": 1592849896.89554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849894.540811}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:16 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.679538882 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:16 INFO 140052404180800] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=47, train loss <loss>=3.72912675684\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:16 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:17 INFO 140052404180800] Epoch[48] Batch[0] avg_epoch_loss=3.565064\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=3.56506419182\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:18 INFO 140052404180800] Epoch[48] Batch[5] avg_epoch_loss=3.660626\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=3.66062629223\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:18 INFO 140052404180800] Epoch[48] Batch [5]#011Speed: 339.90 samples/sec#011loss=3.660626\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] Epoch[48] Batch[10] avg_epoch_loss=3.664281\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=3.66866731644\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] Epoch[48] Batch [10]#011Speed: 337.21 samples/sec#011loss=3.668667\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2408.153772354126, \"sum\": 2408.153772354126, \"min\": 2408.153772354126}}, \"EndTime\": 1592849899.304188, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849896.895616}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.86963023 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=48, train loss <loss>=3.66428130323\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] Epoch[49] Batch[0] avg_epoch_loss=3.707729\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=3.7077293396\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:20 INFO 140052404180800] Epoch[49] Batch[5] avg_epoch_loss=3.722792\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=3.7227922678\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:20 INFO 140052404180800] Epoch[49] Batch [5]#011Speed: 336.89 samples/sec#011loss=3.722792\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] Epoch[49] Batch[10] avg_epoch_loss=3.623396\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=3.50412082672\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] Epoch[49] Batch [10]#011Speed: 335.26 samples/sec#011loss=3.504121\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2367.2990798950195, \"sum\": 2367.2990798950195, \"min\": 2367.2990798950195}}, \"EndTime\": 1592849901.67197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849899.304263}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.759680754 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=49, train loss <loss>=3.62339615822\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:21 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_c7878372-d128-45c0-ba00-3f739aaab233-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 71.46286964416504, \"sum\": 71.46286964416504, \"min\": 71.46286964416504}}, \"EndTime\": 1592849901.743983, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849901.672048}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:22 INFO 140052404180800] Epoch[50] Batch[0] avg_epoch_loss=3.549064\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=3.54906439781\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:23 INFO 140052404180800] Epoch[50] Batch[5] avg_epoch_loss=3.743212\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=3.74321230253\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:23 INFO 140052404180800] Epoch[50] Batch [5]#011Speed: 334.03 samples/sec#011loss=3.743212\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] Epoch[50] Batch[10] avg_epoch_loss=3.674998\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=3.59314031601\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] Epoch[50] Batch [10]#011Speed: 330.21 samples/sec#011loss=3.593140\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2422.0781326293945, \"sum\": 2422.0781326293945, \"min\": 2422.0781326293945}}, \"EndTime\": 1592849904.1662, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849901.744057}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.654781494 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=50, train loss <loss>=3.6749977632\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] Epoch[51] Batch[0] avg_epoch_loss=3.502822\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=3.50282239914\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:25 INFO 140052404180800] Epoch[51] Batch[5] avg_epoch_loss=3.689203\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=3.68920254707\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:25 INFO 140052404180800] Epoch[51] Batch [5]#011Speed: 329.35 samples/sec#011loss=3.689203\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:26 INFO 140052404180800] Epoch[51] Batch[10] avg_epoch_loss=3.662733\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=3.63096938133\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:26 INFO 140052404180800] Epoch[51] Batch [10]#011Speed: 335.96 samples/sec#011loss=3.630969\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:26 INFO 140052404180800] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2419.3031787872314, \"sum\": 2419.3031787872314, \"min\": 2419.3031787872314}}, \"EndTime\": 1592849906.586037, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849904.166277}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:26 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.098196056 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:26 INFO 140052404180800] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=51, train loss <loss>=3.66273292628\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:26 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:27 INFO 140052404180800] Epoch[52] Batch[0] avg_epoch_loss=3.747684\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=3.74768352509\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:27 INFO 140052404180800] Epoch[52] Batch[5] avg_epoch_loss=3.675562\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=3.67556246122\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:27 INFO 140052404180800] Epoch[52] Batch [5]#011Speed: 331.95 samples/sec#011loss=3.675562\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:28 INFO 140052404180800] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2185.0218772888184, \"sum\": 2185.0218772888184, \"min\": 2185.0218772888184}}, \"EndTime\": 1592849908.771626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849906.586112}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:28 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.027184061 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:28 INFO 140052404180800] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=52, train loss <loss>=3.693100667\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:28 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:29 INFO 140052404180800] Epoch[53] Batch[0] avg_epoch_loss=3.624333\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=3.62433314323\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:30 INFO 140052404180800] Epoch[53] Batch[5] avg_epoch_loss=3.628533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=3.62853272756\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:30 INFO 140052404180800] Epoch[53] Batch [5]#011Speed: 336.93 samples/sec#011loss=3.628533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] Epoch[53] Batch[10] avg_epoch_loss=3.583125\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=3.52863631248\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] Epoch[53] Batch [10]#011Speed: 333.67 samples/sec#011loss=3.528636\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2358.8428497314453, \"sum\": 2358.8428497314453, \"min\": 2358.8428497314453}}, \"EndTime\": 1592849911.131038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849908.771681}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.579271367 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=53, train loss <loss>=3.58312526616\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_6e51a862-cd2a-418b-87f4-12a843fb308b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 62.1950626373291, \"sum\": 62.1950626373291, \"min\": 62.1950626373291}}, \"EndTime\": 1592849911.19378, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849911.131105}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] Epoch[54] Batch[0] avg_epoch_loss=3.792016\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=3.79201579094\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:32 INFO 140052404180800] Epoch[54] Batch[5] avg_epoch_loss=3.665534\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=3.66553378105\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:32 INFO 140052404180800] Epoch[54] Batch [5]#011Speed: 328.67 samples/sec#011loss=3.665534\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:33 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2200.468063354492, \"sum\": 2200.468063354492, \"min\": 2200.468063354492}}, \"EndTime\": 1592849913.394391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849911.193863}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:33 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.472166313 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:33 INFO 140052404180800] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=54, train loss <loss>=3.68883979321\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:33 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:33 INFO 140052404180800] Epoch[55] Batch[0] avg_epoch_loss=3.749927\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=3.74992656708\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:34 INFO 140052404180800] Epoch[55] Batch[5] avg_epoch_loss=3.646651\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=3.6466507117\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:34 INFO 140052404180800] Epoch[55] Batch [5]#011Speed: 336.38 samples/sec#011loss=3.646651\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:35 INFO 140052404180800] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2167.011022567749, \"sum\": 2167.011022567749, \"min\": 2167.011022567749}}, \"EndTime\": 1592849915.561925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849913.394449}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:35 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=291.629655879 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:35 INFO 140052404180800] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=55, train loss <loss>=3.68058018684\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:35 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:36 INFO 140052404180800] Epoch[56] Batch[0] avg_epoch_loss=3.620582\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=3.62058210373\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] Epoch[56] Batch[5] avg_epoch_loss=3.591466\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=3.59146598975\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] Epoch[56] Batch [5]#011Speed: 330.15 samples/sec#011loss=3.591466\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2230.2680015563965, \"sum\": 2230.2680015563965, \"min\": 2230.2680015563965}}, \"EndTime\": 1592849917.792748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849915.562008}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.08208122 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=56, train loss <loss>=3.57510271072\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:37 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_d8686949-9e56-4e57-8173-e9085f108aca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 72.18194007873535, \"sum\": 72.18194007873535, \"min\": 72.18194007873535}}, \"EndTime\": 1592849917.865518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849917.79283}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:38 INFO 140052404180800] Epoch[57] Batch[0] avg_epoch_loss=3.579797\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=3.57979726791\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:39 INFO 140052404180800] Epoch[57] Batch[5] avg_epoch_loss=3.644460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=3.64445992311\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:39 INFO 140052404180800] Epoch[57] Batch [5]#011Speed: 333.45 samples/sec#011loss=3.644460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] Epoch[57] Batch[10] avg_epoch_loss=3.688921\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=3.74227356911\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] Epoch[57] Batch [10]#011Speed: 329.54 samples/sec#011loss=3.742274\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2383.6941719055176, \"sum\": 2383.6941719055176, \"min\": 2383.6941719055176}}, \"EndTime\": 1592849920.249351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849917.865589}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.125965351 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=57, train loss <loss>=3.68892067129\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] Epoch[58] Batch[0] avg_epoch_loss=3.445012\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=3.44501233101\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:41 INFO 140052404180800] Epoch[58] Batch[5] avg_epoch_loss=3.667571\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=3.66757102807\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:41 INFO 140052404180800] Epoch[58] Batch [5]#011Speed: 337.43 samples/sec#011loss=3.667571\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:42 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2185.0569248199463, \"sum\": 2185.0569248199463, \"min\": 2185.0569248199463}}, \"EndTime\": 1592849922.434968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849920.249429}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:42 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.475512806 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:42 INFO 140052404180800] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=58, train loss <loss>=3.61681010723\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:42 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:42 INFO 140052404180800] Epoch[59] Batch[0] avg_epoch_loss=3.580854\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=3.58085393906\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:43 INFO 140052404180800] Epoch[59] Batch[5] avg_epoch_loss=3.677078\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=3.67707836628\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:43 INFO 140052404180800] Epoch[59] Batch [5]#011Speed: 330.36 samples/sec#011loss=3.677078\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:44 INFO 140052404180800] Epoch[59] Batch[10] avg_epoch_loss=3.604902\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=3.5182911396\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:44 INFO 140052404180800] Epoch[59] Batch [10]#011Speed: 335.02 samples/sec#011loss=3.518291\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:44 INFO 140052404180800] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2413.567066192627, \"sum\": 2413.567066192627, \"min\": 2413.567066192627}}, \"EndTime\": 1592849924.849089, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849922.435053}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:44 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.855896955 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:44 INFO 140052404180800] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=59, train loss <loss>=3.60490235415\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:44 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:45 INFO 140052404180800] Epoch[60] Batch[0] avg_epoch_loss=3.569016\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=3.56901597977\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:46 INFO 140052404180800] Epoch[60] Batch[5] avg_epoch_loss=3.669552\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=3.66955161095\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:46 INFO 140052404180800] Epoch[60] Batch [5]#011Speed: 336.16 samples/sec#011loss=3.669552\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:47 INFO 140052404180800] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2173.651933670044, \"sum\": 2173.651933670044, \"min\": 2173.651933670044}}, \"EndTime\": 1592849927.023296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849924.849161}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:47 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.499436626 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:47 INFO 140052404180800] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=60, train loss <loss>=3.74140036106\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:47 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:47 INFO 140052404180800] Epoch[61] Batch[0] avg_epoch_loss=3.530441\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=3.53044104576\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:48 INFO 140052404180800] Epoch[61] Batch[5] avg_epoch_loss=3.646362\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=3.64636206627\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:48 INFO 140052404180800] Epoch[61] Batch [5]#011Speed: 325.37 samples/sec#011loss=3.646362\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:49 INFO 140052404180800] Epoch[61] Batch[10] avg_epoch_loss=3.591447\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=3.52554888725\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:49 INFO 140052404180800] Epoch[61] Batch [10]#011Speed: 333.07 samples/sec#011loss=3.525549\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:49 INFO 140052404180800] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2479.5119762420654, \"sum\": 2479.5119762420654, \"min\": 2479.5119762420654}}, \"EndTime\": 1592849929.503393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849927.023365}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:49 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.235122324 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:49 INFO 140052404180800] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=61, train loss <loss>=3.5914469849\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:49 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:50 INFO 140052404180800] Epoch[62] Batch[0] avg_epoch_loss=3.521700\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=3.52170014381\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:50 INFO 140052404180800] Epoch[62] Batch[5] avg_epoch_loss=3.603898\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=3.60389820735\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:50 INFO 140052404180800] Epoch[62] Batch [5]#011Speed: 333.04 samples/sec#011loss=3.603898\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:51 INFO 140052404180800] Epoch[62] Batch[10] avg_epoch_loss=3.615622\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=3.62969002724\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:51 INFO 140052404180800] Epoch[62] Batch [10]#011Speed: 331.71 samples/sec#011loss=3.629690\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:52 INFO 140052404180800] processed a total of 712 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2630.631923675537, \"sum\": 2630.631923675537, \"min\": 2630.631923675537}}, \"EndTime\": 1592849932.134528, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849929.503469}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.645063114 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=62, train loss <loss>=3.68263588349\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:52 INFO 140052404180800] Epoch[63] Batch[0] avg_epoch_loss=3.713997\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=3.71399688721\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:53 INFO 140052404180800] Epoch[63] Batch[5] avg_epoch_loss=3.623153\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=3.62315265338\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:53 INFO 140052404180800] Epoch[63] Batch [5]#011Speed: 330.80 samples/sec#011loss=3.623153\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:54 INFO 140052404180800] Epoch[63] Batch[10] avg_epoch_loss=3.734306\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=3.86769089699\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:54 INFO 140052404180800] Epoch[63] Batch [10]#011Speed: 320.03 samples/sec#011loss=3.867691\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:54 INFO 140052404180800] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2498.7728595733643, \"sum\": 2498.7728595733643, \"min\": 2498.7728595733643}}, \"EndTime\": 1592849934.633901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849932.13461}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:54 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=259.717378297 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:54 INFO 140052404180800] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=63, train loss <loss>=3.73430640047\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:54 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:55 INFO 140052404180800] Epoch[64] Batch[0] avg_epoch_loss=3.657276\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=3.65727567673\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:56 INFO 140052404180800] Epoch[64] Batch[5] avg_epoch_loss=3.654611\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=3.6546107928\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:56 INFO 140052404180800] Epoch[64] Batch [5]#011Speed: 334.38 samples/sec#011loss=3.654611\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:56 INFO 140052404180800] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2177.4890422821045, \"sum\": 2177.4890422821045, \"min\": 2177.4890422821045}}, \"EndTime\": 1592849936.811934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849934.633963}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:56 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.011760909 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:56 INFO 140052404180800] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=64, train loss <loss>=3.6905500412\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:56 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:57 INFO 140052404180800] Epoch[65] Batch[0] avg_epoch_loss=3.398194\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=3.39819431305\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:58 INFO 140052404180800] Epoch[65] Batch[5] avg_epoch_loss=3.580030\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=3.58003008366\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:58 INFO 140052404180800] Epoch[65] Batch [5]#011Speed: 317.14 samples/sec#011loss=3.580030\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] Epoch[65] Batch[10] avg_epoch_loss=3.560404\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=3.53685278893\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] Epoch[65] Batch [10]#011Speed: 328.05 samples/sec#011loss=3.536853\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2495.6889152526855, \"sum\": 2495.6889152526855, \"min\": 2495.6889152526855}}, \"EndTime\": 1592849939.308165, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849936.812018}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.250916597 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=65, train loss <loss>=3.5604040406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_f6523176-ca25-40a7-a4a9-cb8d764d12eb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 62.2708797454834, \"sum\": 62.2708797454834, \"min\": 62.2708797454834}}, \"EndTime\": 1592849939.371052, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849939.30822}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] Epoch[66] Batch[0] avg_epoch_loss=3.685569\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:18:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=3.68556857109\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:00 INFO 140052404180800] Epoch[66] Batch[5] avg_epoch_loss=3.697196\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=3.6971959273\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:00 INFO 140052404180800] Epoch[66] Batch [5]#011Speed: 333.01 samples/sec#011loss=3.697196\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:01 INFO 140052404180800] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2177.053928375244, \"sum\": 2177.053928375244, \"min\": 2177.053928375244}}, \"EndTime\": 1592849941.548248, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849939.37113}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:01 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.854195426 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:01 INFO 140052404180800] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=66, train loss <loss>=3.70901339054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:01 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:02 INFO 140052404180800] Epoch[67] Batch[0] avg_epoch_loss=3.492671\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=3.49267148972\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:02 INFO 140052404180800] Epoch[67] Batch[5] avg_epoch_loss=3.536038\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=3.53603796164\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:02 INFO 140052404180800] Epoch[67] Batch [5]#011Speed: 328.13 samples/sec#011loss=3.536038\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:03 INFO 140052404180800] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2221.954822540283, \"sum\": 2221.954822540283, \"min\": 2221.954822540283}}, \"EndTime\": 1592849943.770764, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849941.548331}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:03 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=268.669011727 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:03 INFO 140052404180800] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=67, train loss <loss>=3.50921933651\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:03 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:03 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_e72c1000-61e7-4de8-a81a-150252b8835a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.60004234313965, \"sum\": 60.60004234313965, \"min\": 60.60004234313965}}, \"EndTime\": 1592849943.831963, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849943.770834}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:04 INFO 140052404180800] Epoch[68] Batch[0] avg_epoch_loss=3.430515\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=3.43051505089\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:05 INFO 140052404180800] Epoch[68] Batch[5] avg_epoch_loss=3.635222\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=3.63522152106\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:05 INFO 140052404180800] Epoch[68] Batch [5]#011Speed: 337.10 samples/sec#011loss=3.635222\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:06 INFO 140052404180800] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2192.533016204834, \"sum\": 2192.533016204834, \"min\": 2192.533016204834}}, \"EndTime\": 1592849946.024625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849943.832034}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.762487614 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=68, train loss <loss>=3.6644197464\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:06 INFO 140052404180800] Epoch[69] Batch[0] avg_epoch_loss=3.669018\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=3.66901779175\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:07 INFO 140052404180800] Epoch[69] Batch[5] avg_epoch_loss=3.673607\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=3.67360655467\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:07 INFO 140052404180800] Epoch[69] Batch [5]#011Speed: 333.73 samples/sec#011loss=3.673607\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] Epoch[69] Batch[10] avg_epoch_loss=3.690538\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=3.71085495949\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] Epoch[69] Batch [10]#011Speed: 313.82 samples/sec#011loss=3.710855\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2418.401002883911, \"sum\": 2418.401002883911, \"min\": 2418.401002883911}}, \"EndTime\": 1592849948.443617, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849946.024708}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.06802223 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=69, train loss <loss>=3.69053764777\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] Epoch[70] Batch[0] avg_epoch_loss=3.637039\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=3.63703918457\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:09 INFO 140052404180800] Epoch[70] Batch[5] avg_epoch_loss=3.665421\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=3.6654206117\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:09 INFO 140052404180800] Epoch[70] Batch [5]#011Speed: 333.85 samples/sec#011loss=3.665421\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] Epoch[70] Batch[10] avg_epoch_loss=3.486631\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=3.27208428383\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] Epoch[70] Batch [10]#011Speed: 329.83 samples/sec#011loss=3.272084\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2392.4400806427, \"sum\": 2392.4400806427, \"min\": 2392.4400806427}}, \"EndTime\": 1592849950.836567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849948.443693}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.258483581 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=70, train loss <loss>=3.48663137176\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:10 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_52777107-bd94-4729-bd79-ccb7c4695912-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 68.50600242614746, \"sum\": 68.50600242614746, \"min\": 68.50600242614746}}, \"EndTime\": 1592849950.905612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849950.836643}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:11 INFO 140052404180800] Epoch[71] Batch[0] avg_epoch_loss=3.647835\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=3.64783501625\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:12 INFO 140052404180800] Epoch[71] Batch[5] avg_epoch_loss=3.603131\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=3.60313073794\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:12 INFO 140052404180800] Epoch[71] Batch [5]#011Speed: 338.15 samples/sec#011loss=3.603131\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] Epoch[71] Batch[10] avg_epoch_loss=3.558355\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=3.5046233654\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] Epoch[71] Batch [10]#011Speed: 325.51 samples/sec#011loss=3.504623\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2424.992084503174, \"sum\": 2424.992084503174, \"min\": 2424.992084503174}}, \"EndTime\": 1592849953.33074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849950.90569}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.802109295 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=71, train loss <loss>=3.55835465951\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] Epoch[72] Batch[0] avg_epoch_loss=3.418768\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=3.41876769066\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:14 INFO 140052404180800] Epoch[72] Batch[5] avg_epoch_loss=3.538973\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=3.5389734904\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:14 INFO 140052404180800] Epoch[72] Batch [5]#011Speed: 334.49 samples/sec#011loss=3.538973\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:15 INFO 140052404180800] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2198.960781097412, \"sum\": 2198.960781097412, \"min\": 2198.960781097412}}, \"EndTime\": 1592849955.530248, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849953.330821}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:15 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.936205861 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:15 INFO 140052404180800] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=72, train loss <loss>=3.5305753231\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:15 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:16 INFO 140052404180800] Epoch[73] Batch[0] avg_epoch_loss=3.577254\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=3.57725358009\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:16 INFO 140052404180800] Epoch[73] Batch[5] avg_epoch_loss=3.613637\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=3.61363697052\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:16 INFO 140052404180800] Epoch[73] Batch [5]#011Speed: 338.65 samples/sec#011loss=3.613637\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:17 INFO 140052404180800] Epoch[73] Batch[10] avg_epoch_loss=3.712430\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=3.83098168373\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:17 INFO 140052404180800] Epoch[73] Batch [10]#011Speed: 328.65 samples/sec#011loss=3.830982\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:17 INFO 140052404180800] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2391.4408683776855, \"sum\": 2391.4408683776855, \"min\": 2391.4408683776855}}, \"EndTime\": 1592849957.922217, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849955.530329}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:17 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.955111064 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:17 INFO 140052404180800] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=73, train loss <loss>=3.71243002198\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:17 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:18 INFO 140052404180800] Epoch[74] Batch[0] avg_epoch_loss=3.549262\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=3.54926228523\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:19 INFO 140052404180800] Epoch[74] Batch[5] avg_epoch_loss=3.561983\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=3.56198263168\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:19 INFO 140052404180800] Epoch[74] Batch [5]#011Speed: 329.98 samples/sec#011loss=3.561983\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] Epoch[74] Batch[10] avg_epoch_loss=3.496486\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=3.41789107323\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] Epoch[74] Batch [10]#011Speed: 332.48 samples/sec#011loss=3.417891\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2395.5390453338623, \"sum\": 2395.5390453338623, \"min\": 2395.5390453338623}}, \"EndTime\": 1592849960.318295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849957.922283}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.568284305 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=74, train loss <loss>=3.49648646875\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] Epoch[75] Batch[0] avg_epoch_loss=3.544590\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=3.54459023476\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:21 INFO 140052404180800] Epoch[75] Batch[5] avg_epoch_loss=3.539098\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=3.53909802437\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:21 INFO 140052404180800] Epoch[75] Batch [5]#011Speed: 335.46 samples/sec#011loss=3.539098\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:22 INFO 140052404180800] Epoch[75] Batch[10] avg_epoch_loss=3.565444\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=3.59705867767\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:22 INFO 140052404180800] Epoch[75] Batch [10]#011Speed: 323.68 samples/sec#011loss=3.597059\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:22 INFO 140052404180800] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2404.7482013702393, \"sum\": 2404.7482013702393, \"min\": 2404.7482013702393}}, \"EndTime\": 1592849962.72358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849960.318374}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.524961295 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=75, train loss <loss>=3.56544377587\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:23 INFO 140052404180800] Epoch[76] Batch[0] avg_epoch_loss=3.410967\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=3.41096687317\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:24 INFO 140052404180800] Epoch[76] Batch[5] avg_epoch_loss=3.606668\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=3.6066677173\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:24 INFO 140052404180800] Epoch[76] Batch [5]#011Speed: 319.47 samples/sec#011loss=3.606668\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:24 INFO 140052404180800] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2232.4719429016113, \"sum\": 2232.4719429016113, \"min\": 2232.4719429016113}}, \"EndTime\": 1592849964.95663, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849962.723641}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:24 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.601564376 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:24 INFO 140052404180800] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=76, train loss <loss>=3.56065616608\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:24 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:25 INFO 140052404180800] Epoch[77] Batch[0] avg_epoch_loss=3.590485\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=3.59048461914\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:26 INFO 140052404180800] Epoch[77] Batch[5] avg_epoch_loss=3.637306\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=3.63730641206\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:26 INFO 140052404180800] Epoch[77] Batch [5]#011Speed: 328.45 samples/sec#011loss=3.637306\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:27 INFO 140052404180800] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2196.277141571045, \"sum\": 2196.277141571045, \"min\": 2196.277141571045}}, \"EndTime\": 1592849967.153519, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849964.956706}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.18979358 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=77, train loss <loss>=3.58386602402\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:27 INFO 140052404180800] Epoch[78] Batch[0] avg_epoch_loss=3.677854\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=3.67785358429\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:28 INFO 140052404180800] Epoch[78] Batch[5] avg_epoch_loss=3.634471\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=3.63447097937\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:28 INFO 140052404180800] Epoch[78] Batch [5]#011Speed: 328.66 samples/sec#011loss=3.634471\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] Epoch[78] Batch[10] avg_epoch_loss=3.542189\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=3.43145122528\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] Epoch[78] Batch [10]#011Speed: 329.52 samples/sec#011loss=3.431451\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2386.7859840393066, \"sum\": 2386.7859840393066, \"min\": 2386.7859840393066}}, \"EndTime\": 1592849969.540867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849967.153605}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.414052539 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=78, train loss <loss>=3.54218927297\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] Epoch[79] Batch[0] avg_epoch_loss=3.569267\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=3.56926703453\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:30 INFO 140052404180800] Epoch[79] Batch[5] avg_epoch_loss=3.631828\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=3.63182779153\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:30 INFO 140052404180800] Epoch[79] Batch [5]#011Speed: 333.15 samples/sec#011loss=3.631828\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:31 INFO 140052404180800] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2186.1841678619385, \"sum\": 2186.1841678619385, \"min\": 2186.1841678619385}}, \"EndTime\": 1592849971.727578, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849969.540947}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:31 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.4715506 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:31 INFO 140052404180800] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=79, train loss <loss>=3.67319288254\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:31 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:32 INFO 140052404180800] Epoch[80] Batch[0] avg_epoch_loss=3.383235\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=3.38323497772\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:33 INFO 140052404180800] Epoch[80] Batch[5] avg_epoch_loss=3.500943\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=3.50094270706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:33 INFO 140052404180800] Epoch[80] Batch [5]#011Speed: 334.22 samples/sec#011loss=3.500943\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:33 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2198.601007461548, \"sum\": 2198.601007461548, \"min\": 2198.601007461548}}, \"EndTime\": 1592849973.926697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849971.727633}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:33 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.713766961 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:33 INFO 140052404180800] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=80, train loss <loss>=3.50848441124\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:33 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:34 INFO 140052404180800] Epoch[81] Batch[0] avg_epoch_loss=3.573869\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=3.57386922836\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:35 INFO 140052404180800] Epoch[81] Batch[5] avg_epoch_loss=3.491199\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=3.49119857947\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:35 INFO 140052404180800] Epoch[81] Batch [5]#011Speed: 339.26 samples/sec#011loss=3.491199\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:36 INFO 140052404180800] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2151.9041061401367, \"sum\": 2151.9041061401367, \"min\": 2151.9041061401367}}, \"EndTime\": 1592849976.079179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849973.926779}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=296.930032954 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=81, train loss <loss>=3.49237978458\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:36 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:36 INFO 140052404180800] Epoch[82] Batch[0] avg_epoch_loss=3.581815\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=3.58181548119\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:37 INFO 140052404180800] Epoch[82] Batch[5] avg_epoch_loss=3.506855\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=3.50685532888\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:37 INFO 140052404180800] Epoch[82] Batch [5]#011Speed: 337.31 samples/sec#011loss=3.506855\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:38 INFO 140052404180800] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2183.919906616211, \"sum\": 2183.919906616211, \"min\": 2183.919906616211}}, \"EndTime\": 1592849978.263634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849976.079261}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:38 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.086875858 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:38 INFO 140052404180800] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=82, train loss <loss>=3.5130398035\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:38 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:38 INFO 140052404180800] Epoch[83] Batch[0] avg_epoch_loss=3.709341\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=3.70934128761\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:39 INFO 140052404180800] Epoch[83] Batch[5] avg_epoch_loss=3.620803\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=3.62080256144\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:39 INFO 140052404180800] Epoch[83] Batch [5]#011Speed: 334.51 samples/sec#011loss=3.620803\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:40 INFO 140052404180800] Epoch[83] Batch[10] avg_epoch_loss=3.564458\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=3.49684462547\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:40 INFO 140052404180800] Epoch[83] Batch [10]#011Speed: 332.47 samples/sec#011loss=3.496845\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:40 INFO 140052404180800] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2376.5249252319336, \"sum\": 2376.5249252319336, \"min\": 2376.5249252319336}}, \"EndTime\": 1592849980.640638, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849978.263692}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:40 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.862025234 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:40 INFO 140052404180800] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=83, train loss <loss>=3.56445804509\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:40 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:41 INFO 140052404180800] Epoch[84] Batch[0] avg_epoch_loss=3.592820\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=3.5928196907\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:42 INFO 140052404180800] Epoch[84] Batch[5] avg_epoch_loss=3.542148\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=3.54214839141\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:42 INFO 140052404180800] Epoch[84] Batch [5]#011Speed: 337.25 samples/sec#011loss=3.542148\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:42 INFO 140052404180800] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2183.633804321289, \"sum\": 2183.633804321289, \"min\": 2183.633804321289}}, \"EndTime\": 1592849982.824772, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849980.640713}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:42 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.204121543 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:42 INFO 140052404180800] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=84, train loss <loss>=3.5774222374\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:42 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:43 INFO 140052404180800] Epoch[85] Batch[0] avg_epoch_loss=3.433654\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=3.43365359306\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:44 INFO 140052404180800] Epoch[85] Batch[5] avg_epoch_loss=3.554609\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=3.55460850398\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:44 INFO 140052404180800] Epoch[85] Batch [5]#011Speed: 332.84 samples/sec#011loss=3.554609\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] Epoch[85] Batch[10] avg_epoch_loss=3.480119\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=3.39073166847\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] Epoch[85] Batch [10]#011Speed: 329.79 samples/sec#011loss=3.390732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2388.847827911377, \"sum\": 2388.847827911377, \"min\": 2388.847827911377}}, \"EndTime\": 1592849985.214223, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849982.824856}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.01656472 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=85, train loss <loss>=3.48011903329\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_6d77fe18-bfac-4cb2-a7a5-e530c3c00033-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.74588203430176, \"sum\": 61.74588203430176, \"min\": 61.74588203430176}}, \"EndTime\": 1592849985.276462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849985.214284}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] Epoch[86] Batch[0] avg_epoch_loss=3.716543\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=3.71654295921\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:46 INFO 140052404180800] Epoch[86] Batch[5] avg_epoch_loss=3.504403\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=3.50440255801\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:46 INFO 140052404180800] Epoch[86] Batch [5]#011Speed: 337.10 samples/sec#011loss=3.504403\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:47 INFO 140052404180800] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2152.01997756958, \"sum\": 2152.01997756958, \"min\": 2152.01997756958}}, \"EndTime\": 1592849987.428618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849985.276533}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:47 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.721592174 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:47 INFO 140052404180800] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=86, train loss <loss>=3.47944252491\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:47 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:47 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_1b6766a3-9887-4fb5-ba52-4b02fe5dbfc3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 94.16317939758301, \"sum\": 94.16317939758301, \"min\": 94.16317939758301}}, \"EndTime\": 1592849987.523394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849987.4287}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:47 INFO 140052404180800] Epoch[87] Batch[0] avg_epoch_loss=3.705411\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=3.70541071892\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:48 INFO 140052404180800] Epoch[87] Batch[5] avg_epoch_loss=3.561488\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=3.56148771445\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:48 INFO 140052404180800] Epoch[87] Batch [5]#011Speed: 331.63 samples/sec#011loss=3.561488\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:49 INFO 140052404180800] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2207.719087600708, \"sum\": 2207.719087600708, \"min\": 2207.719087600708}}, \"EndTime\": 1592849989.731249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849987.523463}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:49 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.458630573 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:49 INFO 140052404180800] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=87, train loss <loss>=3.52827448845\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:49 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:50 INFO 140052404180800] Epoch[88] Batch[0] avg_epoch_loss=3.577810\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=3.57780981064\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:51 INFO 140052404180800] Epoch[88] Batch[5] avg_epoch_loss=3.504646\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=3.50464646022\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:51 INFO 140052404180800] Epoch[88] Batch [5]#011Speed: 333.38 samples/sec#011loss=3.504646\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:51 INFO 140052404180800] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2161.7660522460938, \"sum\": 2161.7660522460938, \"min\": 2161.7660522460938}}, \"EndTime\": 1592849991.893594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849989.73133}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:51 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.011153103 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:51 INFO 140052404180800] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=88, train loss <loss>=3.51210932732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:51 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:52 INFO 140052404180800] Epoch[89] Batch[0] avg_epoch_loss=3.480407\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=3.48040723801\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] Epoch[89] Batch[5] avg_epoch_loss=3.495369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=3.49536907673\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] Epoch[89] Batch [5]#011Speed: 324.94 samples/sec#011loss=3.495369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] processed a total of 561 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2019.7629928588867, \"sum\": 2019.7629928588867, \"min\": 2019.7629928588867}}, \"EndTime\": 1592849993.9139, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849991.893676}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.741034193 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=89, train loss <loss>=3.47402551439\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:53 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_c6d15e61-49a1-4d22-91d4-54c35ced45ee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.424089431762695, \"sum\": 60.424089431762695, \"min\": 60.424089431762695}}, \"EndTime\": 1592849993.974924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849993.913972}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:54 INFO 140052404180800] Epoch[90] Batch[0] avg_epoch_loss=3.593981\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=3.59398055077\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:55 INFO 140052404180800] Epoch[90] Batch[5] avg_epoch_loss=3.536186\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=3.53618649642\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:55 INFO 140052404180800] Epoch[90] Batch [5]#011Speed: 339.75 samples/sec#011loss=3.536186\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:56 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2147.5069522857666, \"sum\": 2147.5069522857666, \"min\": 2147.5069522857666}}, \"EndTime\": 1592849996.122559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849993.974989}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:56 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=294.744165635 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:56 INFO 140052404180800] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=90, train loss <loss>=3.54540719986\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:56 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:56 INFO 140052404180800] Epoch[91] Batch[0] avg_epoch_loss=3.545248\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=3.54524827003\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:57 INFO 140052404180800] Epoch[91] Batch[5] avg_epoch_loss=3.508800\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=3.50879967213\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:57 INFO 140052404180800] Epoch[91] Batch [5]#011Speed: 336.59 samples/sec#011loss=3.508800\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:58 INFO 140052404180800] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2164.0050411224365, \"sum\": 2164.0050411224365, \"min\": 2164.0050411224365}}, \"EndTime\": 1592849998.287126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849996.12264}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:58 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=292.035438655 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:58 INFO 140052404180800] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=91, train loss <loss>=3.51009280682\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:58 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:58 INFO 140052404180800] Epoch[92] Batch[0] avg_epoch_loss=3.647347\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=3.64734745026\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:59 INFO 140052404180800] Epoch[92] Batch[5] avg_epoch_loss=3.615560\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=3.61556005478\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:19:59 INFO 140052404180800] Epoch[92] Batch [5]#011Speed: 334.16 samples/sec#011loss=3.615560\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:00 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2172.4610328674316, \"sum\": 2172.4610328674316, \"min\": 2172.4610328674316}}, \"EndTime\": 1592850000.460253, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592849998.287205}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:00 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.216656256 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:00 INFO 140052404180800] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=92, train loss <loss>=3.57981369495\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:00 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:00 INFO 140052404180800] Epoch[93] Batch[0] avg_epoch_loss=3.577375\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=3.57737469673\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:01 INFO 140052404180800] Epoch[93] Batch[5] avg_epoch_loss=3.571599\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=3.57159892718\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:01 INFO 140052404180800] Epoch[93] Batch [5]#011Speed: 333.52 samples/sec#011loss=3.571599\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:02 INFO 140052404180800] Epoch[93] Batch[10] avg_epoch_loss=3.489812\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=3.39166779518\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:02 INFO 140052404180800] Epoch[93] Batch [10]#011Speed: 329.98 samples/sec#011loss=3.391668\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:02 INFO 140052404180800] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2382.0931911468506, \"sum\": 2382.0931911468506, \"min\": 2382.0931911468506}}, \"EndTime\": 1592850002.842887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850000.460333}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:02 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.115896333 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:02 INFO 140052404180800] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=93, train loss <loss>=3.489812049\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:02 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:03 INFO 140052404180800] Epoch[94] Batch[0] avg_epoch_loss=3.586684\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=3.58668422699\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:04 INFO 140052404180800] Epoch[94] Batch[5] avg_epoch_loss=3.576605\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=3.57660516103\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:04 INFO 140052404180800] Epoch[94] Batch [5]#011Speed: 334.74 samples/sec#011loss=3.576605\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:05 INFO 140052404180800] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2244.931936264038, \"sum\": 2244.931936264038, \"min\": 2244.931936264038}}, \"EndTime\": 1592850005.08832, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850002.842962}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:05 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.599464489 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:05 INFO 140052404180800] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=94, train loss <loss>=3.48682274818\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:05 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:05 INFO 140052404180800] Epoch[95] Batch[0] avg_epoch_loss=3.601323\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=3.60132288933\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:06 INFO 140052404180800] Epoch[95] Batch[5] avg_epoch_loss=3.482566\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=3.48256615798\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:06 INFO 140052404180800] Epoch[95] Batch [5]#011Speed: 334.12 samples/sec#011loss=3.482566\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] Epoch[95] Batch[10] avg_epoch_loss=3.549800\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=3.63047966957\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] Epoch[95] Batch [10]#011Speed: 337.90 samples/sec#011loss=3.630480\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2399.535894393921, \"sum\": 2399.535894393921, \"min\": 2399.535894393921}}, \"EndTime\": 1592850007.488394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850005.088401}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.540821924 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=95, train loss <loss>=3.54979957234\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] Epoch[96] Batch[0] avg_epoch_loss=3.440969\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=3.44096946716\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:08 INFO 140052404180800] Epoch[96] Batch[5] avg_epoch_loss=3.439817\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=3.4398171107\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:08 INFO 140052404180800] Epoch[96] Batch [5]#011Speed: 327.36 samples/sec#011loss=3.439817\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:09 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2219.215154647827, \"sum\": 2219.215154647827, \"min\": 2219.215154647827}}, \"EndTime\": 1592850009.708104, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850007.48847}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:09 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.165535629 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:09 INFO 140052404180800] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=96, train loss <loss>=3.47973947525\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:09 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:10 INFO 140052404180800] Epoch[97] Batch[0] avg_epoch_loss=3.458330\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=3.45832967758\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:11 INFO 140052404180800] Epoch[97] Batch[5] avg_epoch_loss=3.559664\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=3.5596635739\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:11 INFO 140052404180800] Epoch[97] Batch [5]#011Speed: 333.89 samples/sec#011loss=3.559664\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:11 INFO 140052404180800] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2178.499937057495, \"sum\": 2178.499937057495, \"min\": 2178.499937057495}}, \"EndTime\": 1592850011.887125, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850009.708186}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:11 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=291.471067148 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:11 INFO 140052404180800] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=97, train loss <loss>=3.522407794\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:11 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:12 INFO 140052404180800] Epoch[98] Batch[0] avg_epoch_loss=3.411878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=3.41187787056\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:13 INFO 140052404180800] Epoch[98] Batch[5] avg_epoch_loss=3.507741\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=3.50774089495\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:13 INFO 140052404180800] Epoch[98] Batch [5]#011Speed: 332.98 samples/sec#011loss=3.507741\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:14 INFO 140052404180800] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2190.8819675445557, \"sum\": 2190.8819675445557, \"min\": 2190.8819675445557}}, \"EndTime\": 1592850014.078578, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850011.887199}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:14 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.867855937 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:14 INFO 140052404180800] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=98, train loss <loss>=3.49253048897\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:14 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:14 INFO 140052404180800] Epoch[99] Batch[0] avg_epoch_loss=3.629090\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=3.62908983231\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:15 INFO 140052404180800] Epoch[99] Batch[5] avg_epoch_loss=3.534335\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=3.53433461984\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:15 INFO 140052404180800] Epoch[99] Batch [5]#011Speed: 331.42 samples/sec#011loss=3.534335\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:16 INFO 140052404180800] Epoch[99] Batch[10] avg_epoch_loss=3.598948\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=3.67648363113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:16 INFO 140052404180800] Epoch[99] Batch [10]#011Speed: 333.88 samples/sec#011loss=3.676484\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:16 INFO 140052404180800] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2429.38494682312, \"sum\": 2429.38494682312, \"min\": 2429.38494682312}}, \"EndTime\": 1592850016.508498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850014.078661}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:16 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.54553862 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:16 INFO 140052404180800] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=99, train loss <loss>=3.59894780679\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:16 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:17 INFO 140052404180800] Epoch[100] Batch[0] avg_epoch_loss=3.537005\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=3.5370054245\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:17 INFO 140052404180800] Epoch[100] Batch[5] avg_epoch_loss=3.564121\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=3.56412128607\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:17 INFO 140052404180800] Epoch[100] Batch [5]#011Speed: 336.82 samples/sec#011loss=3.564121\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:18 INFO 140052404180800] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2240.703821182251, \"sum\": 2240.703821182251, \"min\": 2240.703821182251}}, \"EndTime\": 1592850018.749699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850016.508572}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:18 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.09721201 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:18 INFO 140052404180800] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=100, train loss <loss>=3.4815669775\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:18 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:19 INFO 140052404180800] Epoch[101] Batch[0] avg_epoch_loss=3.438306\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=3.4383058548\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:20 INFO 140052404180800] Epoch[101] Batch[5] avg_epoch_loss=3.543532\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=3.54353205363\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:20 INFO 140052404180800] Epoch[101] Batch [5]#011Speed: 338.95 samples/sec#011loss=3.543532\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] Epoch[101] Batch[10] avg_epoch_loss=3.501226\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=3.45045933723\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] Epoch[101] Batch [10]#011Speed: 333.04 samples/sec#011loss=3.450459\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2397.3329067230225, \"sum\": 2397.3329067230225, \"min\": 2397.3329067230225}}, \"EndTime\": 1592850021.147571, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850018.749783}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.122292939 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=101, train loss <loss>=3.50122627345\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] Epoch[102] Batch[0] avg_epoch_loss=3.430635\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=3.43063497543\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:22 INFO 140052404180800] Epoch[102] Batch[5] avg_epoch_loss=3.490406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=3.49040575822\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:22 INFO 140052404180800] Epoch[102] Batch [5]#011Speed: 336.84 samples/sec#011loss=3.490406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:23 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2208.8561058044434, \"sum\": 2208.8561058044434, \"min\": 2208.8561058044434}}, \"EndTime\": 1592850023.356937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850021.147645}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:23 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.485502633 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:23 INFO 140052404180800] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=102, train loss <loss>=3.47798135281\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:23 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:23 INFO 140052404180800] Epoch[103] Batch[0] avg_epoch_loss=3.493702\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=3.49370217323\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:24 INFO 140052404180800] Epoch[103] Batch[5] avg_epoch_loss=3.536195\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=3.53619476159\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:24 INFO 140052404180800] Epoch[103] Batch [5]#011Speed: 333.07 samples/sec#011loss=3.536195\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:25 INFO 140052404180800] Epoch[103] Batch[10] avg_epoch_loss=3.589875\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=3.65429224968\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:25 INFO 140052404180800] Epoch[103] Batch [10]#011Speed: 333.10 samples/sec#011loss=3.654292\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:25 INFO 140052404180800] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2388.2830142974854, \"sum\": 2388.2830142974854, \"min\": 2388.2830142974854}}, \"EndTime\": 1592850025.745707, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850023.357009}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:25 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.17346418 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:25 INFO 140052404180800] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=103, train loss <loss>=3.589875438\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:25 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:26 INFO 140052404180800] Epoch[104] Batch[0] avg_epoch_loss=3.428622\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=3.42862224579\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:27 INFO 140052404180800] Epoch[104] Batch[5] avg_epoch_loss=3.481968\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=3.48196832339\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:27 INFO 140052404180800] Epoch[104] Batch [5]#011Speed: 338.24 samples/sec#011loss=3.481968\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:27 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2165.5821800231934, \"sum\": 2165.5821800231934, \"min\": 2165.5821800231934}}, \"EndTime\": 1592850027.911831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850025.745782}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.051916815 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=104, train loss <loss>=3.51637792587\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:28 INFO 140052404180800] Epoch[105] Batch[0] avg_epoch_loss=3.447685\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=3.4476852417\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:29 INFO 140052404180800] Epoch[105] Batch[5] avg_epoch_loss=3.552946\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=3.55294589202\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:29 INFO 140052404180800] Epoch[105] Batch [5]#011Speed: 326.21 samples/sec#011loss=3.552946\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:30 INFO 140052404180800] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2210.6120586395264, \"sum\": 2210.6120586395264, \"min\": 2210.6120586395264}}, \"EndTime\": 1592850030.122983, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850027.911913}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:30 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.283725328 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:30 INFO 140052404180800] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=105, train loss <loss>=3.55280570984\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:30 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:30 INFO 140052404180800] Epoch[106] Batch[0] avg_epoch_loss=3.496256\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=3.49625611305\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:31 INFO 140052404180800] Epoch[106] Batch[5] avg_epoch_loss=3.461854\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=3.46185382207\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:31 INFO 140052404180800] Epoch[106] Batch [5]#011Speed: 337.77 samples/sec#011loss=3.461854\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:32 INFO 140052404180800] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2141.2110328674316, \"sum\": 2141.2110328674316, \"min\": 2141.2110328674316}}, \"EndTime\": 1592850032.264719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850030.123066}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:32 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.338303046 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:32 INFO 140052404180800] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=106, train loss <loss>=3.48235390186\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:32 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:32 INFO 140052404180800] Epoch[107] Batch[0] avg_epoch_loss=3.464284\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=3.46428370476\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:33 INFO 140052404180800] Epoch[107] Batch[5] avg_epoch_loss=3.555041\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=3.55504095554\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:33 INFO 140052404180800] Epoch[107] Batch [5]#011Speed: 330.82 samples/sec#011loss=3.555041\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] Epoch[107] Batch[10] avg_epoch_loss=3.448632\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=3.32094054222\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] Epoch[107] Batch [10]#011Speed: 327.70 samples/sec#011loss=3.320941\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2395.6120014190674, \"sum\": 2395.6120014190674, \"min\": 2395.6120014190674}}, \"EndTime\": 1592850034.660916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850032.264788}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.979333469 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=107, train loss <loss>=3.44863167676\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:34 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_5c5e8a38-b0a1-402b-a9fb-94a1bbf3e452-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.55896186828613, \"sum\": 61.55896186828613, \"min\": 61.55896186828613}}, \"EndTime\": 1592850034.723083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850034.660975}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:35 INFO 140052404180800] Epoch[108] Batch[0] avg_epoch_loss=3.402815\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=3.40281486511\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:36 INFO 140052404180800] Epoch[108] Batch[5] avg_epoch_loss=3.439366\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=3.43936562538\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:36 INFO 140052404180800] Epoch[108] Batch [5]#011Speed: 337.16 samples/sec#011loss=3.439366\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] Epoch[108] Batch[10] avg_epoch_loss=3.501274\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=3.57556476593\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] Epoch[108] Batch [10]#011Speed: 338.66 samples/sec#011loss=3.575565\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2351.1128425598145, \"sum\": 2351.1128425598145, \"min\": 2351.1128425598145}}, \"EndTime\": 1592850037.074521, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850034.723349}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=290.061546664 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=108, train loss <loss>=3.50127432563\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] Epoch[109] Batch[0] avg_epoch_loss=3.560049\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=3.56004858017\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:38 INFO 140052404180800] Epoch[109] Batch[5] avg_epoch_loss=3.532083\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=3.53208259741\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:38 INFO 140052404180800] Epoch[109] Batch [5]#011Speed: 340.32 samples/sec#011loss=3.532083\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:39 INFO 140052404180800] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2215.878963470459, \"sum\": 2215.878963470459, \"min\": 2215.878963470459}}, \"EndTime\": 1592850039.290961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850037.074597}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:39 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.429982253 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:39 INFO 140052404180800] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=109, train loss <loss>=3.50159492493\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:39 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:39 INFO 140052404180800] Epoch[110] Batch[0] avg_epoch_loss=3.516378\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=3.51637816429\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:40 INFO 140052404180800] Epoch[110] Batch[5] avg_epoch_loss=3.479680\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=3.47968041897\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:40 INFO 140052404180800] Epoch[110] Batch [5]#011Speed: 335.26 samples/sec#011loss=3.479680\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:41 INFO 140052404180800] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2173.854112625122, \"sum\": 2173.854112625122, \"min\": 2173.854112625122}}, \"EndTime\": 1592850041.465361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850039.291043}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:41 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.332294689 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:41 INFO 140052404180800] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=110, train loss <loss>=3.5297699213\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:41 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:41 INFO 140052404180800] Epoch[111] Batch[0] avg_epoch_loss=3.423920\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=3.42391967773\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:42 INFO 140052404180800] Epoch[111] Batch[5] avg_epoch_loss=3.423658\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=3.42365773519\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:42 INFO 140052404180800] Epoch[111] Batch [5]#011Speed: 336.25 samples/sec#011loss=3.423658\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:43 INFO 140052404180800] Epoch[111] Batch[10] avg_epoch_loss=3.514262\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=3.62298674583\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:43 INFO 140052404180800] Epoch[111] Batch [10]#011Speed: 334.37 samples/sec#011loss=3.622987\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:43 INFO 140052404180800] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2369.565010070801, \"sum\": 2369.565010070801, \"min\": 2369.565010070801}}, \"EndTime\": 1592850043.835494, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850041.465441}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.299558563 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=111, train loss <loss>=3.51426183094\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:44 INFO 140052404180800] Epoch[112] Batch[0] avg_epoch_loss=3.428755\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=3.42875480652\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:45 INFO 140052404180800] Epoch[112] Batch[5] avg_epoch_loss=3.446453\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=3.44645277659\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:45 INFO 140052404180800] Epoch[112] Batch [5]#011Speed: 331.58 samples/sec#011loss=3.446453\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:46 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2179.755926132202, \"sum\": 2179.755926132202, \"min\": 2179.755926132202}}, \"EndTime\": 1592850046.015765, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850043.835566}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:46 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.17244318 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:46 INFO 140052404180800] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=112, train loss <loss>=3.46111454964\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:46 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:46 INFO 140052404180800] Epoch[113] Batch[0] avg_epoch_loss=3.423127\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=3.4231274128\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:47 INFO 140052404180800] Epoch[113] Batch[5] avg_epoch_loss=3.414576\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=3.41457553705\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:47 INFO 140052404180800] Epoch[113] Batch [5]#011Speed: 334.50 samples/sec#011loss=3.414576\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] Epoch[113] Batch[10] avg_epoch_loss=3.397014\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=3.3759390831\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] Epoch[113] Batch [10]#011Speed: 336.67 samples/sec#011loss=3.375939\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2409.754991531372, \"sum\": 2409.754991531372, \"min\": 2409.754991531372}}, \"EndTime\": 1592850048.426064, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850046.015847}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.459529163 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=113, train loss <loss>=3.39701351252\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_af8bd87c-8287-45ce-89be-4d26310519d0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 65.59300422668457, \"sum\": 65.59300422668457, \"min\": 65.59300422668457}}, \"EndTime\": 1592850048.492228, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850048.426136}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] Epoch[114] Batch[0] avg_epoch_loss=3.554645\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=3.55464458466\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:49 INFO 140052404180800] Epoch[114] Batch[5] avg_epoch_loss=3.503546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=3.50354635715\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:49 INFO 140052404180800] Epoch[114] Batch [5]#011Speed: 337.33 samples/sec#011loss=3.503546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:50 INFO 140052404180800] Epoch[114] Batch[10] avg_epoch_loss=3.478410\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=3.44824562073\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:50 INFO 140052404180800] Epoch[114] Batch [10]#011Speed: 335.15 samples/sec#011loss=3.448246\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:50 INFO 140052404180800] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2360.408067703247, \"sum\": 2360.408067703247, \"min\": 2360.408067703247}}, \"EndTime\": 1592850050.852775, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850048.492305}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.260361661 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=114, train loss <loss>=3.47840965878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:50 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:51 INFO 140052404180800] Epoch[115] Batch[0] avg_epoch_loss=3.606430\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=3.60642957687\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:52 INFO 140052404180800] Epoch[115] Batch[5] avg_epoch_loss=3.491203\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=3.49120338758\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:52 INFO 140052404180800] Epoch[115] Batch [5]#011Speed: 332.00 samples/sec#011loss=3.491203\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] Epoch[115] Batch[10] avg_epoch_loss=3.382766\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=3.25264139175\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] Epoch[115] Batch [10]#011Speed: 333.11 samples/sec#011loss=3.252641\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2378.512144088745, \"sum\": 2378.512144088745, \"min\": 2378.512144088745}}, \"EndTime\": 1592850053.231784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850050.852847}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.585811049 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=115, train loss <loss>=3.38276611675\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_fcc6069b-f213-494a-b82d-5f816dbfa5be-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 69.87810134887695, \"sum\": 69.87810134887695, \"min\": 69.87810134887695}}, \"EndTime\": 1592850053.302199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850053.231858}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] Epoch[116] Batch[0] avg_epoch_loss=3.651290\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=3.65129041672\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:54 INFO 140052404180800] Epoch[116] Batch[5] avg_epoch_loss=3.511091\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=3.51109111309\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:54 INFO 140052404180800] Epoch[116] Batch [5]#011Speed: 336.95 samples/sec#011loss=3.511091\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:55 INFO 140052404180800] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2173.4049320220947, \"sum\": 2173.4049320220947, \"min\": 2173.4049320220947}}, \"EndTime\": 1592850055.475735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850053.302272}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:55 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.250779664 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:55 INFO 140052404180800] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=116, train loss <loss>=3.48856811523\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:55 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:55 INFO 140052404180800] Epoch[117] Batch[0] avg_epoch_loss=3.757065\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=3.75706481934\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:56 INFO 140052404180800] Epoch[117] Batch[5] avg_epoch_loss=3.506634\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=3.50663423538\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:56 INFO 140052404180800] Epoch[117] Batch [5]#011Speed: 341.15 samples/sec#011loss=3.506634\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:57 INFO 140052404180800] Epoch[117] Batch[10] avg_epoch_loss=3.386760\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=3.24291014671\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:57 INFO 140052404180800] Epoch[117] Batch [10]#011Speed: 337.64 samples/sec#011loss=3.242910\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:57 INFO 140052404180800] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2346.47798538208, \"sum\": 2346.47798538208, \"min\": 2346.47798538208}}, \"EndTime\": 1592850057.82278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850055.475818}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.587561501 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=117, train loss <loss>=3.38675964962\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:58 INFO 140052404180800] Epoch[118] Batch[0] avg_epoch_loss=3.345756\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=3.34575605392\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:59 INFO 140052404180800] Epoch[118] Batch[5] avg_epoch_loss=3.492585\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=3.49258466562\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:20:59 INFO 140052404180800] Epoch[118] Batch [5]#011Speed: 330.67 samples/sec#011loss=3.492585\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:00 INFO 140052404180800] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2195.9869861602783, \"sum\": 2195.9869861602783, \"min\": 2195.9869861602783}}, \"EndTime\": 1592850060.019254, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850057.822866}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:00 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.782732082 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:00 INFO 140052404180800] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=118, train loss <loss>=3.46776993275\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:00 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:00 INFO 140052404180800] Epoch[119] Batch[0] avg_epoch_loss=3.384851\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=3.38485050201\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:01 INFO 140052404180800] Epoch[119] Batch[5] avg_epoch_loss=3.416731\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=3.41673147678\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:01 INFO 140052404180800] Epoch[119] Batch [5]#011Speed: 337.15 samples/sec#011loss=3.416731\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:02 INFO 140052404180800] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2193.258047103882, \"sum\": 2193.258047103882, \"min\": 2193.258047103882}}, \"EndTime\": 1592850062.213036, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850060.019333}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:02 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.508698415 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:02 INFO 140052404180800] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=119, train loss <loss>=3.45655620098\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:02 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:02 INFO 140052404180800] Epoch[120] Batch[0] avg_epoch_loss=3.358579\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=3.35857915878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:03 INFO 140052404180800] Epoch[120] Batch[5] avg_epoch_loss=3.491719\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=3.49171892802\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:03 INFO 140052404180800] Epoch[120] Batch [5]#011Speed: 336.58 samples/sec#011loss=3.491719\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:04 INFO 140052404180800] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2197.8840827941895, \"sum\": 2197.8840827941895, \"min\": 2197.8840827941895}}, \"EndTime\": 1592850064.41147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850062.213113}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:04 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.710381396 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:04 INFO 140052404180800] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=120, train loss <loss>=3.45327055454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:04 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:04 INFO 140052404180800] Epoch[121] Batch[0] avg_epoch_loss=3.689733\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=3.68973302841\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:05 INFO 140052404180800] Epoch[121] Batch[5] avg_epoch_loss=3.525304\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=3.52530384064\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:05 INFO 140052404180800] Epoch[121] Batch [5]#011Speed: 335.28 samples/sec#011loss=3.525304\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:06 INFO 140052404180800] Epoch[121] Batch[10] avg_epoch_loss=3.530443\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=3.53660998344\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:06 INFO 140052404180800] Epoch[121] Batch [10]#011Speed: 330.43 samples/sec#011loss=3.536610\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:06 INFO 140052404180800] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2381.680965423584, \"sum\": 2381.680965423584, \"min\": 2381.680965423584}}, \"EndTime\": 1592850066.793662, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850064.411547}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.079004794 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=121, train loss <loss>=3.53044299646\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:07 INFO 140052404180800] Epoch[122] Batch[0] avg_epoch_loss=3.526336\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=3.52633619308\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:08 INFO 140052404180800] Epoch[122] Batch[5] avg_epoch_loss=3.528128\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=3.52812810739\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:08 INFO 140052404180800] Epoch[122] Batch [5]#011Speed: 339.56 samples/sec#011loss=3.528128\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:08 INFO 140052404180800] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2169.0170764923096, \"sum\": 2169.0170764923096, \"min\": 2169.0170764923096}}, \"EndTime\": 1592850068.963195, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850066.793741}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.912520384 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=122, train loss <loss>=3.56894145012\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:09 INFO 140052404180800] Epoch[123] Batch[0] avg_epoch_loss=3.309753\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=3.30975294113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:10 INFO 140052404180800] Epoch[123] Batch[5] avg_epoch_loss=3.405068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=3.40506839752\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:10 INFO 140052404180800] Epoch[123] Batch [5]#011Speed: 334.63 samples/sec#011loss=3.405068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] Epoch[123] Batch[10] avg_epoch_loss=3.392600\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=3.3776371479\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] Epoch[123] Batch [10]#011Speed: 337.25 samples/sec#011loss=3.377637\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2419.3620681762695, \"sum\": 2419.3620681762695, \"min\": 2419.3620681762695}}, \"EndTime\": 1592850071.383139, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850068.963278}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.306352903 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=123, train loss <loss>=3.3925996477\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] Epoch[124] Batch[0] avg_epoch_loss=3.385381\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=3.38538122177\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:12 INFO 140052404180800] Epoch[124] Batch[5] avg_epoch_loss=3.516477\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=3.51647698879\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:12 INFO 140052404180800] Epoch[124] Batch [5]#011Speed: 334.92 samples/sec#011loss=3.516477\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:13 INFO 140052404180800] Epoch[124] Batch[10] avg_epoch_loss=3.528183\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=3.54222931862\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:13 INFO 140052404180800] Epoch[124] Batch [10]#011Speed: 330.69 samples/sec#011loss=3.542229\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:13 INFO 140052404180800] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2406.5680503845215, \"sum\": 2406.5680503845215, \"min\": 2406.5680503845215}}, \"EndTime\": 1592850073.790236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850071.383219}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.651988101 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=124, train loss <loss>=3.52818259326\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:13 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:14 INFO 140052404180800] Epoch[125] Batch[0] avg_epoch_loss=3.342607\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=3.34260749817\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:15 INFO 140052404180800] Epoch[125] Batch[5] avg_epoch_loss=3.455899\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=3.45589923859\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:15 INFO 140052404180800] Epoch[125] Batch [5]#011Speed: 339.33 samples/sec#011loss=3.455899\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:16 INFO 140052404180800] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2212.0511531829834, \"sum\": 2212.0511531829834, \"min\": 2212.0511531829834}}, \"EndTime\": 1592850076.002787, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850073.790313}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:16 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.199134302 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:16 INFO 140052404180800] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=125, train loss <loss>=3.47574369907\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:16 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:16 INFO 140052404180800] Epoch[126] Batch[0] avg_epoch_loss=3.568895\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=3.56889533997\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:17 INFO 140052404180800] Epoch[126] Batch[5] avg_epoch_loss=3.383161\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=3.38316078981\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:17 INFO 140052404180800] Epoch[126] Batch [5]#011Speed: 336.87 samples/sec#011loss=3.383161\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:18 INFO 140052404180800] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2167.4139499664307, \"sum\": 2167.4139499664307, \"min\": 2167.4139499664307}}, \"EndTime\": 1592850078.170756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850076.002871}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:18 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=290.653152777 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:18 INFO 140052404180800] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=126, train loss <loss>=3.4424428463\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:18 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:18 INFO 140052404180800] Epoch[127] Batch[0] avg_epoch_loss=3.305910\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=3.30590987206\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:19 INFO 140052404180800] Epoch[127] Batch[5] avg_epoch_loss=3.434209\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=3.43420894941\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:19 INFO 140052404180800] Epoch[127] Batch [5]#011Speed: 337.30 samples/sec#011loss=3.434209\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] Epoch[127] Batch[10] avg_epoch_loss=3.485973\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=3.54809045792\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] Epoch[127] Batch [10]#011Speed: 336.35 samples/sec#011loss=3.548090\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2352.555990219116, \"sum\": 2352.555990219116, \"min\": 2352.555990219116}}, \"EndTime\": 1592850080.523855, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850078.170837}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.45673722 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=127, train loss <loss>=3.48597327146\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] Epoch[128] Batch[0] avg_epoch_loss=3.582534\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=3.58253359795\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:21 INFO 140052404180800] Epoch[128] Batch[5] avg_epoch_loss=3.401060\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=3.40105994542\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:21 INFO 140052404180800] Epoch[128] Batch [5]#011Speed: 339.20 samples/sec#011loss=3.401060\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:22 INFO 140052404180800] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2190.0250911712646, \"sum\": 2190.0250911712646, \"min\": 2190.0250911712646}}, \"EndTime\": 1592850082.714384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850080.523931}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.128454641 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=128, train loss <loss>=3.34929742813\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:22 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:22 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_c60df46d-6e3d-4d57-a21d-05ed272e3811-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.5361156463623, \"sum\": 67.5361156463623, \"min\": 67.5361156463623}}, \"EndTime\": 1592850082.782507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850082.714465}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:23 INFO 140052404180800] Epoch[129] Batch[0] avg_epoch_loss=3.363322\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=3.363322258\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:24 INFO 140052404180800] Epoch[129] Batch[5] avg_epoch_loss=3.408302\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=3.40830242634\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:24 INFO 140052404180800] Epoch[129] Batch [5]#011Speed: 326.70 samples/sec#011loss=3.408302\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:25 INFO 140052404180800] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2300.792932510376, \"sum\": 2300.792932510376, \"min\": 2300.792932510376}}, \"EndTime\": 1592850085.083419, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850082.78256}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:25 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.28013245 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:25 INFO 140052404180800] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=129, train loss <loss>=3.43993909359\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:25 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:25 INFO 140052404180800] Epoch[130] Batch[0] avg_epoch_loss=3.284820\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=3.2848200798\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:26 INFO 140052404180800] Epoch[130] Batch[5] avg_epoch_loss=3.414706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=3.4147062699\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:26 INFO 140052404180800] Epoch[130] Batch [5]#011Speed: 340.14 samples/sec#011loss=3.414706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:27 INFO 140052404180800] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2187.825918197632, \"sum\": 2187.825918197632, \"min\": 2187.825918197632}}, \"EndTime\": 1592850087.27179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850085.083501}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.656594231 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=130, train loss <loss>=3.41493425369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:27 INFO 140052404180800] Epoch[131] Batch[0] avg_epoch_loss=3.458067\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=3.45806717873\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:28 INFO 140052404180800] Epoch[131] Batch[5] avg_epoch_loss=3.459325\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=3.45932515462\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:28 INFO 140052404180800] Epoch[131] Batch [5]#011Speed: 334.43 samples/sec#011loss=3.459325\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:29 INFO 140052404180800] Epoch[131] Batch[10] avg_epoch_loss=3.444982\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=3.42777042389\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:29 INFO 140052404180800] Epoch[131] Batch [10]#011Speed: 329.72 samples/sec#011loss=3.427770\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:29 INFO 140052404180800] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2409.264087677002, \"sum\": 2409.264087677002, \"min\": 2409.264087677002}}, \"EndTime\": 1592850089.681584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850087.271869}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.060741313 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=131, train loss <loss>=3.4449820952\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:29 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:30 INFO 140052404180800] Epoch[132] Batch[0] avg_epoch_loss=3.444589\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=3.44458913803\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:31 INFO 140052404180800] Epoch[132] Batch[5] avg_epoch_loss=3.467836\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=3.46783582369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:31 INFO 140052404180800] Epoch[132] Batch [5]#011Speed: 335.71 samples/sec#011loss=3.467836\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] Epoch[132] Batch[10] avg_epoch_loss=3.539249\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=3.62494425774\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] Epoch[132] Batch [10]#011Speed: 334.47 samples/sec#011loss=3.624944\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2365.934133529663, \"sum\": 2365.934133529663, \"min\": 2365.934133529663}}, \"EndTime\": 1592850092.048, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850089.681661}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.761964454 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=132, train loss <loss>=3.53924874826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] Epoch[133] Batch[0] avg_epoch_loss=3.433520\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=3.43352007866\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:33 INFO 140052404180800] Epoch[133] Batch[5] avg_epoch_loss=3.446201\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=3.44620088736\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:33 INFO 140052404180800] Epoch[133] Batch [5]#011Speed: 335.46 samples/sec#011loss=3.446201\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] Epoch[133] Batch[10] avg_epoch_loss=3.515337\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=3.59830131531\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] Epoch[133] Batch [10]#011Speed: 322.67 samples/sec#011loss=3.598301\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2405.742883682251, \"sum\": 2405.742883682251, \"min\": 2405.742883682251}}, \"EndTime\": 1592850094.454241, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850092.048071}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.760606556 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=133, train loss <loss>=3.51533744552\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] Epoch[134] Batch[0] avg_epoch_loss=3.401940\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=3.40194034576\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:35 INFO 140052404180800] Epoch[134] Batch[5] avg_epoch_loss=3.454253\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=3.45425315698\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:35 INFO 140052404180800] Epoch[134] Batch [5]#011Speed: 339.30 samples/sec#011loss=3.454253\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:36 INFO 140052404180800] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2152.8079509735107, \"sum\": 2152.8079509735107, \"min\": 2152.8079509735107}}, \"EndTime\": 1592850096.607531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850094.454304}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.296764409 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=134, train loss <loss>=3.37941839695\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:36 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:37 INFO 140052404180800] Epoch[135] Batch[0] avg_epoch_loss=3.323886\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=3.3238863945\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:38 INFO 140052404180800] Epoch[135] Batch[5] avg_epoch_loss=3.404071\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=3.40407093366\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:38 INFO 140052404180800] Epoch[135] Batch [5]#011Speed: 334.92 samples/sec#011loss=3.404071\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] Epoch[135] Batch[10] avg_epoch_loss=3.477247\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=3.56505775452\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] Epoch[135] Batch [10]#011Speed: 326.95 samples/sec#011loss=3.565058\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2415.9791469573975, \"sum\": 2415.9791469573975, \"min\": 2415.9791469573975}}, \"EndTime\": 1592850099.024038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850096.607614}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.861388925 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=135, train loss <loss>=3.47724676132\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] Epoch[136] Batch[0] avg_epoch_loss=3.414633\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=3.41463255882\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:40 INFO 140052404180800] Epoch[136] Batch[5] avg_epoch_loss=3.377357\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=3.37735708555\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:40 INFO 140052404180800] Epoch[136] Batch [5]#011Speed: 333.69 samples/sec#011loss=3.377357\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] Epoch[136] Batch[10] avg_epoch_loss=3.379988\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=3.38314590454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] Epoch[136] Batch [10]#011Speed: 328.58 samples/sec#011loss=3.383146\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2398.6949920654297, \"sum\": 2398.6949920654297, \"min\": 2398.6949920654297}}, \"EndTime\": 1592850101.423202, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850099.02411}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.476839127 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=136, train loss <loss>=3.37998836691\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] Epoch[137] Batch[0] avg_epoch_loss=3.472420\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=3.47241973877\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:42 INFO 140052404180800] Epoch[137] Batch[5] avg_epoch_loss=3.440148\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=3.44014759858\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:42 INFO 140052404180800] Epoch[137] Batch [5]#011Speed: 339.77 samples/sec#011loss=3.440148\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] Epoch[137] Batch[10] avg_epoch_loss=3.341526\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=3.22318072319\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] Epoch[137] Batch [10]#011Speed: 326.37 samples/sec#011loss=3.223181\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2405.7300090789795, \"sum\": 2405.7300090789795, \"min\": 2405.7300090789795}}, \"EndTime\": 1592850103.829426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850101.423278}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.901883029 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=137, train loss <loss>=3.34152629159\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:43 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_3c4521f4-23d4-4bab-b722-03cc5479d10b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 62.01910972595215, \"sum\": 62.01910972595215, \"min\": 62.01910972595215}}, \"EndTime\": 1592850103.892021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850103.829523}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:44 INFO 140052404180800] Epoch[138] Batch[0] avg_epoch_loss=3.354450\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=3.35445046425\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:45 INFO 140052404180800] Epoch[138] Batch[5] avg_epoch_loss=3.385847\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=3.38584697247\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:45 INFO 140052404180800] Epoch[138] Batch [5]#011Speed: 324.42 samples/sec#011loss=3.385847\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] Epoch[138] Batch[10] avg_epoch_loss=3.365410\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=3.34088497162\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] Epoch[138] Batch [10]#011Speed: 338.37 samples/sec#011loss=3.340885\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2393.5580253601074, \"sum\": 2393.5580253601074, \"min\": 2393.5580253601074}}, \"EndTime\": 1592850106.285721, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850103.892098}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.385317155 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=138, train loss <loss>=3.36540969935\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] Epoch[139] Batch[0] avg_epoch_loss=3.430689\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=3.43068861961\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:47 INFO 140052404180800] Epoch[139] Batch[5] avg_epoch_loss=3.408460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=3.40846025944\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:47 INFO 140052404180800] Epoch[139] Batch [5]#011Speed: 337.66 samples/sec#011loss=3.408460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:48 INFO 140052404180800] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2156.5887928009033, \"sum\": 2156.5887928009033, \"min\": 2156.5887928009033}}, \"EndTime\": 1592850108.442859, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850106.285792}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:48 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.056177507 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:48 INFO 140052404180800] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=139, train loss <loss>=3.37054896355\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:48 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:48 INFO 140052404180800] Epoch[140] Batch[0] avg_epoch_loss=3.525549\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=3.52554869652\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:49 INFO 140052404180800] Epoch[140] Batch[5] avg_epoch_loss=3.389460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=3.38946012656\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:49 INFO 140052404180800] Epoch[140] Batch [5]#011Speed: 332.31 samples/sec#011loss=3.389460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:50 INFO 140052404180800] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2270.822048187256, \"sum\": 2270.822048187256, \"min\": 2270.822048187256}}, \"EndTime\": 1592850110.71423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850108.442943}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.858499931 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=140, train loss <loss>=3.38383367062\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:50 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:51 INFO 140052404180800] Epoch[141] Batch[0] avg_epoch_loss=3.308625\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=3.30862545967\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:52 INFO 140052404180800] Epoch[141] Batch[5] avg_epoch_loss=3.361242\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=3.36124189695\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:52 INFO 140052404180800] Epoch[141] Batch [5]#011Speed: 340.63 samples/sec#011loss=3.361242\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:52 INFO 140052404180800] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2188.398838043213, \"sum\": 2188.398838043213, \"min\": 2188.398838043213}}, \"EndTime\": 1592850112.903172, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850110.714311}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.443135412 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=141, train loss <loss>=3.35633814335\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:53 INFO 140052404180800] Epoch[142] Batch[0] avg_epoch_loss=3.333660\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=3.33366036415\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:54 INFO 140052404180800] Epoch[142] Batch[5] avg_epoch_loss=3.379173\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=3.37917268276\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:54 INFO 140052404180800] Epoch[142] Batch [5]#011Speed: 323.73 samples/sec#011loss=3.379173\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:55 INFO 140052404180800] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2279.0610790252686, \"sum\": 2279.0610790252686, \"min\": 2279.0610790252686}}, \"EndTime\": 1592850115.182765, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850112.903252}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:55 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.537456763 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:55 INFO 140052404180800] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=142, train loss <loss>=3.40556156635\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:55 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:55 INFO 140052404180800] Epoch[143] Batch[0] avg_epoch_loss=3.268620\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=3.26862049103\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:56 INFO 140052404180800] Epoch[143] Batch[5] avg_epoch_loss=3.381270\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=3.38127020995\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:56 INFO 140052404180800] Epoch[143] Batch [5]#011Speed: 335.13 samples/sec#011loss=3.381270\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:57 INFO 140052404180800] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2162.369966506958, \"sum\": 2162.369966506958, \"min\": 2162.369966506958}}, \"EndTime\": 1592850117.345691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850115.182848}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.469658228 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=143, train loss <loss>=3.41056010723\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:57 INFO 140052404180800] Epoch[144] Batch[0] avg_epoch_loss=3.384408\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=3.38440847397\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:58 INFO 140052404180800] Epoch[144] Batch[5] avg_epoch_loss=3.380872\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=3.38087197145\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:58 INFO 140052404180800] Epoch[144] Batch [5]#011Speed: 330.81 samples/sec#011loss=3.380872\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:59 INFO 140052404180800] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2200.9668350219727, \"sum\": 2200.9668350219727, \"min\": 2200.9668350219727}}, \"EndTime\": 1592850119.547201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850117.345773}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.491196814 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=144, train loss <loss>=3.39880633354\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:21:59 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:00 INFO 140052404180800] Epoch[145] Batch[0] avg_epoch_loss=3.343408\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=3.34340763092\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:00 INFO 140052404180800] Epoch[145] Batch[5] avg_epoch_loss=3.375694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=3.37569411596\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:00 INFO 140052404180800] Epoch[145] Batch [5]#011Speed: 337.28 samples/sec#011loss=3.375694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:01 INFO 140052404180800] Epoch[145] Batch[10] avg_epoch_loss=3.426339\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=3.48711252213\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:01 INFO 140052404180800] Epoch[145] Batch [10]#011Speed: 335.46 samples/sec#011loss=3.487113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:01 INFO 140052404180800] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2386.1629962921143, \"sum\": 2386.1629962921143, \"min\": 2386.1629962921143}}, \"EndTime\": 1592850121.933927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850119.547304}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:01 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.381679976 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:01 INFO 140052404180800] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=145, train loss <loss>=3.42633884603\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:01 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:02 INFO 140052404180800] Epoch[146] Batch[0] avg_epoch_loss=3.439577\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=3.43957686424\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:03 INFO 140052404180800] Epoch[146] Batch[5] avg_epoch_loss=3.446095\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=3.44609506925\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:03 INFO 140052404180800] Epoch[146] Batch [5]#011Speed: 338.90 samples/sec#011loss=3.446095\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:04 INFO 140052404180800] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2221.6148376464844, \"sum\": 2221.6148376464844, \"min\": 2221.6148376464844}}, \"EndTime\": 1592850124.156056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850121.934006}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:04 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.961531672 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:04 INFO 140052404180800] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=146, train loss <loss>=3.41890952587\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:04 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:04 INFO 140052404180800] Epoch[147] Batch[0] avg_epoch_loss=3.454960\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=3.45496034622\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:05 INFO 140052404180800] Epoch[147] Batch[5] avg_epoch_loss=3.376420\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=3.37642002106\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:05 INFO 140052404180800] Epoch[147] Batch [5]#011Speed: 336.77 samples/sec#011loss=3.376420\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:06 INFO 140052404180800] processed a total of 578 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2197.9830265045166, \"sum\": 2197.9830265045166, \"min\": 2197.9830265045166}}, \"EndTime\": 1592850126.354612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850124.156138}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=262.954273349 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=147, train loss <loss>=3.46272323132\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:06 INFO 140052404180800] Epoch[148] Batch[0] avg_epoch_loss=3.413090\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=3.41309022903\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:07 INFO 140052404180800] Epoch[148] Batch[5] avg_epoch_loss=3.399036\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=3.39903581142\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:07 INFO 140052404180800] Epoch[148] Batch [5]#011Speed: 338.10 samples/sec#011loss=3.399036\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:08 INFO 140052404180800] Epoch[148] Batch[10] avg_epoch_loss=3.447606\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=3.50589032173\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:08 INFO 140052404180800] Epoch[148] Batch [10]#011Speed: 327.93 samples/sec#011loss=3.505890\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:08 INFO 140052404180800] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2400.886058807373, \"sum\": 2400.886058807373, \"min\": 2400.886058807373}}, \"EndTime\": 1592850128.75603, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850126.354693}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.135516965 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=148, train loss <loss>=3.44760604338\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:09 INFO 140052404180800] Epoch[149] Batch[0] avg_epoch_loss=3.251292\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=3.2512922287\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:10 INFO 140052404180800] Epoch[149] Batch[5] avg_epoch_loss=3.350456\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=3.35045619806\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:10 INFO 140052404180800] Epoch[149] Batch [5]#011Speed: 336.34 samples/sec#011loss=3.350456\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:10 INFO 140052404180800] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2228.2590866088867, \"sum\": 2228.2590866088867, \"min\": 2228.2590866088867}}, \"EndTime\": 1592850130.984799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850128.756105}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:10 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.409654923 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:10 INFO 140052404180800] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=149, train loss <loss>=3.36942837238\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:10 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:11 INFO 140052404180800] Epoch[150] Batch[0] avg_epoch_loss=3.399601\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=3.39960074425\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:12 INFO 140052404180800] Epoch[150] Batch[5] avg_epoch_loss=3.414695\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=3.41469510396\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:12 INFO 140052404180800] Epoch[150] Batch [5]#011Speed: 335.18 samples/sec#011loss=3.414695\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:13 INFO 140052404180800] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2217.761993408203, \"sum\": 2217.761993408203, \"min\": 2217.761993408203}}, \"EndTime\": 1592850133.203077, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850130.984881}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.390451118 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=150, train loss <loss>=3.33408970833\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:13 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:13 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_c771eaf4-f53f-4958-adcd-042408dd388c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.14995002746582, \"sum\": 66.14995002746582, \"min\": 66.14995002746582}}, \"EndTime\": 1592850133.269806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850133.203157}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:13 INFO 140052404180800] Epoch[151] Batch[0] avg_epoch_loss=3.479347\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=3.47934651375\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:14 INFO 140052404180800] Epoch[151] Batch[5] avg_epoch_loss=3.352602\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=3.35260208448\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:14 INFO 140052404180800] Epoch[151] Batch [5]#011Speed: 331.22 samples/sec#011loss=3.352602\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] Epoch[151] Batch[10] avg_epoch_loss=3.225799\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=3.07363529205\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] Epoch[151] Batch [10]#011Speed: 331.76 samples/sec#011loss=3.073635\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2386.582136154175, \"sum\": 2386.582136154175, \"min\": 2386.582136154175}}, \"EndTime\": 1592850135.656525, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850133.269874}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.181015827 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=151, train loss <loss>=3.22579899701\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:15 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_cb8460cc-61f1-41f5-8026-ee2f7de09609-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 68.20321083068848, \"sum\": 68.20321083068848, \"min\": 68.20321083068848}}, \"EndTime\": 1592850135.725279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850135.656601}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:16 INFO 140052404180800] Epoch[152] Batch[0] avg_epoch_loss=3.312456\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=3.31245565414\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:17 INFO 140052404180800] Epoch[152] Batch[5] avg_epoch_loss=3.417300\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=3.41729994615\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:17 INFO 140052404180800] Epoch[152] Batch [5]#011Speed: 332.37 samples/sec#011loss=3.417300\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] Epoch[152] Batch[10] avg_epoch_loss=3.469645\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=3.53245997429\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] Epoch[152] Batch [10]#011Speed: 332.03 samples/sec#011loss=3.532460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2385.664224624634, \"sum\": 2385.664224624634, \"min\": 2385.664224624634}}, \"EndTime\": 1592850138.111061, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850135.72534}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.772353866 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=152, train loss <loss>=3.46964541349\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] Epoch[153] Batch[0] avg_epoch_loss=3.398470\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=3.39847040176\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:19 INFO 140052404180800] Epoch[153] Batch[5] avg_epoch_loss=3.391853\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=3.39185305436\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:19 INFO 140052404180800] Epoch[153] Batch [5]#011Speed: 325.31 samples/sec#011loss=3.391853\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:20 INFO 140052404180800] Epoch[153] Batch[10] avg_epoch_loss=3.469828\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=3.56339683533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:20 INFO 140052404180800] Epoch[153] Batch [10]#011Speed: 325.88 samples/sec#011loss=3.563397\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:20 INFO 140052404180800] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2430.294990539551, \"sum\": 2430.294990539551, \"min\": 2430.294990539551}}, \"EndTime\": 1592850140.541896, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850138.111128}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.674121905 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=153, train loss <loss>=3.46982750026\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:21 INFO 140052404180800] Epoch[154] Batch[0] avg_epoch_loss=3.347995\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=3.3479950428\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:21 INFO 140052404180800] Epoch[154] Batch[5] avg_epoch_loss=3.318570\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=3.31857033571\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:21 INFO 140052404180800] Epoch[154] Batch [5]#011Speed: 335.66 samples/sec#011loss=3.318570\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:22 INFO 140052404180800] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2187.4279975891113, \"sum\": 2187.4279975891113, \"min\": 2187.4279975891113}}, \"EndTime\": 1592850142.729816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850140.541972}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=292.10788643 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=154, train loss <loss>=3.36437504292\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:23 INFO 140052404180800] Epoch[155] Batch[0] avg_epoch_loss=3.285300\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=3.28529977798\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:24 INFO 140052404180800] Epoch[155] Batch[5] avg_epoch_loss=3.425866\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=3.42586561044\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:24 INFO 140052404180800] Epoch[155] Batch [5]#011Speed: 304.46 samples/sec#011loss=3.425866\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] Epoch[155] Batch[10] avg_epoch_loss=3.451454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=3.4821600914\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] Epoch[155] Batch [10]#011Speed: 332.68 samples/sec#011loss=3.482160\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2479.71510887146, \"sum\": 2479.71510887146, \"min\": 2479.71510887146}}, \"EndTime\": 1592850145.210079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850142.729899}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.776891551 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=155, train loss <loss>=3.45145401088\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] Epoch[156] Batch[0] avg_epoch_loss=3.227448\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=3.22744774818\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:26 INFO 140052404180800] Epoch[156] Batch[5] avg_epoch_loss=3.343640\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=3.3436404864\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:26 INFO 140052404180800] Epoch[156] Batch [5]#011Speed: 337.87 samples/sec#011loss=3.343640\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:27 INFO 140052404180800] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2175.395965576172, \"sum\": 2175.395965576172, \"min\": 2175.395965576172}}, \"EndTime\": 1592850147.386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850145.210155}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.611263986 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=156, train loss <loss>=3.3040191412\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:27 INFO 140052404180800] Epoch[157] Batch[0] avg_epoch_loss=3.357444\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=3.35744357109\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:28 INFO 140052404180800] Epoch[157] Batch[5] avg_epoch_loss=3.393909\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=3.3939088583\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:28 INFO 140052404180800] Epoch[157] Batch [5]#011Speed: 336.70 samples/sec#011loss=3.393909\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:29 INFO 140052404180800] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2234.5101833343506, \"sum\": 2234.5101833343506, \"min\": 2234.5101833343506}}, \"EndTime\": 1592850149.621039, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850147.38608}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.031480267 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=157, train loss <loss>=3.37969138622\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:29 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:30 INFO 140052404180800] Epoch[158] Batch[0] avg_epoch_loss=3.564347\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=3.56434726715\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:31 INFO 140052404180800] Epoch[158] Batch[5] avg_epoch_loss=3.430198\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=3.43019831181\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:31 INFO 140052404180800] Epoch[158] Batch [5]#011Speed: 332.50 samples/sec#011loss=3.430198\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:31 INFO 140052404180800] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2184.378147125244, \"sum\": 2184.378147125244, \"min\": 2184.378147125244}}, \"EndTime\": 1592850151.805992, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850149.621119}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:31 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.495501283 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:31 INFO 140052404180800] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=158, train loss <loss>=3.3536762476\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:31 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:32 INFO 140052404180800] Epoch[159] Batch[0] avg_epoch_loss=3.475533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=3.47553348541\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:33 INFO 140052404180800] Epoch[159] Batch[5] avg_epoch_loss=3.395257\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=3.39525659879\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:33 INFO 140052404180800] Epoch[159] Batch [5]#011Speed: 332.65 samples/sec#011loss=3.395257\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] Epoch[159] Batch[10] avg_epoch_loss=3.324072\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=3.23865156174\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] Epoch[159] Batch [10]#011Speed: 325.77 samples/sec#011loss=3.238652\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2414.168119430542, \"sum\": 2414.168119430542, \"min\": 2414.168119430542}}, \"EndTime\": 1592850154.22072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850151.806067}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.486915164 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=159, train loss <loss>=3.32407249104\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] Epoch[160] Batch[0] avg_epoch_loss=3.131211\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=3.13121128082\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:35 INFO 140052404180800] Epoch[160] Batch[5] avg_epoch_loss=3.349005\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=3.34900534153\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:35 INFO 140052404180800] Epoch[160] Batch [5]#011Speed: 327.43 samples/sec#011loss=3.349005\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:36 INFO 140052404180800] Epoch[160] Batch[10] avg_epoch_loss=3.441896\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=3.55336456299\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:36 INFO 140052404180800] Epoch[160] Batch [10]#011Speed: 332.22 samples/sec#011loss=3.553365\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:36 INFO 140052404180800] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2403.1288623809814, \"sum\": 2403.1288623809814, \"min\": 2403.1288623809814}}, \"EndTime\": 1592850156.62432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850154.220787}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.96540342 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=160, train loss <loss>=3.44189589674\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:36 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:37 INFO 140052404180800] Epoch[161] Batch[0] avg_epoch_loss=3.336815\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=3.33681511879\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:38 INFO 140052404180800] Epoch[161] Batch[5] avg_epoch_loss=3.304454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=3.30445432663\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:38 INFO 140052404180800] Epoch[161] Batch [5]#011Speed: 334.90 samples/sec#011loss=3.304454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:38 INFO 140052404180800] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2183.943033218384, \"sum\": 2183.943033218384, \"min\": 2183.943033218384}}, \"EndTime\": 1592850158.808846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850156.624397}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:38 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=290.742280664 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:38 INFO 140052404180800] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=161, train loss <loss>=3.28093402386\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:38 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:39 INFO 140052404180800] Epoch[162] Batch[0] avg_epoch_loss=3.295267\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=3.2952671051\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:40 INFO 140052404180800] Epoch[162] Batch[5] avg_epoch_loss=3.307637\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=3.30763673782\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:40 INFO 140052404180800] Epoch[162] Batch [5]#011Speed: 331.07 samples/sec#011loss=3.307637\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:40 INFO 140052404180800] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2188.3280277252197, \"sum\": 2188.3280277252197, \"min\": 2188.3280277252197}}, \"EndTime\": 1592850160.997723, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850158.808927}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:40 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.193419074 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:40 INFO 140052404180800] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=162, train loss <loss>=3.36532890797\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:40 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:41 INFO 140052404180800] Epoch[163] Batch[0] avg_epoch_loss=3.404904\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=3.40490436554\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:42 INFO 140052404180800] Epoch[163] Batch[5] avg_epoch_loss=3.347832\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=3.34783244133\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:42 INFO 140052404180800] Epoch[163] Batch [5]#011Speed: 335.04 samples/sec#011loss=3.347832\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:43 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2190.5388832092285, \"sum\": 2190.5388832092285, \"min\": 2190.5388832092285}}, \"EndTime\": 1592850163.188797, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850160.997804}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.954548497 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=163, train loss <loss>=3.38431229591\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:43 INFO 140052404180800] Epoch[164] Batch[0] avg_epoch_loss=3.534229\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=3.53422927856\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:44 INFO 140052404180800] Epoch[164] Batch[5] avg_epoch_loss=3.299123\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=3.29912348588\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:44 INFO 140052404180800] Epoch[164] Batch [5]#011Speed: 320.99 samples/sec#011loss=3.299123\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:45 INFO 140052404180800] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2252.9091835021973, \"sum\": 2252.9091835021973, \"min\": 2252.9091835021973}}, \"EndTime\": 1592850165.442268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850163.188879}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.17597452 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=164, train loss <loss>=3.30034637451\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:45 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:45 INFO 140052404180800] Epoch[165] Batch[0] avg_epoch_loss=3.487319\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=3.48731851578\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:46 INFO 140052404180800] Epoch[165] Batch[5] avg_epoch_loss=3.278855\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=3.27885520458\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:46 INFO 140052404180800] Epoch[165] Batch [5]#011Speed: 337.53 samples/sec#011loss=3.278855\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:47 INFO 140052404180800] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2142.0040130615234, \"sum\": 2142.0040130615234, \"min\": 2142.0040130615234}}, \"EndTime\": 1592850167.584865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850165.442344}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:47 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=293.166683733 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:47 INFO 140052404180800] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=165, train loss <loss>=3.34146318436\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:47 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:48 INFO 140052404180800] Epoch[166] Batch[0] avg_epoch_loss=3.251758\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=3.2517580986\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] Epoch[166] Batch[5] avg_epoch_loss=3.293705\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=3.29370474815\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] Epoch[166] Batch [5]#011Speed: 335.21 samples/sec#011loss=3.293705\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] Epoch[166] Batch[10] avg_epoch_loss=3.307366\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=3.3237590313\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] Epoch[166] Batch [10]#011Speed: 325.42 samples/sec#011loss=3.323759\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2400.4969596862793, \"sum\": 2400.4969596862793, \"min\": 2400.4969596862793}}, \"EndTime\": 1592850169.985936, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850167.584949}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.431085257 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=166, train loss <loss>=3.30736578595\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:49 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:50 INFO 140052404180800] Epoch[167] Batch[0] avg_epoch_loss=3.190251\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=3.19025063515\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:51 INFO 140052404180800] Epoch[167] Batch[5] avg_epoch_loss=3.291130\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=3.29113014539\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:51 INFO 140052404180800] Epoch[167] Batch [5]#011Speed: 325.48 samples/sec#011loss=3.291130\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] Epoch[167] Batch[10] avg_epoch_loss=3.380796\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=3.48839406967\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] Epoch[167] Batch [10]#011Speed: 336.33 samples/sec#011loss=3.488394\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2405.2910804748535, \"sum\": 2405.2910804748535, \"min\": 2405.2910804748535}}, \"EndTime\": 1592850172.391755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850169.98601}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.955098028 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=167, train loss <loss>=3.38079556552\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] Epoch[168] Batch[0] avg_epoch_loss=3.439013\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=3.43901276588\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:53 INFO 140052404180800] Epoch[168] Batch[5] avg_epoch_loss=3.431130\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=3.43113044898\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:53 INFO 140052404180800] Epoch[168] Batch [5]#011Speed: 321.76 samples/sec#011loss=3.431130\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:54 INFO 140052404180800] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2267.1689987182617, \"sum\": 2267.1689987182617, \"min\": 2267.1689987182617}}, \"EndTime\": 1592850174.659416, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850172.391832}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:54 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.62899956 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:54 INFO 140052404180800] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=168, train loss <loss>=3.46673612595\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:54 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:55 INFO 140052404180800] Epoch[169] Batch[0] avg_epoch_loss=3.358495\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=3.35849523544\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:56 INFO 140052404180800] Epoch[169] Batch[5] avg_epoch_loss=3.422557\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=3.4225568374\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:56 INFO 140052404180800] Epoch[169] Batch [5]#011Speed: 335.02 samples/sec#011loss=3.422557\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] Epoch[169] Batch[10] avg_epoch_loss=3.360169\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=3.28530364037\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] Epoch[169] Batch [10]#011Speed: 335.47 samples/sec#011loss=3.285304\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2405.2791595458984, \"sum\": 2405.2791595458984, \"min\": 2405.2791595458984}}, \"EndTime\": 1592850177.065234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850174.659499}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.473968931 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=169, train loss <loss>=3.36016902057\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] Epoch[170] Batch[0] avg_epoch_loss=3.348310\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=3.34831047058\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:58 INFO 140052404180800] Epoch[170] Batch[5] avg_epoch_loss=3.373268\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=3.37326844533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:58 INFO 140052404180800] Epoch[170] Batch [5]#011Speed: 334.00 samples/sec#011loss=3.373268\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:59 INFO 140052404180800] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2248.589038848877, \"sum\": 2248.589038848877, \"min\": 2248.589038848877}}, \"EndTime\": 1592850179.314336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850177.065308}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.827394806 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=170, train loss <loss>=3.322154212\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:59 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:59 INFO 140052404180800] Epoch[171] Batch[0] avg_epoch_loss=3.254022\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:22:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=3.25402188301\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:00 INFO 140052404180800] Epoch[171] Batch[5] avg_epoch_loss=3.412414\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=3.41241443157\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:00 INFO 140052404180800] Epoch[171] Batch [5]#011Speed: 335.50 samples/sec#011loss=3.412414\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:01 INFO 140052404180800] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2246.7257976531982, \"sum\": 2246.7257976531982, \"min\": 2246.7257976531982}}, \"EndTime\": 1592850181.561709, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850179.314414}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:01 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.063703987 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:01 INFO 140052404180800] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=171, train loss <loss>=3.42389845848\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:01 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:02 INFO 140052404180800] Epoch[172] Batch[0] avg_epoch_loss=3.337196\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=3.33719563484\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:03 INFO 140052404180800] Epoch[172] Batch[5] avg_epoch_loss=3.318795\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=3.31879472733\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:03 INFO 140052404180800] Epoch[172] Batch [5]#011Speed: 330.44 samples/sec#011loss=3.318795\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:03 INFO 140052404180800] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2209.296941757202, \"sum\": 2209.296941757202, \"min\": 2209.296941757202}}, \"EndTime\": 1592850183.771533, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850181.56179}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:03 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=262.511261175 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:03 INFO 140052404180800] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=172, train loss <loss>=3.41448202133\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:03 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:04 INFO 140052404180800] Epoch[173] Batch[0] avg_epoch_loss=3.390370\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=3.39037013054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:05 INFO 140052404180800] Epoch[173] Batch[5] avg_epoch_loss=3.334636\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=3.33463585377\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:05 INFO 140052404180800] Epoch[173] Batch [5]#011Speed: 329.15 samples/sec#011loss=3.334636\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] Epoch[173] Batch[10] avg_epoch_loss=3.269666\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=173, batch=10 train loss <loss>=3.19170145988\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] Epoch[173] Batch [10]#011Speed: 334.18 samples/sec#011loss=3.191701\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2422.6410388946533, \"sum\": 2422.6410388946533, \"min\": 2422.6410388946533}}, \"EndTime\": 1592850186.194709, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850183.771629}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=264.987925121 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=173, train loss <loss>=3.26966567473\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] Epoch[174] Batch[0] avg_epoch_loss=3.428657\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=3.42865681648\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:07 INFO 140052404180800] Epoch[174] Batch[5] avg_epoch_loss=3.378795\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=3.37879506747\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:07 INFO 140052404180800] Epoch[174] Batch [5]#011Speed: 335.96 samples/sec#011loss=3.378795\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:08 INFO 140052404180800] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2194.2319869995117, \"sum\": 2194.2319869995117, \"min\": 2194.2319869995117}}, \"EndTime\": 1592850188.389445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850186.194785}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.704652981 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=174, train loss <loss>=3.41106717587\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:08 INFO 140052404180800] Epoch[175] Batch[0] avg_epoch_loss=3.336899\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=3.33689856529\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:09 INFO 140052404180800] Epoch[175] Batch[5] avg_epoch_loss=3.377546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=3.37754627069\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:09 INFO 140052404180800] Epoch[175] Batch [5]#011Speed: 326.27 samples/sec#011loss=3.377546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:10 INFO 140052404180800] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2261.1868381500244, \"sum\": 2261.1868381500244, \"min\": 2261.1868381500244}}, \"EndTime\": 1592850190.651181, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850188.389552}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:10 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.370693125 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:10 INFO 140052404180800] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=175, train loss <loss>=3.38447396755\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:10 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:11 INFO 140052404180800] Epoch[176] Batch[0] avg_epoch_loss=3.318885\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=3.31888532639\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:12 INFO 140052404180800] Epoch[176] Batch[5] avg_epoch_loss=3.329763\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=3.32976317406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:12 INFO 140052404180800] Epoch[176] Batch [5]#011Speed: 335.61 samples/sec#011loss=3.329763\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] Epoch[176] Batch[10] avg_epoch_loss=3.311206\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=3.28893752098\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] Epoch[176] Batch [10]#011Speed: 336.69 samples/sec#011loss=3.288938\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2350.6059646606445, \"sum\": 2350.6059646606445, \"min\": 2350.6059646606445}}, \"EndTime\": 1592850193.002346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850190.651254}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.996877775 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=176, train loss <loss>=3.31120605902\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] Epoch[177] Batch[0] avg_epoch_loss=3.328310\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=3.32831001282\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:14 INFO 140052404180800] Epoch[177] Batch[5] avg_epoch_loss=3.423568\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=3.42356832822\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:14 INFO 140052404180800] Epoch[177] Batch [5]#011Speed: 329.30 samples/sec#011loss=3.423568\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] Epoch[177] Batch[10] avg_epoch_loss=3.468899\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=3.52329478264\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] Epoch[177] Batch [10]#011Speed: 330.40 samples/sec#011loss=3.523295\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2425.671100616455, \"sum\": 2425.671100616455, \"min\": 2425.671100616455}}, \"EndTime\": 1592850195.428518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850193.002425}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.376473365 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=177, train loss <loss>=3.46889853477\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] Epoch[178] Batch[0] avg_epoch_loss=3.203490\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=3.20348954201\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:16 INFO 140052404180800] Epoch[178] Batch[5] avg_epoch_loss=3.239577\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=3.23957657814\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:16 INFO 140052404180800] Epoch[178] Batch [5]#011Speed: 335.92 samples/sec#011loss=3.239577\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:17 INFO 140052404180800] Epoch[178] Batch[10] avg_epoch_loss=3.339641\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=3.4597173214\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:17 INFO 140052404180800] Epoch[178] Batch [10]#011Speed: 336.20 samples/sec#011loss=3.459717\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:17 INFO 140052404180800] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2370.950937271118, \"sum\": 2370.950937271118, \"min\": 2370.950937271118}}, \"EndTime\": 1592850197.799997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850195.42858}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:17 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.090908872 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:17 INFO 140052404180800] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=178, train loss <loss>=3.33964055235\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:17 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:18 INFO 140052404180800] Epoch[179] Batch[0] avg_epoch_loss=3.361462\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=3.3614616394\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:19 INFO 140052404180800] Epoch[179] Batch[5] avg_epoch_loss=3.298562\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=3.29856197039\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:19 INFO 140052404180800] Epoch[179] Batch [5]#011Speed: 337.37 samples/sec#011loss=3.298562\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] Epoch[179] Batch[10] avg_epoch_loss=3.233645\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=3.15574407578\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] Epoch[179] Batch [10]#011Speed: 331.74 samples/sec#011loss=3.155744\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2396.3890075683594, \"sum\": 2396.3890075683594, \"min\": 2396.3890075683594}}, \"EndTime\": 1592850200.196875, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850197.800072}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.890688185 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=179, train loss <loss>=3.23364474557\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] Epoch[180] Batch[0] avg_epoch_loss=3.373559\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=3.37355852127\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:21 INFO 140052404180800] Epoch[180] Batch[5] avg_epoch_loss=3.338996\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=3.33899629116\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:21 INFO 140052404180800] Epoch[180] Batch [5]#011Speed: 336.58 samples/sec#011loss=3.338996\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:22 INFO 140052404180800] Epoch[180] Batch[10] avg_epoch_loss=3.315619\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=3.28756580353\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:22 INFO 140052404180800] Epoch[180] Batch [10]#011Speed: 333.49 samples/sec#011loss=3.287566\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:22 INFO 140052404180800] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2417.7420139312744, \"sum\": 2417.7420139312744, \"min\": 2417.7420139312744}}, \"EndTime\": 1592850202.615118, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850200.196951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.03353499 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=180, train loss <loss>=3.31561879678\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:23 INFO 140052404180800] Epoch[181] Batch[0] avg_epoch_loss=3.488080\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=3.48808026314\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:24 INFO 140052404180800] Epoch[181] Batch[5] avg_epoch_loss=3.296564\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=3.29656426112\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:24 INFO 140052404180800] Epoch[181] Batch [5]#011Speed: 323.85 samples/sec#011loss=3.296564\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:24 INFO 140052404180800] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2242.3298358917236, \"sum\": 2242.3298358917236, \"min\": 2242.3298358917236}}, \"EndTime\": 1592850204.85799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850202.615173}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:24 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.025232973 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:24 INFO 140052404180800] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=181, train loss <loss>=3.24585936069\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:24 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:25 INFO 140052404180800] Epoch[182] Batch[0] avg_epoch_loss=3.218374\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=3.21837353706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:26 INFO 140052404180800] Epoch[182] Batch[5] avg_epoch_loss=3.288536\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=3.2885358731\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:26 INFO 140052404180800] Epoch[182] Batch [5]#011Speed: 331.23 samples/sec#011loss=3.288536\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:27 INFO 140052404180800] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2183.344841003418, \"sum\": 2183.344841003418, \"min\": 2183.344841003418}}, \"EndTime\": 1592850207.041917, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850204.858067}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.990584077 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=182, train loss <loss>=3.2747961998\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:27 INFO 140052404180800] Epoch[183] Batch[0] avg_epoch_loss=3.267025\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=3.26702451706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:28 INFO 140052404180800] Epoch[183] Batch[5] avg_epoch_loss=3.314583\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=3.31458298365\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:28 INFO 140052404180800] Epoch[183] Batch [5]#011Speed: 337.34 samples/sec#011loss=3.314583\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] Epoch[183] Batch[10] avg_epoch_loss=3.403432\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=3.51005148888\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] Epoch[183] Batch [10]#011Speed: 331.83 samples/sec#011loss=3.510051\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2369.6470260620117, \"sum\": 2369.6470260620117, \"min\": 2369.6470260620117}}, \"EndTime\": 1592850209.412112, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850207.041998}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.928248916 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=183, train loss <loss>=3.40343230421\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] Epoch[184] Batch[0] avg_epoch_loss=3.218974\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=3.21897387505\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:30 INFO 140052404180800] Epoch[184] Batch[5] avg_epoch_loss=3.331454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=3.33145435651\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:30 INFO 140052404180800] Epoch[184] Batch [5]#011Speed: 336.08 samples/sec#011loss=3.331454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:31 INFO 140052404180800] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2248.0578422546387, \"sum\": 2248.0578422546387, \"min\": 2248.0578422546387}}, \"EndTime\": 1592850211.660826, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850209.412192}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:31 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.110046702 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:31 INFO 140052404180800] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=184, train loss <loss>=3.3595375061\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:31 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:32 INFO 140052404180800] Epoch[185] Batch[0] avg_epoch_loss=3.314504\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=3.31450366974\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:33 INFO 140052404180800] Epoch[185] Batch[5] avg_epoch_loss=3.295712\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=3.29571151733\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:33 INFO 140052404180800] Epoch[185] Batch [5]#011Speed: 334.05 samples/sec#011loss=3.295712\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:33 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2209.307909011841, \"sum\": 2209.307909011841, \"min\": 2209.307909011841}}, \"EndTime\": 1592850213.870681, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850211.660908}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:33 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.502605399 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:33 INFO 140052404180800] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=185, train loss <loss>=3.27988352776\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:33 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:34 INFO 140052404180800] Epoch[186] Batch[0] avg_epoch_loss=3.363819\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=3.36381936073\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:35 INFO 140052404180800] Epoch[186] Batch[5] avg_epoch_loss=3.343569\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=3.34356911977\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:35 INFO 140052404180800] Epoch[186] Batch [5]#011Speed: 330.00 samples/sec#011loss=3.343569\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] Epoch[186] Batch[10] avg_epoch_loss=3.212907\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=3.05611314774\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] Epoch[186] Batch [10]#011Speed: 330.40 samples/sec#011loss=3.056113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2380.6469440460205, \"sum\": 2380.6469440460205, \"min\": 2380.6469440460205}}, \"EndTime\": 1592850216.251896, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850213.870743}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.802010566 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=186, train loss <loss>=3.2129073143\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_8e8f5db6-4b1a-4035-9874-225c5dd3fd5c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 62.12496757507324, \"sum\": 62.12496757507324, \"min\": 62.12496757507324}}, \"EndTime\": 1592850216.314563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850216.251971}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] Epoch[187] Batch[0] avg_epoch_loss=3.335546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=3.33554577827\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:37 INFO 140052404180800] Epoch[187] Batch[5] avg_epoch_loss=3.376140\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=3.37614023685\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:37 INFO 140052404180800] Epoch[187] Batch [5]#011Speed: 335.00 samples/sec#011loss=3.376140\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:38 INFO 140052404180800] Epoch[187] Batch[10] avg_epoch_loss=3.364617\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=3.3507897377\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:38 INFO 140052404180800] Epoch[187] Batch [10]#011Speed: 331.85 samples/sec#011loss=3.350790\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:38 INFO 140052404180800] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2392.0538425445557, \"sum\": 2392.0538425445557, \"min\": 2392.0538425445557}}, \"EndTime\": 1592850218.706763, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850216.31464}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:38 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.187783196 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:38 INFO 140052404180800] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=187, train loss <loss>=3.36461728269\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:38 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:39 INFO 140052404180800] Epoch[188] Batch[0] avg_epoch_loss=3.152476\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=3.15247583389\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:40 INFO 140052404180800] Epoch[188] Batch[5] avg_epoch_loss=3.251816\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=3.25181639194\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:40 INFO 140052404180800] Epoch[188] Batch [5]#011Speed: 323.92 samples/sec#011loss=3.251816\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] Epoch[188] Batch[10] avg_epoch_loss=3.389814\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=3.55541205406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] Epoch[188] Batch [10]#011Speed: 329.70 samples/sec#011loss=3.555412\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2421.787977218628, \"sum\": 2421.787977218628, \"min\": 2421.787977218628}}, \"EndTime\": 1592850221.129102, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850218.706831}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=266.730626145 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=188, train loss <loss>=3.38981442018\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] Epoch[189] Batch[0] avg_epoch_loss=3.257489\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=3.25748872757\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:42 INFO 140052404180800] Epoch[189] Batch[5] avg_epoch_loss=3.345068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=3.3450678587\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:42 INFO 140052404180800] Epoch[189] Batch [5]#011Speed: 334.35 samples/sec#011loss=3.345068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] Epoch[189] Batch[10] avg_epoch_loss=3.408825\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=3.4853345871\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] Epoch[189] Batch [10]#011Speed: 327.19 samples/sec#011loss=3.485335\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2396.631956100464, \"sum\": 2396.631956100464, \"min\": 2396.631956100464}}, \"EndTime\": 1592850223.526313, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850221.129197}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.533456863 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=189, train loss <loss>=3.40882546251\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] Epoch[190] Batch[0] avg_epoch_loss=3.415271\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=3.41527056694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:44 INFO 140052404180800] Epoch[190] Batch[5] avg_epoch_loss=3.282888\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=3.28288761775\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:44 INFO 140052404180800] Epoch[190] Batch [5]#011Speed: 326.20 samples/sec#011loss=3.282888\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:45 INFO 140052404180800] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2224.905014038086, \"sum\": 2224.905014038086, \"min\": 2224.905014038086}}, \"EndTime\": 1592850225.751735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850223.52638}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.750826824 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=190, train loss <loss>=3.27525599003\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:45 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:46 INFO 140052404180800] Epoch[191] Batch[0] avg_epoch_loss=3.346694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=3.34669423103\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:47 INFO 140052404180800] Epoch[191] Batch[5] avg_epoch_loss=3.330120\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=3.33012012641\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:47 INFO 140052404180800] Epoch[191] Batch [5]#011Speed: 335.16 samples/sec#011loss=3.330120\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:47 INFO 140052404180800] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2180.9589862823486, \"sum\": 2180.9589862823486, \"min\": 2180.9589862823486}}, \"EndTime\": 1592850227.933285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850225.751805}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:47 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.681552078 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:47 INFO 140052404180800] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=191, train loss <loss>=3.2812057972\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:47 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:48 INFO 140052404180800] Epoch[192] Batch[0] avg_epoch_loss=3.385612\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=3.38561224937\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:49 INFO 140052404180800] Epoch[192] Batch[5] avg_epoch_loss=3.302874\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=3.30287428697\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:49 INFO 140052404180800] Epoch[192] Batch [5]#011Speed: 322.85 samples/sec#011loss=3.302874\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:50 INFO 140052404180800] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2226.13787651062, \"sum\": 2226.13787651062, \"min\": 2226.13787651062}}, \"EndTime\": 1592850230.160018, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850227.933346}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.553458846 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=192, train loss <loss>=3.24277434349\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:50 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:50 INFO 140052404180800] Epoch[193] Batch[0] avg_epoch_loss=3.373970\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=3.37397003174\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:51 INFO 140052404180800] Epoch[193] Batch[5] avg_epoch_loss=3.292003\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=3.29200323423\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:51 INFO 140052404180800] Epoch[193] Batch [5]#011Speed: 338.76 samples/sec#011loss=3.292003\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:52 INFO 140052404180800] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2217.9689407348633, \"sum\": 2217.9689407348633, \"min\": 2217.9689407348633}}, \"EndTime\": 1592850232.378524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850230.160099}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.675628032 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=193, train loss <loss>=3.31706404686\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:52 INFO 140052404180800] Epoch[194] Batch[0] avg_epoch_loss=3.286327\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=3.28632736206\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:53 INFO 140052404180800] Epoch[194] Batch[5] avg_epoch_loss=3.256553\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=3.25655273596\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:53 INFO 140052404180800] Epoch[194] Batch [5]#011Speed: 338.07 samples/sec#011loss=3.256553\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:54 INFO 140052404180800] Epoch[194] Batch[10] avg_epoch_loss=3.259959\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=3.26404747963\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:54 INFO 140052404180800] Epoch[194] Batch [10]#011Speed: 321.22 samples/sec#011loss=3.264047\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:54 INFO 140052404180800] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2423.809051513672, \"sum\": 2423.809051513672, \"min\": 2423.809051513672}}, \"EndTime\": 1592850234.802872, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850232.378606}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:54 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.223449065 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:54 INFO 140052404180800] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=194, train loss <loss>=3.25995943763\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:54 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:55 INFO 140052404180800] Epoch[195] Batch[0] avg_epoch_loss=3.290877\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=3.29087662697\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:56 INFO 140052404180800] Epoch[195] Batch[5] avg_epoch_loss=3.389827\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=3.38982685407\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:56 INFO 140052404180800] Epoch[195] Batch [5]#011Speed: 330.64 samples/sec#011loss=3.389827\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] Epoch[195] Batch[10] avg_epoch_loss=3.355118\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=3.31346831322\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] Epoch[195] Batch [10]#011Speed: 335.61 samples/sec#011loss=3.313468\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2402.815103530884, \"sum\": 2402.815103530884, \"min\": 2402.815103530884}}, \"EndTime\": 1592850237.206195, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850234.802948}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.82064576 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=195, train loss <loss>=3.35511842641\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] Epoch[196] Batch[0] avg_epoch_loss=3.194084\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=3.1940844059\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:58 INFO 140052404180800] Epoch[196] Batch[5] avg_epoch_loss=3.229337\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=3.22933693727\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:58 INFO 140052404180800] Epoch[196] Batch [5]#011Speed: 337.38 samples/sec#011loss=3.229337\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] Epoch[196] Batch[10] avg_epoch_loss=3.165239\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=3.08832097054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] Epoch[196] Batch [10]#011Speed: 328.29 samples/sec#011loss=3.088321\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2402.829885482788, \"sum\": 2402.829885482788, \"min\": 2402.829885482788}}, \"EndTime\": 1592850239.609515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850237.20627}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.246940943 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=196, train loss <loss>=3.16523877057\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:23:59 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_ebd53f30-b932-486c-b24f-d4462a39aec9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 98.94585609436035, \"sum\": 98.94585609436035, \"min\": 98.94585609436035}}, \"EndTime\": 1592850239.709038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850239.609592}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:00 INFO 140052404180800] Epoch[197] Batch[0] avg_epoch_loss=3.358230\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=3.35823011398\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:01 INFO 140052404180800] Epoch[197] Batch[5] avg_epoch_loss=3.282143\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=3.28214295705\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:01 INFO 140052404180800] Epoch[197] Batch [5]#011Speed: 336.88 samples/sec#011loss=3.282143\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] Epoch[197] Batch[10] avg_epoch_loss=3.236715\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=197, batch=10 train loss <loss>=3.18220086098\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] Epoch[197] Batch [10]#011Speed: 329.95 samples/sec#011loss=3.182201\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2397.339105606079, \"sum\": 2397.339105606079, \"min\": 2397.339105606079}}, \"EndTime\": 1592850242.106515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850239.709114}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.704966353 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=197, train loss <loss>=3.23671473156\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] Epoch[198] Batch[0] avg_epoch_loss=3.085988\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=3.08598804474\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:03 INFO 140052404180800] Epoch[198] Batch[5] avg_epoch_loss=3.242608\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=3.24260834853\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:03 INFO 140052404180800] Epoch[198] Batch [5]#011Speed: 337.60 samples/sec#011loss=3.242608\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:04 INFO 140052404180800] Epoch[198] Batch[10] avg_epoch_loss=3.328404\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=198, batch=10 train loss <loss>=3.43135876656\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:04 INFO 140052404180800] Epoch[198] Batch [10]#011Speed: 332.68 samples/sec#011loss=3.431359\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:04 INFO 140052404180800] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2411.435127258301, \"sum\": 2411.435127258301, \"min\": 2411.435127258301}}, \"EndTime\": 1592850244.518416, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850242.106585}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:04 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.048502283 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:04 INFO 140052404180800] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=198, train loss <loss>=3.32840399309\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:04 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:05 INFO 140052404180800] Epoch[199] Batch[0] avg_epoch_loss=3.264803\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=3.26480340958\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:05 INFO 140052404180800] Epoch[199] Batch[5] avg_epoch_loss=3.263826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=3.26382565498\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:05 INFO 140052404180800] Epoch[199] Batch [5]#011Speed: 332.09 samples/sec#011loss=3.263826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:06 INFO 140052404180800] Epoch[199] Batch[10] avg_epoch_loss=3.352717\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=199, batch=10 train loss <loss>=3.45938763618\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:06 INFO 140052404180800] Epoch[199] Batch [10]#011Speed: 329.04 samples/sec#011loss=3.459388\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:06 INFO 140052404180800] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2426.10502243042, \"sum\": 2426.10502243042, \"min\": 2426.10502243042}}, \"EndTime\": 1592850246.945021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850244.518493}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.55466676 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=199, train loss <loss>=3.35271746462\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:07 INFO 140052404180800] Epoch[200] Batch[0] avg_epoch_loss=3.147965\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=3.14796495438\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:08 INFO 140052404180800] Epoch[200] Batch[5] avg_epoch_loss=3.268926\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=3.26892622312\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:08 INFO 140052404180800] Epoch[200] Batch [5]#011Speed: 337.46 samples/sec#011loss=3.268926\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] Epoch[200] Batch[10] avg_epoch_loss=3.257208\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=200, batch=10 train loss <loss>=3.24314718246\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] Epoch[200] Batch [10]#011Speed: 328.19 samples/sec#011loss=3.243147\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2388.054847717285, \"sum\": 2388.054847717285, \"min\": 2388.054847717285}}, \"EndTime\": 1592850249.333626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850246.945101}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.337792221 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=200, train loss <loss>=3.25720847737\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] Epoch[201] Batch[0] avg_epoch_loss=2.985600\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=2.9856004715\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:10 INFO 140052404180800] Epoch[201] Batch[5] avg_epoch_loss=3.244085\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=3.24408483505\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:10 INFO 140052404180800] Epoch[201] Batch [5]#011Speed: 334.47 samples/sec#011loss=3.244085\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:11 INFO 140052404180800] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2228.652000427246, \"sum\": 2228.652000427246, \"min\": 2228.652000427246}}, \"EndTime\": 1592850251.562765, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850249.333702}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:11 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.705374142 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:11 INFO 140052404180800] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=201, train loss <loss>=3.29439971447\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:11 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:12 INFO 140052404180800] Epoch[202] Batch[0] avg_epoch_loss=3.287425\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=3.28742527962\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:12 INFO 140052404180800] Epoch[202] Batch[5] avg_epoch_loss=3.306393\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=3.30639310678\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:12 INFO 140052404180800] Epoch[202] Batch [5]#011Speed: 334.43 samples/sec#011loss=3.306393\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:13 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2184.5850944519043, \"sum\": 2184.5850944519043, \"min\": 2184.5850944519043}}, \"EndTime\": 1592850253.74788, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850251.562846}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.743625136 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=202, train loss <loss>=3.2727243185\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:13 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:14 INFO 140052404180800] Epoch[203] Batch[0] avg_epoch_loss=3.191377\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=3.19137716293\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:15 INFO 140052404180800] Epoch[203] Batch[5] avg_epoch_loss=3.291703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=3.29170314471\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:15 INFO 140052404180800] Epoch[203] Batch [5]#011Speed: 328.74 samples/sec#011loss=3.291703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] Epoch[203] Batch[10] avg_epoch_loss=3.232486\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=203, batch=10 train loss <loss>=3.16142435074\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] Epoch[203] Batch [10]#011Speed: 325.39 samples/sec#011loss=3.161424\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2413.551092147827, \"sum\": 2413.551092147827, \"min\": 2413.551092147827}}, \"EndTime\": 1592850256.161997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850253.747951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=265.570470167 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=203, train loss <loss>=3.23248551109\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] Epoch[204] Batch[0] avg_epoch_loss=3.277668\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=3.27766823769\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:17 INFO 140052404180800] Epoch[204] Batch[5] avg_epoch_loss=3.339571\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=3.33957143625\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:17 INFO 140052404180800] Epoch[204] Batch [5]#011Speed: 333.81 samples/sec#011loss=3.339571\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:18 INFO 140052404180800] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2178.128957748413, \"sum\": 2178.128957748413, \"min\": 2178.128957748413}}, \"EndTime\": 1592850258.340658, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850256.16208}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:18 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.28847261 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:18 INFO 140052404180800] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=204, train loss <loss>=3.37373747826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:18 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:18 INFO 140052404180800] Epoch[205] Batch[0] avg_epoch_loss=3.281774\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=3.28177380562\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:19 INFO 140052404180800] Epoch[205] Batch[5] avg_epoch_loss=3.253037\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=3.25303729375\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:19 INFO 140052404180800] Epoch[205] Batch [5]#011Speed: 332.80 samples/sec#011loss=3.253037\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:20 INFO 140052404180800] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2201.107978820801, \"sum\": 2201.107978820801, \"min\": 2201.107978820801}}, \"EndTime\": 1592850260.542295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850258.340726}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.578043274 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=205, train loss <loss>=3.35101282597\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:21 INFO 140052404180800] Epoch[206] Batch[0] avg_epoch_loss=3.265676\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=3.26567554474\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:21 INFO 140052404180800] Epoch[206] Batch[5] avg_epoch_loss=3.296974\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=3.29697374503\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:21 INFO 140052404180800] Epoch[206] Batch [5]#011Speed: 333.70 samples/sec#011loss=3.296974\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:22 INFO 140052404180800] Epoch[206] Batch[10] avg_epoch_loss=3.286077\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=206, batch=10 train loss <loss>=3.27300052643\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:22 INFO 140052404180800] Epoch[206] Batch [10]#011Speed: 331.81 samples/sec#011loss=3.273001\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:22 INFO 140052404180800] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2417.9909229278564, \"sum\": 2417.9909229278564, \"min\": 2417.9909229278564}}, \"EndTime\": 1592850262.960822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850260.542377}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.596555763 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=206, train loss <loss>=3.28607682748\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:23 INFO 140052404180800] Epoch[207] Batch[0] avg_epoch_loss=3.205263\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=3.20526266098\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:24 INFO 140052404180800] Epoch[207] Batch[5] avg_epoch_loss=3.316708\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=3.31670832634\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:24 INFO 140052404180800] Epoch[207] Batch [5]#011Speed: 325.64 samples/sec#011loss=3.316708\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:25 INFO 140052404180800] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2225.486993789673, \"sum\": 2225.486993789673, \"min\": 2225.486993789673}}, \"EndTime\": 1592850265.186823, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850262.960885}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:25 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.434796051 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:25 INFO 140052404180800] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=207, train loss <loss>=3.234044981\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:25 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:25 INFO 140052404180800] Epoch[208] Batch[0] avg_epoch_loss=3.232410\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=3.23241019249\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:26 INFO 140052404180800] Epoch[208] Batch[5] avg_epoch_loss=3.269967\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=3.26996680101\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:26 INFO 140052404180800] Epoch[208] Batch [5]#011Speed: 333.85 samples/sec#011loss=3.269967\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:27 INFO 140052404180800] processed a total of 569 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1989.799976348877, \"sum\": 1989.799976348877, \"min\": 1989.799976348877}}, \"EndTime\": 1592850267.177149, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850265.186877}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.943453009 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=208, train loss <loss>=3.30113535457\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:27 INFO 140052404180800] Epoch[209] Batch[0] avg_epoch_loss=3.171427\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=3.17142653465\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:28 INFO 140052404180800] Epoch[209] Batch[5] avg_epoch_loss=3.263694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=3.26369440556\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:28 INFO 140052404180800] Epoch[209] Batch [5]#011Speed: 334.08 samples/sec#011loss=3.263694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:29 INFO 140052404180800] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2170.562982559204, \"sum\": 2170.562982559204, \"min\": 2170.562982559204}}, \"EndTime\": 1592850269.348298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850267.177223}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.638939975 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=209, train loss <loss>=3.23228604794\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:29 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:29 INFO 140052404180800] Epoch[210] Batch[0] avg_epoch_loss=3.203836\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=3.20383572578\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:30 INFO 140052404180800] Epoch[210] Batch[5] avg_epoch_loss=3.289219\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=3.28921862443\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:30 INFO 140052404180800] Epoch[210] Batch [5]#011Speed: 313.78 samples/sec#011loss=3.289219\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:31 INFO 140052404180800] Epoch[210] Batch[10] avg_epoch_loss=3.202625\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=210, batch=10 train loss <loss>=3.09871273041\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:31 INFO 140052404180800] Epoch[210] Batch [10]#011Speed: 296.91 samples/sec#011loss=3.098713\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:31 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2560.5602264404297, \"sum\": 2560.5602264404297, \"min\": 2560.5602264404297}}, \"EndTime\": 1592850271.909411, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850269.348359}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:31 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=250.322276648 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:31 INFO 140052404180800] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=210, train loss <loss>=3.20262503624\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:31 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:32 INFO 140052404180800] Epoch[211] Batch[0] avg_epoch_loss=3.367644\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=3.36764407158\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:33 INFO 140052404180800] Epoch[211] Batch[5] avg_epoch_loss=3.320124\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=3.32012438774\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:33 INFO 140052404180800] Epoch[211] Batch [5]#011Speed: 331.50 samples/sec#011loss=3.320124\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:34 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2207.942008972168, \"sum\": 2207.942008972168, \"min\": 2207.942008972168}}, \"EndTime\": 1592850274.117876, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850271.909514}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.49102605 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=211, train loss <loss>=3.27481911182\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:34 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:34 INFO 140052404180800] Epoch[212] Batch[0] avg_epoch_loss=3.296642\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=3.29664206505\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:35 INFO 140052404180800] Epoch[212] Batch[5] avg_epoch_loss=3.270266\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=3.27026581764\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:35 INFO 140052404180800] Epoch[212] Batch [5]#011Speed: 328.13 samples/sec#011loss=3.270266\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:36 INFO 140052404180800] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2186.5460872650146, \"sum\": 2186.5460872650146, \"min\": 2186.5460872650146}}, \"EndTime\": 1592850276.305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850274.117943}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.939522189 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=212, train loss <loss>=3.25162339211\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:36 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:36 INFO 140052404180800] Epoch[213] Batch[0] avg_epoch_loss=3.240388\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=3.24038767815\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:37 INFO 140052404180800] Epoch[213] Batch[5] avg_epoch_loss=3.201650\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=3.20165030162\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:37 INFO 140052404180800] Epoch[213] Batch [5]#011Speed: 340.48 samples/sec#011loss=3.201650\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:38 INFO 140052404180800] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2139.6420001983643, \"sum\": 2139.6420001983643, \"min\": 2139.6420001983643}}, \"EndTime\": 1592850278.445247, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850276.305078}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:38 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=292.0889602 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:38 INFO 140052404180800] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=213, train loss <loss>=3.22893285751\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:38 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:38 INFO 140052404180800] Epoch[214] Batch[0] avg_epoch_loss=3.185131\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=3.185131073\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:39 INFO 140052404180800] Epoch[214] Batch[5] avg_epoch_loss=3.257013\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=3.25701332092\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:39 INFO 140052404180800] Epoch[214] Batch [5]#011Speed: 330.47 samples/sec#011loss=3.257013\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:40 INFO 140052404180800] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2337.8469944000244, \"sum\": 2337.8469944000244, \"min\": 2337.8469944000244}}, \"EndTime\": 1592850280.783665, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850278.445327}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:40 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=259.627601179 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:40 INFO 140052404180800] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=214, train loss <loss>=3.32859175205\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:40 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:41 INFO 140052404180800] Epoch[215] Batch[0] avg_epoch_loss=3.199446\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=3.19944620132\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:42 INFO 140052404180800] Epoch[215] Batch[5] avg_epoch_loss=3.255860\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=3.25586005052\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:42 INFO 140052404180800] Epoch[215] Batch [5]#011Speed: 338.41 samples/sec#011loss=3.255860\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] Epoch[215] Batch[10] avg_epoch_loss=3.330970\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=215, batch=10 train loss <loss>=3.42110204697\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] Epoch[215] Batch [10]#011Speed: 331.70 samples/sec#011loss=3.421102\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2412.0500087738037, \"sum\": 2412.0500087738037, \"min\": 2412.0500087738037}}, \"EndTime\": 1592850283.196243, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850280.783746}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.955542412 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=215, train loss <loss>=3.3309700489\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] Epoch[216] Batch[0] avg_epoch_loss=3.286369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=3.28636932373\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:44 INFO 140052404180800] Epoch[216] Batch[5] avg_epoch_loss=3.264536\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=3.26453606288\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:44 INFO 140052404180800] Epoch[216] Batch [5]#011Speed: 336.06 samples/sec#011loss=3.264536\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:45 INFO 140052404180800] Epoch[216] Batch[10] avg_epoch_loss=3.207805\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=216, batch=10 train loss <loss>=3.13972668648\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:45 INFO 140052404180800] Epoch[216] Batch [10]#011Speed: 329.58 samples/sec#011loss=3.139727\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:45 INFO 140052404180800] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2430.311918258667, \"sum\": 2430.311918258667, \"min\": 2430.311918258667}}, \"EndTime\": 1592850285.627053, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850283.196317}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.969462582 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=216, train loss <loss>=3.20780452815\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:45 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:46 INFO 140052404180800] Epoch[217] Batch[0] avg_epoch_loss=3.337272\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=3.33727216721\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:47 INFO 140052404180800] Epoch[217] Batch[5] avg_epoch_loss=3.248199\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=3.24819938342\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:47 INFO 140052404180800] Epoch[217] Batch [5]#011Speed: 333.90 samples/sec#011loss=3.248199\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] Epoch[217] Batch[10] avg_epoch_loss=3.231140\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=217, batch=10 train loss <loss>=3.210668993\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] Epoch[217] Batch [10]#011Speed: 333.78 samples/sec#011loss=3.210669\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2434.225082397461, \"sum\": 2434.225082397461, \"min\": 2434.225082397461}}, \"EndTime\": 1592850288.061773, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850285.627127}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.141962 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=217, train loss <loss>=3.23114011504\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] Epoch[218] Batch[0] avg_epoch_loss=3.275578\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=3.27557849884\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:49 INFO 140052404180800] Epoch[218] Batch[5] avg_epoch_loss=3.235266\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=3.23526597023\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:49 INFO 140052404180800] Epoch[218] Batch [5]#011Speed: 337.88 samples/sec#011loss=3.235266\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] Epoch[218] Batch[10] avg_epoch_loss=3.285272\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=218, batch=10 train loss <loss>=3.34527816772\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] Epoch[218] Batch [10]#011Speed: 330.19 samples/sec#011loss=3.345278\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2393.5599327087402, \"sum\": 2393.5599327087402, \"min\": 2393.5599327087402}}, \"EndTime\": 1592850290.45585, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850288.061849}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.131659406 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=218, train loss <loss>=3.28527151455\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] Epoch[219] Batch[0] avg_epoch_loss=3.224370\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=3.22437000275\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:51 INFO 140052404180800] Epoch[219] Batch[5] avg_epoch_loss=3.217379\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=3.21737889449\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:51 INFO 140052404180800] Epoch[219] Batch [5]#011Speed: 333.07 samples/sec#011loss=3.217379\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:52 INFO 140052404180800] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2214.1289710998535, \"sum\": 2214.1289710998535, \"min\": 2214.1289710998535}}, \"EndTime\": 1592850292.670477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850290.455926}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.426006262 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=219, train loss <loss>=3.23987944126\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:53 INFO 140052404180800] Epoch[220] Batch[0] avg_epoch_loss=3.332010\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=3.33200979233\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:54 INFO 140052404180800] Epoch[220] Batch[5] avg_epoch_loss=3.294109\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=3.29410851002\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:54 INFO 140052404180800] Epoch[220] Batch [5]#011Speed: 339.10 samples/sec#011loss=3.294109\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] Epoch[220] Batch[10] avg_epoch_loss=3.261154\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=220, batch=10 train loss <loss>=3.2216091156\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] Epoch[220] Batch [10]#011Speed: 321.97 samples/sec#011loss=3.221609\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2413.2308959960938, \"sum\": 2413.2308959960938, \"min\": 2413.2308959960938}}, \"EndTime\": 1592850295.08424, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850292.670546}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.767194568 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=220, train loss <loss>=3.26115423983\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] Epoch[221] Batch[0] avg_epoch_loss=3.364301\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=3.36430096626\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:56 INFO 140052404180800] Epoch[221] Batch[5] avg_epoch_loss=3.265737\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=3.26573713621\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:56 INFO 140052404180800] Epoch[221] Batch [5]#011Speed: 334.83 samples/sec#011loss=3.265737\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:57 INFO 140052404180800] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2166.985034942627, \"sum\": 2166.985034942627, \"min\": 2166.985034942627}}, \"EndTime\": 1592850297.251777, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850295.084312}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.559529475 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=221, train loss <loss>=3.27319049835\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:57 INFO 140052404180800] Epoch[222] Batch[0] avg_epoch_loss=3.220981\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=3.22098064423\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:58 INFO 140052404180800] Epoch[222] Batch[5] avg_epoch_loss=3.224786\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=3.22478592396\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:58 INFO 140052404180800] Epoch[222] Batch [5]#011Speed: 334.92 samples/sec#011loss=3.224786\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:59 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2162.7681255340576, \"sum\": 2162.7681255340576, \"min\": 2162.7681255340576}}, \"EndTime\": 1592850299.41508, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850297.251842}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=294.513495096 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=222, train loss <loss>=3.25246288776\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:59 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:59 INFO 140052404180800] Epoch[223] Batch[0] avg_epoch_loss=3.379152\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:24:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=3.37915182114\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:00 INFO 140052404180800] Epoch[223] Batch[5] avg_epoch_loss=3.277540\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=3.27753957113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:00 INFO 140052404180800] Epoch[223] Batch [5]#011Speed: 327.98 samples/sec#011loss=3.277540\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:01 INFO 140052404180800] Epoch[223] Batch[10] avg_epoch_loss=3.235122\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=3.18422141075\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:01 INFO 140052404180800] Epoch[223] Batch [10]#011Speed: 334.43 samples/sec#011loss=3.184221\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:01 INFO 140052404180800] processed a total of 700 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2392.441987991333, \"sum\": 2392.441987991333, \"min\": 2392.441987991333}}, \"EndTime\": 1592850301.808056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850299.415162}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:01 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=292.574256587 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:01 INFO 140052404180800] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=223, train loss <loss>=3.2351222255\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:01 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:02 INFO 140052404180800] Epoch[224] Batch[0] avg_epoch_loss=3.188065\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=3.18806505203\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:03 INFO 140052404180800] Epoch[224] Batch[5] avg_epoch_loss=3.243327\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=3.24332678318\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:03 INFO 140052404180800] Epoch[224] Batch [5]#011Speed: 339.89 samples/sec#011loss=3.243327\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] Epoch[224] Batch[10] avg_epoch_loss=3.196117\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=224, batch=10 train loss <loss>=3.13946547508\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] Epoch[224] Batch [10]#011Speed: 333.02 samples/sec#011loss=3.139465\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2391.278028488159, \"sum\": 2391.278028488159, \"min\": 2391.278028488159}}, \"EndTime\": 1592850304.199838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850301.808133}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.099194752 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=224, train loss <loss>=3.19611709768\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] Epoch[225] Batch[0] avg_epoch_loss=3.279685\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=3.27968502045\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:05 INFO 140052404180800] Epoch[225] Batch[5] avg_epoch_loss=3.209710\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=3.20970956484\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:05 INFO 140052404180800] Epoch[225] Batch [5]#011Speed: 327.39 samples/sec#011loss=3.209710\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:06 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2231.369972229004, \"sum\": 2231.369972229004, \"min\": 2231.369972229004}}, \"EndTime\": 1592850306.431687, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850304.199913}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.633650841 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=225, train loss <loss>=3.17006626129\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:06 INFO 140052404180800] Epoch[226] Batch[0] avg_epoch_loss=3.180422\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=3.18042159081\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:07 INFO 140052404180800] Epoch[226] Batch[5] avg_epoch_loss=3.262275\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=3.26227502028\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:07 INFO 140052404180800] Epoch[226] Batch [5]#011Speed: 335.93 samples/sec#011loss=3.262275\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:08 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2194.2169666290283, \"sum\": 2194.2169666290283, \"min\": 2194.2169666290283}}, \"EndTime\": 1592850308.626528, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850306.43177}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=290.293118759 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=226, train loss <loss>=3.30380094051\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:09 INFO 140052404180800] Epoch[227] Batch[0] avg_epoch_loss=3.225192\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=3.22519159317\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:10 INFO 140052404180800] Epoch[227] Batch[5] avg_epoch_loss=3.234524\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=3.23452417056\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:10 INFO 140052404180800] Epoch[227] Batch [5]#011Speed: 332.01 samples/sec#011loss=3.234524\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:10 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2201.014995574951, \"sum\": 2201.014995574951, \"min\": 2201.014995574951}}, \"EndTime\": 1592850310.82807, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850308.626607}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:10 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.579292384 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:10 INFO 140052404180800] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=227, train loss <loss>=3.25418374538\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:10 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:11 INFO 140052404180800] Epoch[228] Batch[0] avg_epoch_loss=3.378521\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=3.37852096558\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:12 INFO 140052404180800] Epoch[228] Batch[5] avg_epoch_loss=3.246097\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=3.24609696865\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:12 INFO 140052404180800] Epoch[228] Batch [5]#011Speed: 336.26 samples/sec#011loss=3.246097\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] Epoch[228] Batch[10] avg_epoch_loss=3.299278\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=228, batch=10 train loss <loss>=3.36309537888\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] Epoch[228] Batch [10]#011Speed: 339.53 samples/sec#011loss=3.363095\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2363.884925842285, \"sum\": 2363.884925842285, \"min\": 2363.884925842285}}, \"EndTime\": 1592850313.192494, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850310.82815}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.843097922 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=228, train loss <loss>=3.29927806421\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] Epoch[229] Batch[0] avg_epoch_loss=3.372285\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=3.37228488922\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:14 INFO 140052404180800] Epoch[229] Batch[5] avg_epoch_loss=3.286903\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=3.28690334161\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:14 INFO 140052404180800] Epoch[229] Batch [5]#011Speed: 338.16 samples/sec#011loss=3.286903\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:15 INFO 140052404180800] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2219.069004058838, \"sum\": 2219.069004058838, \"min\": 2219.069004058838}}, \"EndTime\": 1592850315.412059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850313.192571}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:15 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.227163686 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:15 INFO 140052404180800] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=229, train loss <loss>=3.23341114521\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:15 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:15 INFO 140052404180800] Epoch[230] Batch[0] avg_epoch_loss=3.270585\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=3.27058506012\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:16 INFO 140052404180800] Epoch[230] Batch[5] avg_epoch_loss=3.231939\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=3.23193864028\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:16 INFO 140052404180800] Epoch[230] Batch [5]#011Speed: 332.97 samples/sec#011loss=3.231939\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:17 INFO 140052404180800] Epoch[230] Batch[10] avg_epoch_loss=3.308085\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=230, batch=10 train loss <loss>=3.39946165085\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:17 INFO 140052404180800] Epoch[230] Batch [10]#011Speed: 337.25 samples/sec#011loss=3.399462\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:17 INFO 140052404180800] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2383.579969406128, \"sum\": 2383.579969406128, \"min\": 2383.579969406128}}, \"EndTime\": 1592850317.796179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850315.412141}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:17 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.461999465 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:17 INFO 140052404180800] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=230, train loss <loss>=3.30808546326\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:17 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:18 INFO 140052404180800] Epoch[231] Batch[0] avg_epoch_loss=3.357663\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=3.3576631546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:19 INFO 140052404180800] Epoch[231] Batch[5] avg_epoch_loss=3.228025\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=3.22802515825\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:19 INFO 140052404180800] Epoch[231] Batch [5]#011Speed: 338.08 samples/sec#011loss=3.228025\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] Epoch[231] Batch[10] avg_epoch_loss=3.198207\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=3.16242456436\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] Epoch[231] Batch [10]#011Speed: 334.08 samples/sec#011loss=3.162425\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2361.9542121887207, \"sum\": 2361.9542121887207, \"min\": 2361.9542121887207}}, \"EndTime\": 1592850320.158632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850317.796254}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.534276386 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=231, train loss <loss>=3.19820670648\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] Epoch[232] Batch[0] avg_epoch_loss=3.181486\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=3.18148589134\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:21 INFO 140052404180800] Epoch[232] Batch[5] avg_epoch_loss=3.222154\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=3.22215433915\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:21 INFO 140052404180800] Epoch[232] Batch [5]#011Speed: 334.45 samples/sec#011loss=3.222154\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:22 INFO 140052404180800] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2214.5040035247803, \"sum\": 2214.5040035247803, \"min\": 2214.5040035247803}}, \"EndTime\": 1592850322.373608, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850320.158701}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.860961687 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=232, train loss <loss>=3.1876447916\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:22 INFO 140052404180800] Epoch[233] Batch[0] avg_epoch_loss=3.066965\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=3.06696510315\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:23 INFO 140052404180800] Epoch[233] Batch[5] avg_epoch_loss=3.204954\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=3.20495430628\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:23 INFO 140052404180800] Epoch[233] Batch [5]#011Speed: 333.63 samples/sec#011loss=3.204954\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:24 INFO 140052404180800] Epoch[233] Batch[10] avg_epoch_loss=3.179169\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=233, batch=10 train loss <loss>=3.14822649956\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:24 INFO 140052404180800] Epoch[233] Batch [10]#011Speed: 322.48 samples/sec#011loss=3.148226\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:24 INFO 140052404180800] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2446.3250637054443, \"sum\": 2446.3250637054443, \"min\": 2446.3250637054443}}, \"EndTime\": 1592850324.82045, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850322.373688}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:24 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.276306889 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:24 INFO 140052404180800] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=233, train loss <loss>=3.17916893959\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:24 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:25 INFO 140052404180800] Epoch[234] Batch[0] avg_epoch_loss=3.298339\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=234, batch=0 train loss <loss>=3.29833936691\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:26 INFO 140052404180800] Epoch[234] Batch[5] avg_epoch_loss=3.260461\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=234, batch=5 train loss <loss>=3.26046129068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:26 INFO 140052404180800] Epoch[234] Batch [5]#011Speed: 334.34 samples/sec#011loss=3.260461\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:27 INFO 140052404180800] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2196.665048599243, \"sum\": 2196.665048599243, \"min\": 2196.665048599243}}, \"EndTime\": 1592850327.017625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850324.820527}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.782733723 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=234, train loss <loss>=3.26836845875\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:27 INFO 140052404180800] Epoch[235] Batch[0] avg_epoch_loss=3.143500\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=235, batch=0 train loss <loss>=3.14350008965\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:28 INFO 140052404180800] Epoch[235] Batch[5] avg_epoch_loss=3.175698\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=235, batch=5 train loss <loss>=3.17569828033\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:28 INFO 140052404180800] Epoch[235] Batch [5]#011Speed: 337.18 samples/sec#011loss=3.175698\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:29 INFO 140052404180800] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2186.4120960235596, \"sum\": 2186.4120960235596, \"min\": 2186.4120960235596}}, \"EndTime\": 1592850329.204585, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850327.017707}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.042440431 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=235, train loss <loss>=3.20124201775\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:29 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:29 INFO 140052404180800] Epoch[236] Batch[0] avg_epoch_loss=3.232091\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=236, batch=0 train loss <loss>=3.23209142685\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:30 INFO 140052404180800] Epoch[236] Batch[5] avg_epoch_loss=3.173688\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=236, batch=5 train loss <loss>=3.17368777593\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:30 INFO 140052404180800] Epoch[236] Batch [5]#011Speed: 335.85 samples/sec#011loss=3.173688\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] Epoch[236] Batch[10] avg_epoch_loss=3.165010\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=236, batch=10 train loss <loss>=3.15459752083\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] Epoch[236] Batch [10]#011Speed: 334.32 samples/sec#011loss=3.154598\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2408.42604637146, \"sum\": 2408.42604637146, \"min\": 2408.42604637146}}, \"EndTime\": 1592850331.613601, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850329.204666}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.025035277 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=236, train loss <loss>=3.16501038725\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:31 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_d183449e-b8c4-4d0c-8708-b119f7fc93d8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.44511222839355, \"sum\": 66.44511222839355, \"min\": 66.44511222839355}}, \"EndTime\": 1592850331.680609, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850331.613678}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:32 INFO 140052404180800] Epoch[237] Batch[0] avg_epoch_loss=3.086758\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=237, batch=0 train loss <loss>=3.08675837517\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:33 INFO 140052404180800] Epoch[237] Batch[5] avg_epoch_loss=3.194862\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=237, batch=5 train loss <loss>=3.19486232599\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:33 INFO 140052404180800] Epoch[237] Batch [5]#011Speed: 334.39 samples/sec#011loss=3.194862\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] Epoch[237] Batch[10] avg_epoch_loss=3.273490\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=237, batch=10 train loss <loss>=3.36784286499\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] Epoch[237] Batch [10]#011Speed: 334.13 samples/sec#011loss=3.367843\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2380.2900314331055, \"sum\": 2380.2900314331055, \"min\": 2380.2900314331055}}, \"EndTime\": 1592850334.061044, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850331.68069}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.542095037 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=237, train loss <loss>=3.27348984372\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] Epoch[238] Batch[0] avg_epoch_loss=3.217686\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=238, batch=0 train loss <loss>=3.21768569946\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:35 INFO 140052404180800] Epoch[238] Batch[5] avg_epoch_loss=3.211286\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=238, batch=5 train loss <loss>=3.21128602823\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:35 INFO 140052404180800] Epoch[238] Batch [5]#011Speed: 331.39 samples/sec#011loss=3.211286\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:36 INFO 140052404180800] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2205.759048461914, \"sum\": 2205.759048461914, \"min\": 2205.759048461914}}, \"EndTime\": 1592850336.267336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850334.061122}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.694292653 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=238, train loss <loss>=3.18446190357\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:36 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:36 INFO 140052404180800] Epoch[239] Batch[0] avg_epoch_loss=3.281285\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=239, batch=0 train loss <loss>=3.28128480911\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:37 INFO 140052404180800] Epoch[239] Batch[5] avg_epoch_loss=3.190575\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=239, batch=5 train loss <loss>=3.19057476521\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:37 INFO 140052404180800] Epoch[239] Batch [5]#011Speed: 337.72 samples/sec#011loss=3.190575\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:38 INFO 140052404180800] Epoch[239] Batch[10] avg_epoch_loss=3.224337\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=239, batch=10 train loss <loss>=3.2648519516\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:38 INFO 140052404180800] Epoch[239] Batch [10]#011Speed: 332.45 samples/sec#011loss=3.264852\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:38 INFO 140052404180800] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2384.3960762023926, \"sum\": 2384.3960762023926, \"min\": 2384.3960762023926}}, \"EndTime\": 1592850338.652259, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850336.267415}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:38 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.367622003 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:38 INFO 140052404180800] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=239, train loss <loss>=3.22433712266\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:38 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:39 INFO 140052404180800] Epoch[240] Batch[0] avg_epoch_loss=3.085525\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=240, batch=0 train loss <loss>=3.08552455902\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:40 INFO 140052404180800] Epoch[240] Batch[5] avg_epoch_loss=3.084992\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=240, batch=5 train loss <loss>=3.08499221007\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:40 INFO 140052404180800] Epoch[240] Batch [5]#011Speed: 335.11 samples/sec#011loss=3.084992\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] Epoch[240] Batch[10] avg_epoch_loss=3.176662\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=240, batch=10 train loss <loss>=3.28666677475\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] Epoch[240] Batch [10]#011Speed: 333.11 samples/sec#011loss=3.286667\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2413.5689735412598, \"sum\": 2413.5689735412598, \"min\": 2413.5689735412598}}, \"EndTime\": 1592850341.066348, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850338.652336}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.512574429 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=240, train loss <loss>=3.17666246674\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] Epoch[241] Batch[0] avg_epoch_loss=3.328724\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=241, batch=0 train loss <loss>=3.32872366905\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:42 INFO 140052404180800] Epoch[241] Batch[5] avg_epoch_loss=3.268859\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=241, batch=5 train loss <loss>=3.26885875066\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:42 INFO 140052404180800] Epoch[241] Batch [5]#011Speed: 332.86 samples/sec#011loss=3.268859\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:43 INFO 140052404180800] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2192.8999423980713, \"sum\": 2192.8999423980713, \"min\": 2192.8999423980713}}, \"EndTime\": 1592850343.259784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850341.066427}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=268.578075062 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=241, train loss <loss>=3.35778691769\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:43 INFO 140052404180800] Epoch[242] Batch[0] avg_epoch_loss=3.259505\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=242, batch=0 train loss <loss>=3.25950503349\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:44 INFO 140052404180800] Epoch[242] Batch[5] avg_epoch_loss=3.231481\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=242, batch=5 train loss <loss>=3.23148079713\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:44 INFO 140052404180800] Epoch[242] Batch [5]#011Speed: 338.67 samples/sec#011loss=3.231481\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:45 INFO 140052404180800] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2195.8489418029785, \"sum\": 2195.8489418029785, \"min\": 2195.8489418029785}}, \"EndTime\": 1592850345.456202, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850343.259862}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.139510493 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=242, train loss <loss>=3.27350308895\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:45 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:45 INFO 140052404180800] Epoch[243] Batch[0] avg_epoch_loss=3.174911\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=243, batch=0 train loss <loss>=3.17491078377\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:46 INFO 140052404180800] Epoch[243] Batch[5] avg_epoch_loss=3.207430\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=243, batch=5 train loss <loss>=3.2074303627\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:46 INFO 140052404180800] Epoch[243] Batch [5]#011Speed: 336.00 samples/sec#011loss=3.207430\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:47 INFO 140052404180800] Epoch[243] Batch[10] avg_epoch_loss=3.173675\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=243, batch=10 train loss <loss>=3.13316955566\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:47 INFO 140052404180800] Epoch[243] Batch [10]#011Speed: 332.62 samples/sec#011loss=3.133170\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:47 INFO 140052404180800] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2432.431936264038, \"sum\": 2432.431936264038, \"min\": 2432.431936264038}}, \"EndTime\": 1592850347.889192, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850345.456277}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:47 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.653768442 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:47 INFO 140052404180800] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=243, train loss <loss>=3.17367545041\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:47 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:48 INFO 140052404180800] Epoch[244] Batch[0] avg_epoch_loss=3.175844\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=244, batch=0 train loss <loss>=3.17584371567\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:49 INFO 140052404180800] Epoch[244] Batch[5] avg_epoch_loss=3.282881\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=244, batch=5 train loss <loss>=3.28288090229\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:49 INFO 140052404180800] Epoch[244] Batch [5]#011Speed: 330.51 samples/sec#011loss=3.282881\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:50 INFO 140052404180800] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2198.577880859375, \"sum\": 2198.577880859375, \"min\": 2198.577880859375}}, \"EndTime\": 1592850350.088296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850347.889268}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=290.626821365 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=244, train loss <loss>=3.24789464474\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:50 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:50 INFO 140052404180800] Epoch[245] Batch[0] avg_epoch_loss=3.262510\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=245, batch=0 train loss <loss>=3.26251006126\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:51 INFO 140052404180800] Epoch[245] Batch[5] avg_epoch_loss=3.248619\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=245, batch=5 train loss <loss>=3.24861864249\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:51 INFO 140052404180800] Epoch[245] Batch [5]#011Speed: 336.60 samples/sec#011loss=3.248619\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:52 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2174.9019622802734, \"sum\": 2174.9019622802734, \"min\": 2174.9019622802734}}, \"EndTime\": 1592850352.263729, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850350.088377}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=291.031936958 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=245, train loss <loss>=3.2675727129\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:52 INFO 140052404180800] Epoch[246] Batch[0] avg_epoch_loss=3.107629\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=246, batch=0 train loss <loss>=3.10762906075\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:53 INFO 140052404180800] Epoch[246] Batch[5] avg_epoch_loss=3.176737\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=246, batch=5 train loss <loss>=3.17673699061\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:53 INFO 140052404180800] Epoch[246] Batch [5]#011Speed: 338.06 samples/sec#011loss=3.176737\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] Epoch[246] Batch[10] avg_epoch_loss=3.149204\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=246, batch=10 train loss <loss>=3.11616544724\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] Epoch[246] Batch [10]#011Speed: 326.21 samples/sec#011loss=3.116165\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2436.595916748047, \"sum\": 2436.595916748047, \"min\": 2436.595916748047}}, \"EndTime\": 1592850354.700847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850352.26381}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.730291199 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=246, train loss <loss>=3.14920447089\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:54 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_799f823c-61c5-47bd-a7ba-a0f13a6ccd6f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 75.89912414550781, \"sum\": 75.89912414550781, \"min\": 75.89912414550781}}, \"EndTime\": 1592850354.77732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850354.700922}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:55 INFO 140052404180800] Epoch[247] Batch[0] avg_epoch_loss=3.068277\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=247, batch=0 train loss <loss>=3.06827688217\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:56 INFO 140052404180800] Epoch[247] Batch[5] avg_epoch_loss=3.194878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=247, batch=5 train loss <loss>=3.19487810135\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:56 INFO 140052404180800] Epoch[247] Batch [5]#011Speed: 333.68 samples/sec#011loss=3.194878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] Epoch[247] Batch[10] avg_epoch_loss=3.285736\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=247, batch=10 train loss <loss>=3.39476618767\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] Epoch[247] Batch [10]#011Speed: 331.40 samples/sec#011loss=3.394766\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2393.4779167175293, \"sum\": 2393.4779167175293, \"min\": 2393.4779167175293}}, \"EndTime\": 1592850357.170936, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850354.777396}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.228502245 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=247, train loss <loss>=3.2857363224\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] Epoch[248] Batch[0] avg_epoch_loss=3.108755\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=248, batch=0 train loss <loss>=3.10875511169\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:58 INFO 140052404180800] Epoch[248] Batch[5] avg_epoch_loss=3.246383\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=248, batch=5 train loss <loss>=3.24638346831\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:58 INFO 140052404180800] Epoch[248] Batch [5]#011Speed: 334.95 samples/sec#011loss=3.246383\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:59 INFO 140052404180800] Epoch[248] Batch[10] avg_epoch_loss=3.311145\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=248, batch=10 train loss <loss>=3.38885784149\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:59 INFO 140052404180800] Epoch[248] Batch [10]#011Speed: 331.22 samples/sec#011loss=3.388858\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:59 INFO 140052404180800] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2385.057210922241, \"sum\": 2385.057210922241, \"min\": 2385.057210922241}}, \"EndTime\": 1592850359.556503, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850357.171026}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.902411485 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=248, train loss <loss>=3.31114454703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:25:59 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:00 INFO 140052404180800] Epoch[249] Batch[0] avg_epoch_loss=2.981542\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=249, batch=0 train loss <loss>=2.98154187202\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:00 INFO 140052404180800] Epoch[249] Batch[5] avg_epoch_loss=3.241126\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=249, batch=5 train loss <loss>=3.24112566312\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:00 INFO 140052404180800] Epoch[249] Batch [5]#011Speed: 333.12 samples/sec#011loss=3.241126\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:01 INFO 140052404180800] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2212.4361991882324, \"sum\": 2212.4361991882324, \"min\": 2212.4361991882324}}, \"EndTime\": 1592850361.769535, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850359.556579}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:01 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=265.749727785 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:01 INFO 140052404180800] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=249, train loss <loss>=3.29289021492\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:01 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:02 INFO 140052404180800] Epoch[250] Batch[0] avg_epoch_loss=3.218375\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=250, batch=0 train loss <loss>=3.21837520599\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:03 INFO 140052404180800] Epoch[250] Batch[5] avg_epoch_loss=3.193896\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=250, batch=5 train loss <loss>=3.19389617443\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:03 INFO 140052404180800] Epoch[250] Batch [5]#011Speed: 327.22 samples/sec#011loss=3.193896\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:04 INFO 140052404180800] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2241.5759563446045, \"sum\": 2241.5759563446045, \"min\": 2241.5759563446045}}, \"EndTime\": 1592850364.011643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850361.769617}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:04 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.795260407 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:04 INFO 140052404180800] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=250, train loss <loss>=3.22588713169\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:04 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:04 INFO 140052404180800] Epoch[251] Batch[0] avg_epoch_loss=3.247893\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=251, batch=0 train loss <loss>=3.2478928566\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:05 INFO 140052404180800] Epoch[251] Batch[5] avg_epoch_loss=3.251685\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=251, batch=5 train loss <loss>=3.25168526173\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:05 INFO 140052404180800] Epoch[251] Batch [5]#011Speed: 299.37 samples/sec#011loss=3.251685\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] Epoch[251] Batch[10] avg_epoch_loss=3.109226\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=251, batch=10 train loss <loss>=2.93827471733\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] Epoch[251] Batch [10]#011Speed: 335.02 samples/sec#011loss=2.938275\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2490.868091583252, \"sum\": 2490.868091583252, \"min\": 2490.868091583252}}, \"EndTime\": 1592850366.503024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850364.011704}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=261.343134839 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=251, train loss <loss>=3.10922592336\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:06 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_b340a3df-5184-4ccd-91d7-f4d7a096c589-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 72.91007041931152, \"sum\": 72.91007041931152, \"min\": 72.91007041931152}}, \"EndTime\": 1592850366.576511, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850366.503101}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:07 INFO 140052404180800] Epoch[252] Batch[0] avg_epoch_loss=3.340472\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=252, batch=0 train loss <loss>=3.34047222137\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:07 INFO 140052404180800] Epoch[252] Batch[5] avg_epoch_loss=3.184911\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=252, batch=5 train loss <loss>=3.18491109212\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:07 INFO 140052404180800] Epoch[252] Batch [5]#011Speed: 334.58 samples/sec#011loss=3.184911\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:08 INFO 140052404180800] Epoch[252] Batch[10] avg_epoch_loss=3.235572\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=252, batch=10 train loss <loss>=3.29636569023\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:08 INFO 140052404180800] Epoch[252] Batch [10]#011Speed: 327.73 samples/sec#011loss=3.296366\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:08 INFO 140052404180800] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2397.2129821777344, \"sum\": 2397.2129821777344, \"min\": 2397.2129821777344}}, \"EndTime\": 1592850368.97387, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850366.576593}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.135532083 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=252, train loss <loss>=3.23557227308\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:09 INFO 140052404180800] Epoch[253] Batch[0] avg_epoch_loss=3.090863\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=253, batch=0 train loss <loss>=3.09086298943\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:10 INFO 140052404180800] Epoch[253] Batch[5] avg_epoch_loss=3.113888\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=253, batch=5 train loss <loss>=3.11388766766\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:10 INFO 140052404180800] Epoch[253] Batch [5]#011Speed: 330.48 samples/sec#011loss=3.113888\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:11 INFO 140052404180800] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2253.21102142334, \"sum\": 2253.21102142334, \"min\": 2253.21102142334}}, \"EndTime\": 1592850371.227577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850368.973946}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:11 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.485495794 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:11 INFO 140052404180800] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=253, train loss <loss>=3.17459692955\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:11 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:11 INFO 140052404180800] Epoch[254] Batch[0] avg_epoch_loss=3.197125\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=254, batch=0 train loss <loss>=3.19712471962\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:12 INFO 140052404180800] Epoch[254] Batch[5] avg_epoch_loss=3.154809\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=254, batch=5 train loss <loss>=3.15480947495\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:12 INFO 140052404180800] Epoch[254] Batch [5]#011Speed: 334.40 samples/sec#011loss=3.154809\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:13 INFO 140052404180800] processed a total of 582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2213.531017303467, \"sum\": 2213.531017303467, \"min\": 2213.531017303467}}, \"EndTime\": 1592850373.441653, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850371.227659}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=262.914900822 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=254, train loss <loss>=3.08473975658\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:13 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:13 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_aed3867a-178f-49c2-b06f-7ee4a913190a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 62.11590766906738, \"sum\": 62.11590766906738, \"min\": 62.11590766906738}}, \"EndTime\": 1592850373.504372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850373.441734}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:13 INFO 140052404180800] Epoch[255] Batch[0] avg_epoch_loss=3.296978\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=255, batch=0 train loss <loss>=3.29697847366\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:14 INFO 140052404180800] Epoch[255] Batch[5] avg_epoch_loss=3.197751\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=255, batch=5 train loss <loss>=3.19775096575\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:14 INFO 140052404180800] Epoch[255] Batch [5]#011Speed: 330.57 samples/sec#011loss=3.197751\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:15 INFO 140052404180800] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2193.0580139160156, \"sum\": 2193.0580139160156, \"min\": 2193.0580139160156}}, \"EndTime\": 1592850375.697564, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850373.50444}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:15 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.976146358 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:15 INFO 140052404180800] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=255, train loss <loss>=3.16210787296\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:15 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:16 INFO 140052404180800] Epoch[256] Batch[0] avg_epoch_loss=3.161166\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=256, batch=0 train loss <loss>=3.16116642952\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:17 INFO 140052404180800] Epoch[256] Batch[5] avg_epoch_loss=3.186044\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=256, batch=5 train loss <loss>=3.18604385853\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:17 INFO 140052404180800] Epoch[256] Batch [5]#011Speed: 337.56 samples/sec#011loss=3.186044\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] Epoch[256] Batch[10] avg_epoch_loss=3.099850\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=256, batch=10 train loss <loss>=2.99641809464\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] Epoch[256] Batch [10]#011Speed: 325.09 samples/sec#011loss=2.996418\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2396.5280055999756, \"sum\": 2396.5280055999756, \"min\": 2396.5280055999756}}, \"EndTime\": 1592850378.094688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850375.697634}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=268.292509084 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=256, train loss <loss>=3.09985032949\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] Epoch[257] Batch[0] avg_epoch_loss=3.230605\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=257, batch=0 train loss <loss>=3.23060488701\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:19 INFO 140052404180800] Epoch[257] Batch[5] avg_epoch_loss=3.195822\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=257, batch=5 train loss <loss>=3.19582227866\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:19 INFO 140052404180800] Epoch[257] Batch [5]#011Speed: 327.61 samples/sec#011loss=3.195822\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:20 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2265.7809257507324, \"sum\": 2265.7809257507324, \"min\": 2265.7809257507324}}, \"EndTime\": 1592850380.360949, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850378.094764}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.270043705 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=257, train loss <loss>=3.21955509186\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:20 INFO 140052404180800] Epoch[258] Batch[0] avg_epoch_loss=3.319391\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=258, batch=0 train loss <loss>=3.31939053535\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:21 INFO 140052404180800] Epoch[258] Batch[5] avg_epoch_loss=3.235770\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=258, batch=5 train loss <loss>=3.23576955001\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:21 INFO 140052404180800] Epoch[258] Batch [5]#011Speed: 338.31 samples/sec#011loss=3.235770\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:22 INFO 140052404180800] Epoch[258] Batch[10] avg_epoch_loss=3.129301\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=258, batch=10 train loss <loss>=3.00153784752\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:22 INFO 140052404180800] Epoch[258] Batch [10]#011Speed: 327.03 samples/sec#011loss=3.001538\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:22 INFO 140052404180800] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2405.205011367798, \"sum\": 2405.205011367798, \"min\": 2405.205011367798}}, \"EndTime\": 1592850382.766726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850380.361032}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.313371358 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=258, train loss <loss>=3.12930059433\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:23 INFO 140052404180800] Epoch[259] Batch[0] avg_epoch_loss=2.989195\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=259, batch=0 train loss <loss>=2.98919463158\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:24 INFO 140052404180800] Epoch[259] Batch[5] avg_epoch_loss=3.203118\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=259, batch=5 train loss <loss>=3.20311796665\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:24 INFO 140052404180800] Epoch[259] Batch [5]#011Speed: 339.58 samples/sec#011loss=3.203118\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:24 INFO 140052404180800] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2208.1940174102783, \"sum\": 2208.1940174102783, \"min\": 2208.1940174102783}}, \"EndTime\": 1592850384.975428, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850382.766802}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:24 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.681794312 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:24 INFO 140052404180800] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=259, train loss <loss>=3.19004848003\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:24 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:25 INFO 140052404180800] Epoch[260] Batch[0] avg_epoch_loss=3.161867\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=260, batch=0 train loss <loss>=3.16186666489\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:26 INFO 140052404180800] Epoch[260] Batch[5] avg_epoch_loss=3.297467\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=260, batch=5 train loss <loss>=3.29746727149\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:26 INFO 140052404180800] Epoch[260] Batch [5]#011Speed: 314.38 samples/sec#011loss=3.297467\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] Epoch[260] Batch[10] avg_epoch_loss=3.191555\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=260, batch=10 train loss <loss>=3.06445918083\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] Epoch[260] Batch [10]#011Speed: 333.27 samples/sec#011loss=3.064459\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2477.169990539551, \"sum\": 2477.169990539551, \"min\": 2477.169990539551}}, \"EndTime\": 1592850387.453127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850384.97551}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=263.191898341 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=260, train loss <loss>=3.19155450301\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] Epoch[261] Batch[0] avg_epoch_loss=3.260027\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=261, batch=0 train loss <loss>=3.2600274086\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:28 INFO 140052404180800] Epoch[261] Batch[5] avg_epoch_loss=3.177787\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=261, batch=5 train loss <loss>=3.1777873834\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:28 INFO 140052404180800] Epoch[261] Batch [5]#011Speed: 337.12 samples/sec#011loss=3.177787\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] Epoch[261] Batch[10] avg_epoch_loss=3.068870\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=261, batch=10 train loss <loss>=2.93816828728\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] Epoch[261] Batch [10]#011Speed: 333.53 samples/sec#011loss=2.938168\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2424.9298572540283, \"sum\": 2424.9298572540283, \"min\": 2424.9298572540283}}, \"EndTime\": 1592850389.878588, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850387.453203}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=266.38714831 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=261, train loss <loss>=3.06886961243\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:29 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_88d8dfe0-1b2c-47db-ace0-067192f36318-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 63.30704689025879, \"sum\": 63.30704689025879, \"min\": 63.30704689025879}}, \"EndTime\": 1592850389.942466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850389.878665}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:30 INFO 140052404180800] Epoch[262] Batch[0] avg_epoch_loss=3.040317\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=262, batch=0 train loss <loss>=3.04031705856\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:31 INFO 140052404180800] Epoch[262] Batch[5] avg_epoch_loss=3.124812\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=262, batch=5 train loss <loss>=3.12481200695\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:31 INFO 140052404180800] Epoch[262] Batch [5]#011Speed: 329.86 samples/sec#011loss=3.124812\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:32 INFO 140052404180800] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2179.517984390259, \"sum\": 2179.517984390259, \"min\": 2179.517984390259}}, \"EndTime\": 1592850392.122127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850389.942541}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:32 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.357346461 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:32 INFO 140052404180800] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=262, train loss <loss>=3.22221679688\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:32 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:32 INFO 140052404180800] Epoch[263] Batch[0] avg_epoch_loss=3.276849\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=263, batch=0 train loss <loss>=3.27684903145\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:33 INFO 140052404180800] Epoch[263] Batch[5] avg_epoch_loss=3.229167\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=263, batch=5 train loss <loss>=3.22916662693\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:33 INFO 140052404180800] Epoch[263] Batch [5]#011Speed: 339.68 samples/sec#011loss=3.229167\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:34 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2170.444965362549, \"sum\": 2170.444965362549, \"min\": 2170.444965362549}}, \"EndTime\": 1592850394.293127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850392.122209}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=291.629280959 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=263, train loss <loss>=3.26117520332\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:34 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:34 INFO 140052404180800] Epoch[264] Batch[0] avg_epoch_loss=3.231965\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=264, batch=0 train loss <loss>=3.23196530342\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:35 INFO 140052404180800] Epoch[264] Batch[5] avg_epoch_loss=3.088150\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=264, batch=5 train loss <loss>=3.08814970652\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:35 INFO 140052404180800] Epoch[264] Batch [5]#011Speed: 330.77 samples/sec#011loss=3.088150\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:36 INFO 140052404180800] Epoch[264] Batch[10] avg_epoch_loss=3.172038\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=264, batch=10 train loss <loss>=3.27270321846\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:36 INFO 140052404180800] Epoch[264] Batch [10]#011Speed: 332.52 samples/sec#011loss=3.272703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:36 INFO 140052404180800] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2380.079984664917, \"sum\": 2380.079984664917, \"min\": 2380.079984664917}}, \"EndTime\": 1592850396.673783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850394.29321}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.750180928 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=264, train loss <loss>=3.17203766649\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:36 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:37 INFO 140052404180800] Epoch[265] Batch[0] avg_epoch_loss=2.944826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=265, batch=0 train loss <loss>=2.9448261261\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:38 INFO 140052404180800] Epoch[265] Batch[5] avg_epoch_loss=3.077664\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=265, batch=5 train loss <loss>=3.0776635011\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:38 INFO 140052404180800] Epoch[265] Batch [5]#011Speed: 337.87 samples/sec#011loss=3.077664\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] Epoch[265] Batch[10] avg_epoch_loss=3.149081\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=265, batch=10 train loss <loss>=3.23478150368\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] Epoch[265] Batch [10]#011Speed: 336.69 samples/sec#011loss=3.234782\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2364.0048503875732, \"sum\": 2364.0048503875732, \"min\": 2364.0048503875732}}, \"EndTime\": 1592850399.038302, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850396.67386}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.518011306 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=265, train loss <loss>=3.149080775\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] Epoch[266] Batch[0] avg_epoch_loss=3.165924\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=266, batch=0 train loss <loss>=3.16592431068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:40 INFO 140052404180800] Epoch[266] Batch[5] avg_epoch_loss=3.184405\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=266, batch=5 train loss <loss>=3.18440496922\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:40 INFO 140052404180800] Epoch[266] Batch [5]#011Speed: 331.17 samples/sec#011loss=3.184405\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:41 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2235.6221675872803, \"sum\": 2235.6221675872803, \"min\": 2235.6221675872803}}, \"EndTime\": 1592850401.274486, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850399.038384}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:41 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.996278709 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:41 INFO 140052404180800] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=266, train loss <loss>=3.14351916313\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:41 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:41 INFO 140052404180800] Epoch[267] Batch[0] avg_epoch_loss=3.123758\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=267, batch=0 train loss <loss>=3.12375807762\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:42 INFO 140052404180800] Epoch[267] Batch[5] avg_epoch_loss=3.120798\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=267, batch=5 train loss <loss>=3.12079755465\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:42 INFO 140052404180800] Epoch[267] Batch [5]#011Speed: 333.59 samples/sec#011loss=3.120798\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:43 INFO 140052404180800] Epoch[267] Batch[10] avg_epoch_loss=3.090706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=267, batch=10 train loss <loss>=3.05459647179\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:43 INFO 140052404180800] Epoch[267] Batch [10]#011Speed: 327.15 samples/sec#011loss=3.054596\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:43 INFO 140052404180800] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2395.430088043213, \"sum\": 2395.430088043213, \"min\": 2395.430088043213}}, \"EndTime\": 1592850403.670514, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850401.27457}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.434339536 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=267, train loss <loss>=3.09070615335\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:44 INFO 140052404180800] Epoch[268] Batch[0] avg_epoch_loss=3.162074\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=268, batch=0 train loss <loss>=3.16207385063\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:45 INFO 140052404180800] Epoch[268] Batch[5] avg_epoch_loss=3.169633\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=268, batch=5 train loss <loss>=3.1696331501\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:45 INFO 140052404180800] Epoch[268] Batch [5]#011Speed: 335.17 samples/sec#011loss=3.169633\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:45 INFO 140052404180800] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2200.788974761963, \"sum\": 2200.788974761963, \"min\": 2200.788974761963}}, \"EndTime\": 1592850405.871865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850403.67059}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.067420662 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=268, train loss <loss>=3.12165465355\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:45 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:46 INFO 140052404180800] Epoch[269] Batch[0] avg_epoch_loss=3.160765\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=269, batch=0 train loss <loss>=3.16076517105\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:47 INFO 140052404180800] Epoch[269] Batch[5] avg_epoch_loss=3.221455\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=269, batch=5 train loss <loss>=3.22145549456\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:47 INFO 140052404180800] Epoch[269] Batch [5]#011Speed: 340.28 samples/sec#011loss=3.221455\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:48 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2148.7491130828857, \"sum\": 2148.7491130828857, \"min\": 2148.7491130828857}}, \"EndTime\": 1592850408.021166, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850405.871945}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:48 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=290.385180363 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:48 INFO 140052404180800] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=269, train loss <loss>=3.17351567745\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:48 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:48 INFO 140052404180800] Epoch[270] Batch[0] avg_epoch_loss=3.200179\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=270, batch=0 train loss <loss>=3.20017886162\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:49 INFO 140052404180800] Epoch[270] Batch[5] avg_epoch_loss=3.209369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=270, batch=5 train loss <loss>=3.2093688647\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:49 INFO 140052404180800] Epoch[270] Batch [5]#011Speed: 337.87 samples/sec#011loss=3.209369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:50 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2166.4769649505615, \"sum\": 2166.4769649505615, \"min\": 2166.4769649505615}}, \"EndTime\": 1592850410.188229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850408.021249}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.009655616 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=270, train loss <loss>=3.24865989685\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:50 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:50 INFO 140052404180800] Epoch[271] Batch[0] avg_epoch_loss=3.156445\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=271, batch=0 train loss <loss>=3.15644526482\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:51 INFO 140052404180800] Epoch[271] Batch[5] avg_epoch_loss=3.204340\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=271, batch=5 train loss <loss>=3.20433966319\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:51 INFO 140052404180800] Epoch[271] Batch [5]#011Speed: 329.07 samples/sec#011loss=3.204340\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:52 INFO 140052404180800] Epoch[271] Batch[10] avg_epoch_loss=3.113276\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=271, batch=10 train loss <loss>=3.00400013924\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:52 INFO 140052404180800] Epoch[271] Batch [10]#011Speed: 330.01 samples/sec#011loss=3.004000\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:52 INFO 140052404180800] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2399.653911590576, \"sum\": 2399.653911590576, \"min\": 2399.653911590576}}, \"EndTime\": 1592850412.588457, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850410.188309}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.692701457 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=271, train loss <loss>=3.11327624321\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:53 INFO 140052404180800] Epoch[272] Batch[0] avg_epoch_loss=3.237695\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=272, batch=0 train loss <loss>=3.23769450188\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:54 INFO 140052404180800] Epoch[272] Batch[5] avg_epoch_loss=3.292207\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=272, batch=5 train loss <loss>=3.29220708211\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:54 INFO 140052404180800] Epoch[272] Batch [5]#011Speed: 334.77 samples/sec#011loss=3.292207\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] Epoch[272] Batch[10] avg_epoch_loss=3.347947\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=272, batch=10 train loss <loss>=3.41483464241\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] Epoch[272] Batch [10]#011Speed: 314.00 samples/sec#011loss=3.414835\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2441.8680667877197, \"sum\": 2441.8680667877197, \"min\": 2441.8680667877197}}, \"EndTime\": 1592850415.030852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850412.588534}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.683638823 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=272, train loss <loss>=3.34794688225\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] Epoch[273] Batch[0] avg_epoch_loss=3.215004\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=273, batch=0 train loss <loss>=3.21500444412\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:56 INFO 140052404180800] Epoch[273] Batch[5] avg_epoch_loss=3.204382\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=273, batch=5 train loss <loss>=3.20438230038\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:56 INFO 140052404180800] Epoch[273] Batch [5]#011Speed: 315.06 samples/sec#011loss=3.204382\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] Epoch[273] Batch[10] avg_epoch_loss=3.283286\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=273, batch=10 train loss <loss>=3.37797093391\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] Epoch[273] Batch [10]#011Speed: 330.92 samples/sec#011loss=3.377971\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2446.9408988952637, \"sum\": 2446.9408988952637, \"min\": 2446.9408988952637}}, \"EndTime\": 1592850417.478332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850415.030915}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=263.173682357 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=273, train loss <loss>=3.28328622471\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] Epoch[274] Batch[0] avg_epoch_loss=3.269498\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=274, batch=0 train loss <loss>=3.26949763298\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:58 INFO 140052404180800] Epoch[274] Batch[5] avg_epoch_loss=3.173708\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=274, batch=5 train loss <loss>=3.17370788256\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:58 INFO 140052404180800] Epoch[274] Batch [5]#011Speed: 337.50 samples/sec#011loss=3.173708\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:59 INFO 140052404180800] Epoch[274] Batch[10] avg_epoch_loss=3.250561\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=274, batch=10 train loss <loss>=3.3427839756\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:59 INFO 140052404180800] Epoch[274] Batch [10]#011Speed: 335.34 samples/sec#011loss=3.342784\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:59 INFO 140052404180800] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2385.5268955230713, \"sum\": 2385.5268955230713, \"min\": 2385.5268955230713}}, \"EndTime\": 1592850419.864345, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850417.478408}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.368278161 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=274, train loss <loss>=3.25056065213\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:26:59 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:00 INFO 140052404180800] Epoch[275] Batch[0] avg_epoch_loss=3.115043\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=275, batch=0 train loss <loss>=3.1150431633\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:01 INFO 140052404180800] Epoch[275] Batch[5] avg_epoch_loss=3.119113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=275, batch=5 train loss <loss>=3.11911292871\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:01 INFO 140052404180800] Epoch[275] Batch [5]#011Speed: 328.86 samples/sec#011loss=3.119113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:02 INFO 140052404180800] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2222.7330207824707, \"sum\": 2222.7330207824707, \"min\": 2222.7330207824707}}, \"EndTime\": 1592850422.087533, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850419.864419}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:02 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.422700715 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:02 INFO 140052404180800] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=275, train loss <loss>=3.15306351185\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:02 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:02 INFO 140052404180800] Epoch[276] Batch[0] avg_epoch_loss=3.059471\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=276, batch=0 train loss <loss>=3.05947113037\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:03 INFO 140052404180800] Epoch[276] Batch[5] avg_epoch_loss=3.143416\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=276, batch=5 train loss <loss>=3.14341561\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:03 INFO 140052404180800] Epoch[276] Batch [5]#011Speed: 337.66 samples/sec#011loss=3.143416\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:04 INFO 140052404180800] Epoch[276] Batch[10] avg_epoch_loss=3.156104\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=276, batch=10 train loss <loss>=3.17132930756\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:04 INFO 140052404180800] Epoch[276] Batch [10]#011Speed: 327.63 samples/sec#011loss=3.171329\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:04 INFO 140052404180800] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2440.232038497925, \"sum\": 2440.232038497925, \"min\": 2440.232038497925}}, \"EndTime\": 1592850424.528271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850422.087613}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:04 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.600580566 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:04 INFO 140052404180800] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=276, train loss <loss>=3.15610365434\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:04 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:05 INFO 140052404180800] Epoch[277] Batch[0] avg_epoch_loss=3.182917\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=277, batch=0 train loss <loss>=3.18291687965\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:05 INFO 140052404180800] Epoch[277] Batch[5] avg_epoch_loss=3.210406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=277, batch=5 train loss <loss>=3.21040642262\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:05 INFO 140052404180800] Epoch[277] Batch [5]#011Speed: 330.92 samples/sec#011loss=3.210406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:06 INFO 140052404180800] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2219.099998474121, \"sum\": 2219.099998474121, \"min\": 2219.099998474121}}, \"EndTime\": 1592850426.747848, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850424.528346}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.828297903 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=277, train loss <loss>=3.18987863064\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:07 INFO 140052404180800] Epoch[278] Batch[0] avg_epoch_loss=3.223674\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=278, batch=0 train loss <loss>=3.2236738205\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:08 INFO 140052404180800] Epoch[278] Batch[5] avg_epoch_loss=3.150460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=278, batch=5 train loss <loss>=3.15045956771\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:08 INFO 140052404180800] Epoch[278] Batch [5]#011Speed: 333.72 samples/sec#011loss=3.150460\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:08 INFO 140052404180800] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2229.9599647521973, \"sum\": 2229.9599647521973, \"min\": 2229.9599647521973}}, \"EndTime\": 1592850428.978348, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850426.74793}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.949746649 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=278, train loss <loss>=3.12501153946\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:09 INFO 140052404180800] Epoch[279] Batch[0] avg_epoch_loss=3.195648\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=279, batch=0 train loss <loss>=3.19564819336\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:10 INFO 140052404180800] Epoch[279] Batch[5] avg_epoch_loss=3.139001\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=279, batch=5 train loss <loss>=3.13900077343\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:10 INFO 140052404180800] Epoch[279] Batch [5]#011Speed: 335.58 samples/sec#011loss=3.139001\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:11 INFO 140052404180800] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2207.7670097351074, \"sum\": 2207.7670097351074, \"min\": 2207.7670097351074}}, \"EndTime\": 1592850431.186642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850428.97843}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:11 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.43494714 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:11 INFO 140052404180800] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=279, train loss <loss>=3.12363138199\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:11 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:11 INFO 140052404180800] Epoch[280] Batch[0] avg_epoch_loss=2.992054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=280, batch=0 train loss <loss>=2.99205422401\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:12 INFO 140052404180800] Epoch[280] Batch[5] avg_epoch_loss=3.047093\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=280, batch=5 train loss <loss>=3.04709251722\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:12 INFO 140052404180800] Epoch[280] Batch [5]#011Speed: 335.58 samples/sec#011loss=3.047093\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] Epoch[280] Batch[10] avg_epoch_loss=3.053466\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=280, batch=10 train loss <loss>=3.06111330986\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] Epoch[280] Batch [10]#011Speed: 331.13 samples/sec#011loss=3.061113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2393.5391902923584, \"sum\": 2393.5391902923584, \"min\": 2393.5391902923584}}, \"EndTime\": 1592850433.580709, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850431.186725}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.818277439 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=280, train loss <loss>=3.05346560478\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:13 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_d7512a8e-9598-4eee-8dc3-6be6b4614221-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 90.12103080749512, \"sum\": 90.12103080749512, \"min\": 90.12103080749512}}, \"EndTime\": 1592850433.671372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850433.580787}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:14 INFO 140052404180800] Epoch[281] Batch[0] avg_epoch_loss=3.195709\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=281, batch=0 train loss <loss>=3.1957089901\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:15 INFO 140052404180800] Epoch[281] Batch[5] avg_epoch_loss=3.161789\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=281, batch=5 train loss <loss>=3.16178882122\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:15 INFO 140052404180800] Epoch[281] Batch [5]#011Speed: 338.83 samples/sec#011loss=3.161789\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] Epoch[281] Batch[10] avg_epoch_loss=3.145338\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=281, batch=10 train loss <loss>=3.12559757233\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] Epoch[281] Batch [10]#011Speed: 324.55 samples/sec#011loss=3.125598\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2398.653984069824, \"sum\": 2398.653984069824, \"min\": 2398.653984069824}}, \"EndTime\": 1592850436.070155, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850433.671446}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.057086002 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=281, train loss <loss>=3.14533825354\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] Epoch[282] Batch[0] avg_epoch_loss=3.255701\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=282, batch=0 train loss <loss>=3.25570106506\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:17 INFO 140052404180800] Epoch[282] Batch[5] avg_epoch_loss=3.160073\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=282, batch=5 train loss <loss>=3.16007316113\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:17 INFO 140052404180800] Epoch[282] Batch [5]#011Speed: 339.17 samples/sec#011loss=3.160073\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] Epoch[282] Batch[10] avg_epoch_loss=3.089673\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=282, batch=10 train loss <loss>=3.00519313812\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] Epoch[282] Batch [10]#011Speed: 330.09 samples/sec#011loss=3.005193\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2379.573106765747, \"sum\": 2379.573106765747, \"min\": 2379.573106765747}}, \"EndTime\": 1592850438.450246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850436.070231}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.970214559 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=282, train loss <loss>=3.08967315067\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] Epoch[283] Batch[0] avg_epoch_loss=3.015770\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=283, batch=0 train loss <loss>=3.01577043533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:19 INFO 140052404180800] Epoch[283] Batch[5] avg_epoch_loss=3.128900\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=283, batch=5 train loss <loss>=3.12889961402\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:19 INFO 140052404180800] Epoch[283] Batch [5]#011Speed: 334.55 samples/sec#011loss=3.128900\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:20 INFO 140052404180800] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1991.8549060821533, \"sum\": 1991.8549060821533, \"min\": 1991.8549060821533}}, \"EndTime\": 1592850440.442611, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850438.450321}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.162287475 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=283, train loss <loss>=3.13355588913\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:20 INFO 140052404180800] Epoch[284] Batch[0] avg_epoch_loss=3.146819\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=284, batch=0 train loss <loss>=3.1468193531\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:21 INFO 140052404180800] Epoch[284] Batch[5] avg_epoch_loss=3.210350\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=284, batch=5 train loss <loss>=3.21035039425\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:21 INFO 140052404180800] Epoch[284] Batch [5]#011Speed: 328.97 samples/sec#011loss=3.210350\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:22 INFO 140052404180800] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2177.7961254119873, \"sum\": 2177.7961254119873, \"min\": 2177.7961254119873}}, \"EndTime\": 1592850442.620931, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850440.442681}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.135987408 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=284, train loss <loss>=3.20096895695\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:23 INFO 140052404180800] Epoch[285] Batch[0] avg_epoch_loss=3.098489\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=285, batch=0 train loss <loss>=3.09848928452\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:24 INFO 140052404180800] Epoch[285] Batch[5] avg_epoch_loss=3.058368\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=285, batch=5 train loss <loss>=3.05836832523\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:24 INFO 140052404180800] Epoch[285] Batch [5]#011Speed: 336.18 samples/sec#011loss=3.058368\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] Epoch[285] Batch[10] avg_epoch_loss=3.123343\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=285, batch=10 train loss <loss>=3.20131173134\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] Epoch[285] Batch [10]#011Speed: 322.00 samples/sec#011loss=3.201312\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2410.611152648926, \"sum\": 2410.611152648926, \"min\": 2410.611152648926}}, \"EndTime\": 1592850445.032184, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850442.621009}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.924787194 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=285, train loss <loss>=3.12334260074\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] Epoch[286] Batch[0] avg_epoch_loss=3.252021\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=286, batch=0 train loss <loss>=3.2520210743\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:26 INFO 140052404180800] Epoch[286] Batch[5] avg_epoch_loss=3.195860\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=286, batch=5 train loss <loss>=3.195860068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:26 INFO 140052404180800] Epoch[286] Batch [5]#011Speed: 330.28 samples/sec#011loss=3.195860\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] Epoch[286] Batch[10] avg_epoch_loss=3.206244\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=286, batch=10 train loss <loss>=3.21870560646\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] Epoch[286] Batch [10]#011Speed: 333.44 samples/sec#011loss=3.218706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2449.2249488830566, \"sum\": 2449.2249488830566, \"min\": 2449.2249488830566}}, \"EndTime\": 1592850447.481929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850445.032261}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.459913194 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=286, train loss <loss>=3.20624440367\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] Epoch[287] Batch[0] avg_epoch_loss=2.901825\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=287, batch=0 train loss <loss>=2.90182471275\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:28 INFO 140052404180800] Epoch[287] Batch[5] avg_epoch_loss=3.077399\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=287, batch=5 train loss <loss>=3.07739937305\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:28 INFO 140052404180800] Epoch[287] Batch [5]#011Speed: 335.63 samples/sec#011loss=3.077399\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:29 INFO 140052404180800] Epoch[287] Batch[10] avg_epoch_loss=3.136485\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=287, batch=10 train loss <loss>=3.20738806725\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:29 INFO 140052404180800] Epoch[287] Batch [10]#011Speed: 330.38 samples/sec#011loss=3.207388\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:30 INFO 140052404180800] processed a total of 713 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2586.782932281494, \"sum\": 2586.782932281494, \"min\": 2586.782932281494}}, \"EndTime\": 1592850450.069242, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850447.482009}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:30 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.620204427 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:30 INFO 140052404180800] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=287, train loss <loss>=3.08054876328\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:30 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:30 INFO 140052404180800] Epoch[288] Batch[0] avg_epoch_loss=3.217130\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=288, batch=0 train loss <loss>=3.21712994576\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:31 INFO 140052404180800] Epoch[288] Batch[5] avg_epoch_loss=3.188426\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=288, batch=5 train loss <loss>=3.1884260575\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:31 INFO 140052404180800] Epoch[288] Batch [5]#011Speed: 319.32 samples/sec#011loss=3.188426\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] Epoch[288] Batch[10] avg_epoch_loss=3.169145\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=288, batch=10 train loss <loss>=3.1460085392\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] Epoch[288] Batch [10]#011Speed: 336.30 samples/sec#011loss=3.146009\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2408.735990524292, \"sum\": 2408.735990524292, \"min\": 2408.735990524292}}, \"EndTime\": 1592850452.478544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850450.069314}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.707857138 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=288, train loss <loss>=3.16914536736\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] Epoch[289] Batch[0] avg_epoch_loss=3.012647\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=289, batch=0 train loss <loss>=3.01264739037\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:33 INFO 140052404180800] Epoch[289] Batch[5] avg_epoch_loss=3.114053\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=289, batch=5 train loss <loss>=3.11405273279\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:33 INFO 140052404180800] Epoch[289] Batch [5]#011Speed: 334.16 samples/sec#011loss=3.114053\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:34 INFO 140052404180800] Epoch[289] Batch[10] avg_epoch_loss=3.079715\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=289, batch=10 train loss <loss>=3.03850893974\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:34 INFO 140052404180800] Epoch[289] Batch [10]#011Speed: 333.50 samples/sec#011loss=3.038509\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:34 INFO 140052404180800] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2383.2380771636963, \"sum\": 2383.2380771636963, \"min\": 2383.2380771636963}}, \"EndTime\": 1592850454.862298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850452.478621}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.787633296 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=289, train loss <loss>=3.07971464504\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:34 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:35 INFO 140052404180800] Epoch[290] Batch[0] avg_epoch_loss=3.199306\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=290, batch=0 train loss <loss>=3.19930648804\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:36 INFO 140052404180800] Epoch[290] Batch[5] avg_epoch_loss=3.173559\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=290, batch=5 train loss <loss>=3.1735585928\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:36 INFO 140052404180800] Epoch[290] Batch [5]#011Speed: 326.77 samples/sec#011loss=3.173559\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:37 INFO 140052404180800] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2203.885078430176, \"sum\": 2203.885078430176, \"min\": 2203.885078430176}}, \"EndTime\": 1592850457.066719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850454.862378}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:37 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.213637583 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:37 INFO 140052404180800] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=290, train loss <loss>=3.1234446764\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:37 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:37 INFO 140052404180800] Epoch[291] Batch[0] avg_epoch_loss=3.187588\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=291, batch=0 train loss <loss>=3.18758821487\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:38 INFO 140052404180800] Epoch[291] Batch[5] avg_epoch_loss=3.192377\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=291, batch=5 train loss <loss>=3.19237669309\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:38 INFO 140052404180800] Epoch[291] Batch [5]#011Speed: 336.64 samples/sec#011loss=3.192377\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:39 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2162.7418994903564, \"sum\": 2162.7418994903564, \"min\": 2162.7418994903564}}, \"EndTime\": 1592850459.230014, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850457.066801}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:39 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.508485674 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:39 INFO 140052404180800] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=291, train loss <loss>=3.12918512821\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:39 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:39 INFO 140052404180800] Epoch[292] Batch[0] avg_epoch_loss=3.011039\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=292, batch=0 train loss <loss>=3.01103949547\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:40 INFO 140052404180800] Epoch[292] Batch[5] avg_epoch_loss=3.135677\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=292, batch=5 train loss <loss>=3.13567749659\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:40 INFO 140052404180800] Epoch[292] Batch [5]#011Speed: 334.51 samples/sec#011loss=3.135677\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:41 INFO 140052404180800] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2169.8999404907227, \"sum\": 2169.8999404907227, \"min\": 2169.8999404907227}}, \"EndTime\": 1592850461.400442, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850459.230088}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:41 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.791834461 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:41 INFO 140052404180800] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=292, train loss <loss>=3.16345658302\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:41 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:41 INFO 140052404180800] Epoch[293] Batch[0] avg_epoch_loss=3.037604\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=293, batch=0 train loss <loss>=3.03760433197\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:42 INFO 140052404180800] Epoch[293] Batch[5] avg_epoch_loss=3.107190\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=293, batch=5 train loss <loss>=3.1071900924\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:42 INFO 140052404180800] Epoch[293] Batch [5]#011Speed: 338.18 samples/sec#011loss=3.107190\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:43 INFO 140052404180800] Epoch[293] Batch[10] avg_epoch_loss=3.073670\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=293, batch=10 train loss <loss>=3.03344573975\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:43 INFO 140052404180800] Epoch[293] Batch [10]#011Speed: 330.98 samples/sec#011loss=3.033446\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:43 INFO 140052404180800] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2351.186990737915, \"sum\": 2351.186990737915, \"min\": 2351.186990737915}}, \"EndTime\": 1592850463.752162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850461.400518}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.776421413 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=293, train loss <loss>=3.07366993211\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:44 INFO 140052404180800] Epoch[294] Batch[0] avg_epoch_loss=3.103429\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=294, batch=0 train loss <loss>=3.10342860222\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:45 INFO 140052404180800] Epoch[294] Batch[5] avg_epoch_loss=3.060044\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=294, batch=5 train loss <loss>=3.06004377206\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:45 INFO 140052404180800] Epoch[294] Batch [5]#011Speed: 338.94 samples/sec#011loss=3.060044\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:45 INFO 140052404180800] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2178.3430576324463, \"sum\": 2178.3430576324463, \"min\": 2178.3430576324463}}, \"EndTime\": 1592850465.931008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850463.752239}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.735937157 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=294, train loss <loss>=3.06566371918\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:45 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:46 INFO 140052404180800] Epoch[295] Batch[0] avg_epoch_loss=2.963912\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=295, batch=0 train loss <loss>=2.96391224861\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:47 INFO 140052404180800] Epoch[295] Batch[5] avg_epoch_loss=3.127143\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=295, batch=5 train loss <loss>=3.12714266777\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:47 INFO 140052404180800] Epoch[295] Batch [5]#011Speed: 340.42 samples/sec#011loss=3.127143\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:48 INFO 140052404180800] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2164.433002471924, \"sum\": 2164.433002471924, \"min\": 2164.433002471924}}, \"EndTime\": 1592850468.095961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850465.93109}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:48 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=291.515580728 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:48 INFO 140052404180800] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=295, train loss <loss>=3.18961806297\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:48 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:48 INFO 140052404180800] Epoch[296] Batch[0] avg_epoch_loss=3.139122\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=296, batch=0 train loss <loss>=3.13912153244\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:49 INFO 140052404180800] Epoch[296] Batch[5] avg_epoch_loss=3.150057\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=296, batch=5 train loss <loss>=3.15005703767\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:49 INFO 140052404180800] Epoch[296] Batch [5]#011Speed: 335.72 samples/sec#011loss=3.150057\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:50 INFO 140052404180800] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2164.263963699341, \"sum\": 2164.263963699341, \"min\": 2164.263963699341}}, \"EndTime\": 1592850470.260759, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850468.096041}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.148813299 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=296, train loss <loss>=3.11050510406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:50 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:50 INFO 140052404180800] Epoch[297] Batch[0] avg_epoch_loss=3.213009\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=297, batch=0 train loss <loss>=3.21300911903\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:51 INFO 140052404180800] Epoch[297] Batch[5] avg_epoch_loss=3.101748\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=297, batch=5 train loss <loss>=3.10174842676\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:51 INFO 140052404180800] Epoch[297] Batch [5]#011Speed: 331.20 samples/sec#011loss=3.101748\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] Epoch[297] Batch[10] avg_epoch_loss=2.998412\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=297, batch=10 train loss <loss>=2.87440738678\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] Epoch[297] Batch [10]#011Speed: 332.43 samples/sec#011loss=2.874407\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2361.909866333008, \"sum\": 2361.909866333008, \"min\": 2361.909866333008}}, \"EndTime\": 1592850472.623214, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850470.26082}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.496237146 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=297, train loss <loss>=2.9984115904\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:52 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_11c96d5e-34a7-4ac6-a648-38af45ff166e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.492919921875, \"sum\": 61.492919921875, \"min\": 61.492919921875}}, \"EndTime\": 1592850472.685271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850472.623275}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:53 INFO 140052404180800] Epoch[298] Batch[0] avg_epoch_loss=3.152094\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=298, batch=0 train loss <loss>=3.15209388733\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:54 INFO 140052404180800] Epoch[298] Batch[5] avg_epoch_loss=3.089749\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=298, batch=5 train loss <loss>=3.0897491773\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:54 INFO 140052404180800] Epoch[298] Batch [5]#011Speed: 334.18 samples/sec#011loss=3.089749\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] Epoch[298] Batch[10] avg_epoch_loss=3.034003\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=298, batch=10 train loss <loss>=2.96710839272\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] Epoch[298] Batch [10]#011Speed: 331.37 samples/sec#011loss=2.967108\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2381.4949989318848, \"sum\": 2381.4949989318848, \"min\": 2381.4949989318848}}, \"EndTime\": 1592850475.066904, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850472.685344}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.62054047 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=298, train loss <loss>=3.03400336612\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] Epoch[299] Batch[0] avg_epoch_loss=3.178619\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=299, batch=0 train loss <loss>=3.17861938477\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:56 INFO 140052404180800] Epoch[299] Batch[5] avg_epoch_loss=3.154619\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=299, batch=5 train loss <loss>=3.15461921692\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:56 INFO 140052404180800] Epoch[299] Batch [5]#011Speed: 332.63 samples/sec#011loss=3.154619\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:57 INFO 140052404180800] processed a total of 579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2213.908910751343, \"sum\": 2213.908910751343, \"min\": 2213.908910751343}}, \"EndTime\": 1592850477.281321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850475.066981}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=261.514181035 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=299, train loss <loss>=3.03499386311\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:57 INFO 140052404180800] Epoch[300] Batch[0] avg_epoch_loss=3.063576\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=300, batch=0 train loss <loss>=3.06357622147\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:58 INFO 140052404180800] Epoch[300] Batch[5] avg_epoch_loss=3.102881\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=300, batch=5 train loss <loss>=3.10288067659\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:58 INFO 140052404180800] Epoch[300] Batch [5]#011Speed: 338.82 samples/sec#011loss=3.102881\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] Epoch[300] Batch[10] avg_epoch_loss=2.982619\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=300, batch=10 train loss <loss>=2.83830473423\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] Epoch[300] Batch [10]#011Speed: 337.68 samples/sec#011loss=2.838305\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2363.4018898010254, \"sum\": 2363.4018898010254, \"min\": 2363.4018898010254}}, \"EndTime\": 1592850479.645292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850477.281403}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.321848152 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=300, train loss <loss>=2.98261888461\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:27:59 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_b314b202-d5e4-4daf-bef1-fa03816d6599-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 65.33312797546387, \"sum\": 65.33312797546387, \"min\": 65.33312797546387}}, \"EndTime\": 1592850479.711203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850479.645369}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:00 INFO 140052404180800] Epoch[301] Batch[0] avg_epoch_loss=2.800977\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=301, batch=0 train loss <loss>=2.80097723007\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:01 INFO 140052404180800] Epoch[301] Batch[5] avg_epoch_loss=3.004940\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=301, batch=5 train loss <loss>=3.00494031111\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:01 INFO 140052404180800] Epoch[301] Batch [5]#011Speed: 335.07 samples/sec#011loss=3.004940\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:01 INFO 140052404180800] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2221.7869758605957, \"sum\": 2221.7869758605957, \"min\": 2221.7869758605957}}, \"EndTime\": 1592850481.93312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850479.711266}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:01 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.787739118 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:01 INFO 140052404180800] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=301, train loss <loss>=3.04284341335\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:01 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:02 INFO 140052404180800] Epoch[302] Batch[0] avg_epoch_loss=3.308096\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=302, batch=0 train loss <loss>=3.30809640884\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:03 INFO 140052404180800] Epoch[302] Batch[5] avg_epoch_loss=3.134229\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=302, batch=5 train loss <loss>=3.13422858715\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:03 INFO 140052404180800] Epoch[302] Batch [5]#011Speed: 338.09 samples/sec#011loss=3.134229\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] Epoch[302] Batch[10] avg_epoch_loss=3.188346\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=302, batch=10 train loss <loss>=3.2532869339\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] Epoch[302] Batch [10]#011Speed: 336.36 samples/sec#011loss=3.253287\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2363.0008697509766, \"sum\": 2363.0008697509766, \"min\": 2363.0008697509766}}, \"EndTime\": 1592850484.296715, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850481.933204}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.36845149 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=302, train loss <loss>=3.18834601749\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] Epoch[303] Batch[0] avg_epoch_loss=3.327976\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=303, batch=0 train loss <loss>=3.32797574997\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:05 INFO 140052404180800] Epoch[303] Batch[5] avg_epoch_loss=3.145463\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=303, batch=5 train loss <loss>=3.14546259244\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:05 INFO 140052404180800] Epoch[303] Batch [5]#011Speed: 334.36 samples/sec#011loss=3.145463\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:06 INFO 140052404180800] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2217.705011367798, \"sum\": 2217.705011367798, \"min\": 2217.705011367798}}, \"EndTime\": 1592850486.514914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850484.296791}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.064019908 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=303, train loss <loss>=3.12589259148\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:06 INFO 140052404180800] Epoch[304] Batch[0] avg_epoch_loss=3.153322\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=304, batch=0 train loss <loss>=3.15332174301\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:07 INFO 140052404180800] Epoch[304] Batch[5] avg_epoch_loss=3.084742\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=304, batch=5 train loss <loss>=3.08474242687\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:07 INFO 140052404180800] Epoch[304] Batch [5]#011Speed: 331.58 samples/sec#011loss=3.084742\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:08 INFO 140052404180800] Epoch[304] Batch[10] avg_epoch_loss=3.007578\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=304, batch=10 train loss <loss>=2.91498022079\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:08 INFO 140052404180800] Epoch[304] Batch [10]#011Speed: 336.17 samples/sec#011loss=2.914980\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:08 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2353.384017944336, \"sum\": 2353.384017944336, \"min\": 2353.384017944336}}, \"EndTime\": 1592850488.868845, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850486.514988}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.360299636 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=304, train loss <loss>=3.00757778775\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:09 INFO 140052404180800] Epoch[305] Batch[0] avg_epoch_loss=3.195371\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=305, batch=0 train loss <loss>=3.19537138939\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:10 INFO 140052404180800] Epoch[305] Batch[5] avg_epoch_loss=3.099320\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=305, batch=5 train loss <loss>=3.09932033221\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:10 INFO 140052404180800] Epoch[305] Batch [5]#011Speed: 334.45 samples/sec#011loss=3.099320\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:11 INFO 140052404180800] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2199.2359161376953, \"sum\": 2199.2359161376953, \"min\": 2199.2359161376953}}, \"EndTime\": 1592850491.0686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850488.868924}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:11 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.897528135 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:11 INFO 140052404180800] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=305, train loss <loss>=3.17273097038\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:11 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:11 INFO 140052404180800] Epoch[306] Batch[0] avg_epoch_loss=3.091984\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=306, batch=0 train loss <loss>=3.091984272\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:12 INFO 140052404180800] Epoch[306] Batch[5] avg_epoch_loss=3.086971\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=306, batch=5 train loss <loss>=3.08697052797\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:12 INFO 140052404180800] Epoch[306] Batch [5]#011Speed: 332.77 samples/sec#011loss=3.086971\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:13 INFO 140052404180800] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2198.168992996216, \"sum\": 2198.168992996216, \"min\": 2198.168992996216}}, \"EndTime\": 1592850493.267299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850491.068684}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=282.037785387 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=306, train loss <loss>=3.07873313427\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:13 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:13 INFO 140052404180800] Epoch[307] Batch[0] avg_epoch_loss=2.981163\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=307, batch=0 train loss <loss>=2.98116254807\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:14 INFO 140052404180800] Epoch[307] Batch[5] avg_epoch_loss=3.055596\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=307, batch=5 train loss <loss>=3.05559583505\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:14 INFO 140052404180800] Epoch[307] Batch [5]#011Speed: 334.45 samples/sec#011loss=3.055596\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:15 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2206.498861312866, \"sum\": 2206.498861312866, \"min\": 2206.498861312866}}, \"EndTime\": 1592850495.474355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850493.26738}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:15 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.864726413 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:15 INFO 140052404180800] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=307, train loss <loss>=3.02948422432\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:15 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:15 INFO 140052404180800] Epoch[308] Batch[0] avg_epoch_loss=3.065878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=308, batch=0 train loss <loss>=3.06587815285\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:16 INFO 140052404180800] Epoch[308] Batch[5] avg_epoch_loss=3.078082\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=308, batch=5 train loss <loss>=3.07808236281\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:16 INFO 140052404180800] Epoch[308] Batch [5]#011Speed: 332.24 samples/sec#011loss=3.078082\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:17 INFO 140052404180800] Epoch[308] Batch[10] avg_epoch_loss=3.020600\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=308, batch=10 train loss <loss>=2.95162081718\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:17 INFO 140052404180800] Epoch[308] Batch [10]#011Speed: 336.54 samples/sec#011loss=2.951621\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:17 INFO 140052404180800] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2392.28892326355, \"sum\": 2392.28892326355, \"min\": 2392.28892326355}}, \"EndTime\": 1592850497.867176, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850495.474435}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:17 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.440396603 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:17 INFO 140052404180800] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=308, train loss <loss>=3.02059984207\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:17 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:18 INFO 140052404180800] Epoch[309] Batch[0] avg_epoch_loss=3.107312\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=309, batch=0 train loss <loss>=3.10731196404\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:19 INFO 140052404180800] Epoch[309] Batch[5] avg_epoch_loss=3.173041\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=309, batch=5 train loss <loss>=3.17304134369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:19 INFO 140052404180800] Epoch[309] Batch [5]#011Speed: 340.77 samples/sec#011loss=3.173041\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] Epoch[309] Batch[10] avg_epoch_loss=3.144946\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=309, batch=10 train loss <loss>=3.11123137474\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] Epoch[309] Batch [10]#011Speed: 335.80 samples/sec#011loss=3.111231\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2365.8008575439453, \"sum\": 2365.8008575439453, \"min\": 2365.8008575439453}}, \"EndTime\": 1592850500.233454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850497.867248}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.031003277 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=309, train loss <loss>=3.14494590326\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] Epoch[310] Batch[0] avg_epoch_loss=3.206900\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=310, batch=0 train loss <loss>=3.20690011978\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:21 INFO 140052404180800] Epoch[310] Batch[5] avg_epoch_loss=3.156361\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=310, batch=5 train loss <loss>=3.15636138121\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:21 INFO 140052404180800] Epoch[310] Batch [5]#011Speed: 317.43 samples/sec#011loss=3.156361\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:22 INFO 140052404180800] Epoch[310] Batch[10] avg_epoch_loss=3.158806\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=310, batch=10 train loss <loss>=3.16173853874\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:22 INFO 140052404180800] Epoch[310] Batch [10]#011Speed: 336.32 samples/sec#011loss=3.161739\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:22 INFO 140052404180800] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2428.770065307617, \"sum\": 2428.770065307617, \"min\": 2428.770065307617}}, \"EndTime\": 1592850502.662759, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850500.233557}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.317335315 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=310, train loss <loss>=3.15880554373\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:23 INFO 140052404180800] Epoch[311] Batch[0] avg_epoch_loss=2.959476\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=311, batch=0 train loss <loss>=2.95947551727\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:24 INFO 140052404180800] Epoch[311] Batch[5] avg_epoch_loss=3.112360\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=311, batch=5 train loss <loss>=3.11235984166\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:24 INFO 140052404180800] Epoch[311] Batch [5]#011Speed: 338.57 samples/sec#011loss=3.112360\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:24 INFO 140052404180800] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2185.2850914001465, \"sum\": 2185.2850914001465, \"min\": 2185.2850914001465}}, \"EndTime\": 1592850504.848563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850502.662835}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:24 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.615590588 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:24 INFO 140052404180800] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=311, train loss <loss>=3.0613090992\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:24 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:25 INFO 140052404180800] Epoch[312] Batch[0] avg_epoch_loss=3.109206\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=312, batch=0 train loss <loss>=3.10920619965\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:26 INFO 140052404180800] Epoch[312] Batch[5] avg_epoch_loss=3.093684\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=312, batch=5 train loss <loss>=3.09368356069\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:26 INFO 140052404180800] Epoch[312] Batch [5]#011Speed: 325.74 samples/sec#011loss=3.093684\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:27 INFO 140052404180800] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2262.2549533843994, \"sum\": 2262.2549533843994, \"min\": 2262.2549533843994}}, \"EndTime\": 1592850507.11134, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850504.848646}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.356292651 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=312, train loss <loss>=3.05713074207\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:27 INFO 140052404180800] Epoch[313] Batch[0] avg_epoch_loss=3.113581\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=313, batch=0 train loss <loss>=3.11358118057\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:28 INFO 140052404180800] Epoch[313] Batch[5] avg_epoch_loss=3.131135\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=313, batch=5 train loss <loss>=3.1311349074\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:28 INFO 140052404180800] Epoch[313] Batch [5]#011Speed: 333.81 samples/sec#011loss=3.131135\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] Epoch[313] Batch[10] avg_epoch_loss=3.036992\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=313, batch=10 train loss <loss>=2.92402067184\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] Epoch[313] Batch [10]#011Speed: 331.99 samples/sec#011loss=2.924021\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2360.880136489868, \"sum\": 2360.880136489868, \"min\": 2360.880136489868}}, \"EndTime\": 1592850509.472719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850507.111399}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.461242412 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=313, train loss <loss>=3.03699207306\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] Epoch[314] Batch[0] avg_epoch_loss=3.199661\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=314, batch=0 train loss <loss>=3.1996614933\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:30 INFO 140052404180800] Epoch[314] Batch[5] avg_epoch_loss=3.168807\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=314, batch=5 train loss <loss>=3.1688066721\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:30 INFO 140052404180800] Epoch[314] Batch [5]#011Speed: 333.43 samples/sec#011loss=3.168807\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:31 INFO 140052404180800] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2196.194887161255, \"sum\": 2196.194887161255, \"min\": 2196.194887161255}}, \"EndTime\": 1592850511.669401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850509.47279}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:31 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.718218858 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:31 INFO 140052404180800] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=314, train loss <loss>=3.08312134743\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:31 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:32 INFO 140052404180800] Epoch[315] Batch[0] avg_epoch_loss=3.069771\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=315, batch=0 train loss <loss>=3.06977081299\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:33 INFO 140052404180800] Epoch[315] Batch[5] avg_epoch_loss=3.066332\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=315, batch=5 train loss <loss>=3.06633241971\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:33 INFO 140052404180800] Epoch[315] Batch [5]#011Speed: 333.95 samples/sec#011loss=3.066332\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] Epoch[315] Batch[10] avg_epoch_loss=3.011983\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=315, batch=10 train loss <loss>=2.94676394463\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] Epoch[315] Batch [10]#011Speed: 332.07 samples/sec#011loss=2.946764\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2387.113094329834, \"sum\": 2387.113094329834, \"min\": 2387.113094329834}}, \"EndTime\": 1592850514.057141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850511.66951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.44534927 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=315, train loss <loss>=3.01198311286\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] Epoch[316] Batch[0] avg_epoch_loss=3.130857\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=316, batch=0 train loss <loss>=3.13085651398\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:35 INFO 140052404180800] Epoch[316] Batch[5] avg_epoch_loss=3.068457\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=316, batch=5 train loss <loss>=3.06845728556\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:35 INFO 140052404180800] Epoch[316] Batch [5]#011Speed: 327.86 samples/sec#011loss=3.068457\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] Epoch[316] Batch[10] avg_epoch_loss=3.075966\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=316, batch=10 train loss <loss>=3.08497610092\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] Epoch[316] Batch [10]#011Speed: 324.98 samples/sec#011loss=3.084976\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2427.2429943084717, \"sum\": 2427.2429943084717, \"min\": 2427.2429943084717}}, \"EndTime\": 1592850516.484955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850514.057211}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.548559764 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=316, train loss <loss>=3.075965838\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] Epoch[317] Batch[0] avg_epoch_loss=2.965911\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=317, batch=0 train loss <loss>=2.96591091156\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:37 INFO 140052404180800] Epoch[317] Batch[5] avg_epoch_loss=3.073809\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=317, batch=5 train loss <loss>=3.07380906741\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:37 INFO 140052404180800] Epoch[317] Batch [5]#011Speed: 333.97 samples/sec#011loss=3.073809\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:38 INFO 140052404180800] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2189.4330978393555, \"sum\": 2189.4330978393555, \"min\": 2189.4330978393555}}, \"EndTime\": 1592850518.674906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850516.485029}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:38 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.965694704 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:38 INFO 140052404180800] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=317, train loss <loss>=3.05596950054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:38 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:39 INFO 140052404180800] Epoch[318] Batch[0] avg_epoch_loss=3.191369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=318, batch=0 train loss <loss>=3.19136881828\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:40 INFO 140052404180800] Epoch[318] Batch[5] avg_epoch_loss=3.000772\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=318, batch=5 train loss <loss>=3.00077164173\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:40 INFO 140052404180800] Epoch[318] Batch [5]#011Speed: 333.42 samples/sec#011loss=3.000772\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] Epoch[318] Batch[10] avg_epoch_loss=3.150131\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=318, batch=10 train loss <loss>=3.32936172485\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] Epoch[318] Batch [10]#011Speed: 335.01 samples/sec#011loss=3.329362\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2400.0959396362305, \"sum\": 2400.0959396362305, \"min\": 2400.0959396362305}}, \"EndTime\": 1592850521.075544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850518.674989}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.143555195 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=318, train loss <loss>=3.15013077042\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] Epoch[319] Batch[0] avg_epoch_loss=2.958406\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=319, batch=0 train loss <loss>=2.95840573311\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:42 INFO 140052404180800] Epoch[319] Batch[5] avg_epoch_loss=3.042727\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=319, batch=5 train loss <loss>=3.04272687435\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:42 INFO 140052404180800] Epoch[319] Batch [5]#011Speed: 332.48 samples/sec#011loss=3.042727\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:43 INFO 140052404180800] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2211.392879486084, \"sum\": 2211.392879486084, \"min\": 2211.392879486084}}, \"EndTime\": 1592850523.287421, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850521.075619}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=287.133889831 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=319, train loss <loss>=3.06585826874\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:43 INFO 140052404180800] Epoch[320] Batch[0] avg_epoch_loss=3.113388\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=320, batch=0 train loss <loss>=3.11338829994\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:44 INFO 140052404180800] Epoch[320] Batch[5] avg_epoch_loss=3.164724\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=320, batch=5 train loss <loss>=3.16472438971\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:44 INFO 140052404180800] Epoch[320] Batch [5]#011Speed: 334.61 samples/sec#011loss=3.164724\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:45 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2214.7741317749023, \"sum\": 2214.7741317749023, \"min\": 2214.7741317749023}}, \"EndTime\": 1592850525.50272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850523.287503}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.792573576 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=320, train loss <loss>=3.11740190983\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:45 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:45 INFO 140052404180800] Epoch[321] Batch[0] avg_epoch_loss=2.966394\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=321, batch=0 train loss <loss>=2.96639442444\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:46 INFO 140052404180800] Epoch[321] Batch[5] avg_epoch_loss=2.987948\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=321, batch=5 train loss <loss>=2.98794841766\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:46 INFO 140052404180800] Epoch[321] Batch [5]#011Speed: 329.20 samples/sec#011loss=2.987948\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:47 INFO 140052404180800] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2245.2449798583984, \"sum\": 2245.2449798583984, \"min\": 2245.2449798583984}}, \"EndTime\": 1592850527.748508, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850525.502802}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:47 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=263.65436205 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:47 INFO 140052404180800] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=321, train loss <loss>=2.94598038197\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:47 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:47 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_46001614-069f-49aa-9e66-5d0172f4c2e7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 96.36211395263672, \"sum\": 96.36211395263672, \"min\": 96.36211395263672}}, \"EndTime\": 1592850527.845477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850527.74859}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:48 INFO 140052404180800] Epoch[322] Batch[0] avg_epoch_loss=3.171887\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=322, batch=0 train loss <loss>=3.17188692093\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:49 INFO 140052404180800] Epoch[322] Batch[5] avg_epoch_loss=3.091110\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=322, batch=5 train loss <loss>=3.09111015002\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:49 INFO 140052404180800] Epoch[322] Batch [5]#011Speed: 339.17 samples/sec#011loss=3.091110\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] Epoch[322] Batch[10] avg_epoch_loss=3.044231\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=322, batch=10 train loss <loss>=2.9879755497\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] Epoch[322] Batch [10]#011Speed: 333.34 samples/sec#011loss=2.987976\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2375.9942054748535, \"sum\": 2375.9942054748535, \"min\": 2375.9942054748535}}, \"EndTime\": 1592850530.221613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850527.845556}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=277.344526466 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=322, train loss <loss>=3.04423078624\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] Epoch[323] Batch[0] avg_epoch_loss=2.862235\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=323, batch=0 train loss <loss>=2.86223506927\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:51 INFO 140052404180800] Epoch[323] Batch[5] avg_epoch_loss=3.015711\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=323, batch=5 train loss <loss>=3.01571118832\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:51 INFO 140052404180800] Epoch[323] Batch [5]#011Speed: 337.76 samples/sec#011loss=3.015711\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:52 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2241.197109222412, \"sum\": 2241.197109222412, \"min\": 2241.197109222412}}, \"EndTime\": 1592850532.463306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850530.221689}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.407854002 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=323, train loss <loss>=3.04096932411\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:52 INFO 140052404180800] Epoch[324] Batch[0] avg_epoch_loss=3.116358\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=324, batch=0 train loss <loss>=3.11635756493\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:53 INFO 140052404180800] Epoch[324] Batch[5] avg_epoch_loss=3.171237\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=324, batch=5 train loss <loss>=3.17123734951\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:53 INFO 140052404180800] Epoch[324] Batch [5]#011Speed: 332.61 samples/sec#011loss=3.171237\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:54 INFO 140052404180800] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2194.9920654296875, \"sum\": 2194.9920654296875, \"min\": 2194.9920654296875}}, \"EndTime\": 1592850534.658841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850532.46339}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:54 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=291.557670288 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:54 INFO 140052404180800] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=324, train loss <loss>=3.14371030331\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:54 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:55 INFO 140052404180800] Epoch[325] Batch[0] avg_epoch_loss=3.031583\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=325, batch=0 train loss <loss>=3.03158330917\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:56 INFO 140052404180800] Epoch[325] Batch[5] avg_epoch_loss=3.053732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=325, batch=5 train loss <loss>=3.05373239517\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:56 INFO 140052404180800] Epoch[325] Batch [5]#011Speed: 321.37 samples/sec#011loss=3.053732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] Epoch[325] Batch[10] avg_epoch_loss=3.060000\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=325, batch=10 train loss <loss>=3.06752066612\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] Epoch[325] Batch [10]#011Speed: 326.50 samples/sec#011loss=3.067521\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2448.6100673675537, \"sum\": 2448.6100673675537, \"min\": 2448.6100673675537}}, \"EndTime\": 1592850537.107958, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850534.65892}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.878569874 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=325, train loss <loss>=3.05999979106\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] Epoch[326] Batch[0] avg_epoch_loss=2.958002\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=326, batch=0 train loss <loss>=2.95800161362\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:58 INFO 140052404180800] Epoch[326] Batch[5] avg_epoch_loss=3.044251\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=326, batch=5 train loss <loss>=3.04425088565\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:58 INFO 140052404180800] Epoch[326] Batch [5]#011Speed: 336.80 samples/sec#011loss=3.044251\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] Epoch[326] Batch[10] avg_epoch_loss=3.103780\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=326, batch=10 train loss <loss>=3.17521476746\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] Epoch[326] Batch [10]#011Speed: 334.86 samples/sec#011loss=3.175215\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2396.436929702759, \"sum\": 2396.436929702759, \"min\": 2396.436929702759}}, \"EndTime\": 1592850539.504938, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850537.108038}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.885198105 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=326, train loss <loss>=3.10377992283\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] Epoch[327] Batch[0] avg_epoch_loss=3.153095\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:28:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=327, batch=0 train loss <loss>=3.15309476852\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:00 INFO 140052404180800] Epoch[327] Batch[5] avg_epoch_loss=3.061253\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=327, batch=5 train loss <loss>=3.06125255426\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:00 INFO 140052404180800] Epoch[327] Batch [5]#011Speed: 331.77 samples/sec#011loss=3.061253\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:01 INFO 140052404180800] Epoch[327] Batch[10] avg_epoch_loss=3.024088\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=327, batch=10 train loss <loss>=2.97949032784\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:01 INFO 140052404180800] Epoch[327] Batch [10]#011Speed: 312.88 samples/sec#011loss=2.979490\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:01 INFO 140052404180800] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2471.179962158203, \"sum\": 2471.179962158203, \"min\": 2471.179962158203}}, \"EndTime\": 1592850541.976614, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850539.505013}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:01 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=268.684951019 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:01 INFO 140052404180800] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=327, train loss <loss>=3.02408790588\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:01 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:02 INFO 140052404180800] Epoch[328] Batch[0] avg_epoch_loss=3.144278\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=328, batch=0 train loss <loss>=3.14427781105\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:03 INFO 140052404180800] Epoch[328] Batch[5] avg_epoch_loss=3.056149\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=328, batch=5 train loss <loss>=3.05614908536\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:03 INFO 140052404180800] Epoch[328] Batch [5]#011Speed: 331.85 samples/sec#011loss=3.056149\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:04 INFO 140052404180800] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2246.2852001190186, \"sum\": 2246.2852001190186, \"min\": 2246.2852001190186}}, \"EndTime\": 1592850544.223444, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850541.976693}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:04 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.456018989 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:04 INFO 140052404180800] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=328, train loss <loss>=3.0266700983\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:04 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:04 INFO 140052404180800] Epoch[329] Batch[0] avg_epoch_loss=3.139312\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=329, batch=0 train loss <loss>=3.13931155205\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:05 INFO 140052404180800] Epoch[329] Batch[5] avg_epoch_loss=3.063274\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=329, batch=5 train loss <loss>=3.06327354908\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:05 INFO 140052404180800] Epoch[329] Batch [5]#011Speed: 331.99 samples/sec#011loss=3.063274\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:06 INFO 140052404180800] Epoch[329] Batch[10] avg_epoch_loss=3.122625\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=329, batch=10 train loss <loss>=3.19384570122\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:06 INFO 140052404180800] Epoch[329] Batch [10]#011Speed: 331.97 samples/sec#011loss=3.193846\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:06 INFO 140052404180800] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2371.998071670532, \"sum\": 2371.998071670532, \"min\": 2371.998071670532}}, \"EndTime\": 1592850546.596086, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850544.223516}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:06 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.752743242 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:06 INFO 140052404180800] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=329, train loss <loss>=3.12262452732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:06 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:07 INFO 140052404180800] Epoch[330] Batch[0] avg_epoch_loss=3.168450\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=330, batch=0 train loss <loss>=3.16845011711\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:08 INFO 140052404180800] Epoch[330] Batch[5] avg_epoch_loss=3.140741\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=330, batch=5 train loss <loss>=3.14074063301\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:08 INFO 140052404180800] Epoch[330] Batch [5]#011Speed: 326.80 samples/sec#011loss=3.140741\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] Epoch[330] Batch[10] avg_epoch_loss=3.187212\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=330, batch=10 train loss <loss>=3.2429775238\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] Epoch[330] Batch [10]#011Speed: 332.30 samples/sec#011loss=3.242978\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2416.3100719451904, \"sum\": 2416.3100719451904, \"min\": 2416.3100719451904}}, \"EndTime\": 1592850549.012915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850546.596161}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.061804029 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=330, train loss <loss>=3.18721194701\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] Epoch[331] Batch[0] avg_epoch_loss=3.013320\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=331, batch=0 train loss <loss>=3.01332044601\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:10 INFO 140052404180800] Epoch[331] Batch[5] avg_epoch_loss=3.076300\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=331, batch=5 train loss <loss>=3.07629994551\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:10 INFO 140052404180800] Epoch[331] Batch [5]#011Speed: 324.93 samples/sec#011loss=3.076300\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:11 INFO 140052404180800] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2216.765880584717, \"sum\": 2216.765880584717, \"min\": 2216.765880584717}}, \"EndTime\": 1592850551.23021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850549.012992}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:11 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.693481788 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:11 INFO 140052404180800] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=331, train loss <loss>=3.04518811703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:11 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:11 INFO 140052404180800] Epoch[332] Batch[0] avg_epoch_loss=3.070743\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=332, batch=0 train loss <loss>=3.07074284554\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:12 INFO 140052404180800] Epoch[332] Batch[5] avg_epoch_loss=3.142202\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=332, batch=5 train loss <loss>=3.14220162233\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:12 INFO 140052404180800] Epoch[332] Batch [5]#011Speed: 331.39 samples/sec#011loss=3.142202\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:13 INFO 140052404180800] Epoch[332] Batch[10] avg_epoch_loss=3.055050\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=332, batch=10 train loss <loss>=2.95046887398\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:13 INFO 140052404180800] Epoch[332] Batch [10]#011Speed: 332.41 samples/sec#011loss=2.950469\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:13 INFO 140052404180800] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2411.9038581848145, \"sum\": 2411.9038581848145, \"min\": 2411.9038581848145}}, \"EndTime\": 1592850553.642648, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850551.230291}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:13 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.069534735 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:13 INFO 140052404180800] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=332, train loss <loss>=3.05505037308\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:13 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:14 INFO 140052404180800] Epoch[333] Batch[0] avg_epoch_loss=3.139369\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=333, batch=0 train loss <loss>=3.13936948776\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:15 INFO 140052404180800] Epoch[333] Batch[5] avg_epoch_loss=3.120323\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=333, batch=5 train loss <loss>=3.12032294273\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:15 INFO 140052404180800] Epoch[333] Batch [5]#011Speed: 332.61 samples/sec#011loss=3.120323\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:15 INFO 140052404180800] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2218.951940536499, \"sum\": 2218.951940536499, \"min\": 2218.951940536499}}, \"EndTime\": 1592850555.862147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850553.642725}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:15 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.847088515 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:15 INFO 140052404180800] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=333, train loss <loss>=3.13248136044\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:15 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:16 INFO 140052404180800] Epoch[334] Batch[0] avg_epoch_loss=3.192601\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=334, batch=0 train loss <loss>=3.19260144234\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:17 INFO 140052404180800] Epoch[334] Batch[5] avg_epoch_loss=3.104716\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=334, batch=5 train loss <loss>=3.10471598307\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:17 INFO 140052404180800] Epoch[334] Batch [5]#011Speed: 332.95 samples/sec#011loss=3.104716\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] Epoch[334] Batch[10] avg_epoch_loss=2.994587\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=334, batch=10 train loss <loss>=2.86243124008\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] Epoch[334] Batch [10]#011Speed: 334.31 samples/sec#011loss=2.862431\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2402.379035949707, \"sum\": 2402.379035949707, \"min\": 2402.379035949707}}, \"EndTime\": 1592850558.265055, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850555.862227}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.136625792 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=334, train loss <loss>=2.99458655444\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] Epoch[335] Batch[0] avg_epoch_loss=3.030247\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=335, batch=0 train loss <loss>=3.03024721146\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:19 INFO 140052404180800] Epoch[335] Batch[5] avg_epoch_loss=3.074193\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=335, batch=5 train loss <loss>=3.07419280211\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:19 INFO 140052404180800] Epoch[335] Batch [5]#011Speed: 337.44 samples/sec#011loss=3.074193\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:20 INFO 140052404180800] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2232.607126235962, \"sum\": 2232.607126235962, \"min\": 2232.607126235962}}, \"EndTime\": 1592850560.498186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850558.26513}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:20 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.031478894 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:20 INFO 140052404180800] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=335, train loss <loss>=3.08937103748\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:20 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:20 INFO 140052404180800] Epoch[336] Batch[0] avg_epoch_loss=3.267784\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=336, batch=0 train loss <loss>=3.26778411865\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:21 INFO 140052404180800] Epoch[336] Batch[5] avg_epoch_loss=3.097915\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=336, batch=5 train loss <loss>=3.09791529179\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:21 INFO 140052404180800] Epoch[336] Batch [5]#011Speed: 336.09 samples/sec#011loss=3.097915\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:22 INFO 140052404180800] Epoch[336] Batch[10] avg_epoch_loss=3.130831\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=336, batch=10 train loss <loss>=3.17032880783\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:22 INFO 140052404180800] Epoch[336] Batch [10]#011Speed: 319.11 samples/sec#011loss=3.170329\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:22 INFO 140052404180800] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2441.6048526763916, \"sum\": 2441.6048526763916, \"min\": 2441.6048526763916}}, \"EndTime\": 1592850562.940323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850560.498266}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:22 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.435396692 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:22 INFO 140052404180800] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=336, train loss <loss>=3.13083052635\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:22 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:23 INFO 140052404180800] Epoch[337] Batch[0] avg_epoch_loss=3.017326\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=337, batch=0 train loss <loss>=3.01732611656\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:24 INFO 140052404180800] Epoch[337] Batch[5] avg_epoch_loss=3.042189\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=337, batch=5 train loss <loss>=3.04218947887\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:24 INFO 140052404180800] Epoch[337] Batch [5]#011Speed: 337.27 samples/sec#011loss=3.042189\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:25 INFO 140052404180800] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2192.4548149108887, \"sum\": 2192.4548149108887, \"min\": 2192.4548149108887}}, \"EndTime\": 1592850565.133271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850562.940396}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:25 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.877928594 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:25 INFO 140052404180800] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=337, train loss <loss>=3.01503405571\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:25 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:25 INFO 140052404180800] Epoch[338] Batch[0] avg_epoch_loss=3.038280\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=338, batch=0 train loss <loss>=3.03828048706\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:26 INFO 140052404180800] Epoch[338] Batch[5] avg_epoch_loss=3.029152\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=338, batch=5 train loss <loss>=3.02915227413\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:26 INFO 140052404180800] Epoch[338] Batch [5]#011Speed: 313.85 samples/sec#011loss=3.029152\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:27 INFO 140052404180800] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2260.6019973754883, \"sum\": 2260.6019973754883, \"min\": 2260.6019973754883}}, \"EndTime\": 1592850567.394424, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850565.133351}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.575804688 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=338, train loss <loss>=3.00936951637\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:27 INFO 140052404180800] Epoch[339] Batch[0] avg_epoch_loss=3.081357\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=339, batch=0 train loss <loss>=3.08135700226\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:28 INFO 140052404180800] Epoch[339] Batch[5] avg_epoch_loss=3.021550\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=339, batch=5 train loss <loss>=3.02154990037\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:28 INFO 140052404180800] Epoch[339] Batch [5]#011Speed: 335.13 samples/sec#011loss=3.021550\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:29 INFO 140052404180800] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2176.1298179626465, \"sum\": 2176.1298179626465, \"min\": 2176.1298179626465}}, \"EndTime\": 1592850569.571095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850567.394506}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:29 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.735032839 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:29 INFO 140052404180800] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=339, train loss <loss>=3.02020609379\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:29 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:30 INFO 140052404180800] Epoch[340] Batch[0] avg_epoch_loss=3.073910\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=340, batch=0 train loss <loss>=3.07390999794\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:30 INFO 140052404180800] Epoch[340] Batch[5] avg_epoch_loss=3.049610\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=340, batch=5 train loss <loss>=3.04960970084\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:30 INFO 140052404180800] Epoch[340] Batch [5]#011Speed: 332.74 samples/sec#011loss=3.049610\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:31 INFO 140052404180800] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2213.602066040039, \"sum\": 2213.602066040039, \"min\": 2213.602066040039}}, \"EndTime\": 1592850571.785268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850569.571159}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:31 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=267.874763853 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:31 INFO 140052404180800] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=340, train loss <loss>=3.12270169258\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:31 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:32 INFO 140052404180800] Epoch[341] Batch[0] avg_epoch_loss=3.043166\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=341, batch=0 train loss <loss>=3.04316592216\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:33 INFO 140052404180800] Epoch[341] Batch[5] avg_epoch_loss=3.083156\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=341, batch=5 train loss <loss>=3.0831562678\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:33 INFO 140052404180800] Epoch[341] Batch [5]#011Speed: 330.54 samples/sec#011loss=3.083156\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:34 INFO 140052404180800] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2215.564966201782, \"sum\": 2215.564966201782, \"min\": 2215.564966201782}}, \"EndTime\": 1592850574.001419, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850571.78535}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:34 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.761352861 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:34 INFO 140052404180800] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=341, train loss <loss>=3.0051170826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:34 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:34 INFO 140052404180800] Epoch[342] Batch[0] avg_epoch_loss=2.825827\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=342, batch=0 train loss <loss>=2.82582736015\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:35 INFO 140052404180800] Epoch[342] Batch[5] avg_epoch_loss=3.052592\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=342, batch=5 train loss <loss>=3.05259247621\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:35 INFO 140052404180800] Epoch[342] Batch [5]#011Speed: 335.50 samples/sec#011loss=3.052592\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:36 INFO 140052404180800] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2178.7190437316895, \"sum\": 2178.7190437316895, \"min\": 2178.7190437316895}}, \"EndTime\": 1592850576.180736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850574.001507}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:36 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.093399082 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:36 INFO 140052404180800] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=342, train loss <loss>=3.01770846844\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:36 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:36 INFO 140052404180800] Epoch[343] Batch[0] avg_epoch_loss=3.163192\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=343, batch=0 train loss <loss>=3.16319227219\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:37 INFO 140052404180800] Epoch[343] Batch[5] avg_epoch_loss=3.096080\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=343, batch=5 train loss <loss>=3.09608002504\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:37 INFO 140052404180800] Epoch[343] Batch [5]#011Speed: 323.50 samples/sec#011loss=3.096080\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:38 INFO 140052404180800] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2194.6330070495605, \"sum\": 2194.6330070495605, \"min\": 2194.6330070495605}}, \"EndTime\": 1592850578.37595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850576.18082}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:38 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.101130079 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:38 INFO 140052404180800] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=343, train loss <loss>=3.09163193703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:38 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:38 INFO 140052404180800] Epoch[344] Batch[0] avg_epoch_loss=3.145182\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=344, batch=0 train loss <loss>=3.14518237114\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:39 INFO 140052404180800] Epoch[344] Batch[5] avg_epoch_loss=2.986650\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=344, batch=5 train loss <loss>=2.98665014903\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:39 INFO 140052404180800] Epoch[344] Batch [5]#011Speed: 337.07 samples/sec#011loss=2.986650\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:40 INFO 140052404180800] Epoch[344] Batch[10] avg_epoch_loss=3.039140\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=344, batch=10 train loss <loss>=3.1021276474\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:40 INFO 140052404180800] Epoch[344] Batch [10]#011Speed: 336.07 samples/sec#011loss=3.102128\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:40 INFO 140052404180800] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2355.6439876556396, \"sum\": 2355.6439876556396, \"min\": 2355.6439876556396}}, \"EndTime\": 1592850580.732144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850578.376033}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:40 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.014187596 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:40 INFO 140052404180800] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=344, train loss <loss>=3.03913992101\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:40 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:41 INFO 140052404180800] Epoch[345] Batch[0] avg_epoch_loss=3.179054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=345, batch=0 train loss <loss>=3.17905449867\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:42 INFO 140052404180800] Epoch[345] Batch[5] avg_epoch_loss=3.046316\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=345, batch=5 train loss <loss>=3.04631559054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:42 INFO 140052404180800] Epoch[345] Batch [5]#011Speed: 331.64 samples/sec#011loss=3.046316\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] Epoch[345] Batch[10] avg_epoch_loss=2.946915\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=345, batch=10 train loss <loss>=2.82763328552\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] Epoch[345] Batch [10]#011Speed: 333.96 samples/sec#011loss=2.827633\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2387.0980739593506, \"sum\": 2387.0980739593506, \"min\": 2387.0980739593506}}, \"EndTime\": 1592850583.119735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850580.732219}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.352773524 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=345, train loss <loss>=2.94691454281\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] Epoch[346] Batch[0] avg_epoch_loss=2.989047\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=346, batch=0 train loss <loss>=2.98904728889\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:44 INFO 140052404180800] Epoch[346] Batch[5] avg_epoch_loss=3.007267\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=346, batch=5 train loss <loss>=3.0072671175\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:44 INFO 140052404180800] Epoch[346] Batch [5]#011Speed: 335.35 samples/sec#011loss=3.007267\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:45 INFO 140052404180800] Epoch[346] Batch[10] avg_epoch_loss=3.053771\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=346, batch=10 train loss <loss>=3.10957489014\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:45 INFO 140052404180800] Epoch[346] Batch [10]#011Speed: 327.30 samples/sec#011loss=3.109575\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:45 INFO 140052404180800] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2388.6189460754395, \"sum\": 2388.6189460754395, \"min\": 2388.6189460754395}}, \"EndTime\": 1592850585.508867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850583.119803}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:45 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.739438439 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:45 INFO 140052404180800] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=346, train loss <loss>=3.05377065052\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:45 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:46 INFO 140052404180800] Epoch[347] Batch[0] avg_epoch_loss=3.023221\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=347, batch=0 train loss <loss>=3.02322101593\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:46 INFO 140052404180800] Epoch[347] Batch[5] avg_epoch_loss=3.007379\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=347, batch=5 train loss <loss>=3.00737921397\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:46 INFO 140052404180800] Epoch[347] Batch [5]#011Speed: 340.17 samples/sec#011loss=3.007379\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:47 INFO 140052404180800] Epoch[347] Batch[10] avg_epoch_loss=3.014037\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=347, batch=10 train loss <loss>=3.02202677727\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:47 INFO 140052404180800] Epoch[347] Batch [10]#011Speed: 326.87 samples/sec#011loss=3.022027\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:47 INFO 140052404180800] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2434.951066970825, \"sum\": 2434.951066970825, \"min\": 2434.951066970825}}, \"EndTime\": 1592850587.944336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850585.508945}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:47 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=278.843119804 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:47 INFO 140052404180800] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=347, train loss <loss>=3.01403719729\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:47 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:48 INFO 140052404180800] Epoch[348] Batch[0] avg_epoch_loss=2.912103\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=348, batch=0 train loss <loss>=2.91210341454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:49 INFO 140052404180800] Epoch[348] Batch[5] avg_epoch_loss=2.916939\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=348, batch=5 train loss <loss>=2.91693858306\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:49 INFO 140052404180800] Epoch[348] Batch [5]#011Speed: 332.76 samples/sec#011loss=2.916939\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:50 INFO 140052404180800] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2175.6770610809326, \"sum\": 2175.6770610809326, \"min\": 2175.6770610809326}}, \"EndTime\": 1592850590.120501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850587.944411}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:50 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=284.952930969 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:50 INFO 140052404180800] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=348, train loss <loss>=2.91753225327\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:50 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:50 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_2212e63c-5d4a-4c10-b134-bdce46c85b27-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 71.87795639038086, \"sum\": 71.87795639038086, \"min\": 71.87795639038086}}, \"EndTime\": 1592850590.192971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850590.120585}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:50 INFO 140052404180800] Epoch[349] Batch[0] avg_epoch_loss=3.137769\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=349, batch=0 train loss <loss>=3.13776946068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:51 INFO 140052404180800] Epoch[349] Batch[5] avg_epoch_loss=3.072328\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=349, batch=5 train loss <loss>=3.07232765357\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:51 INFO 140052404180800] Epoch[349] Batch [5]#011Speed: 340.22 samples/sec#011loss=3.072328\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:52 INFO 140052404180800] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2187.696933746338, \"sum\": 2187.696933746338, \"min\": 2187.696933746338}}, \"EndTime\": 1592850592.380806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850590.193046}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:52 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.27407651 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:52 INFO 140052404180800] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=349, train loss <loss>=3.01192941666\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:52 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:52 INFO 140052404180800] Epoch[350] Batch[0] avg_epoch_loss=3.125402\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=350, batch=0 train loss <loss>=3.12540173531\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:53 INFO 140052404180800] Epoch[350] Batch[5] avg_epoch_loss=3.011110\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=350, batch=5 train loss <loss>=3.01110998789\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:53 INFO 140052404180800] Epoch[350] Batch [5]#011Speed: 339.49 samples/sec#011loss=3.011110\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:54 INFO 140052404180800] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2193.1800842285156, \"sum\": 2193.1800842285156, \"min\": 2193.1800842285156}}, \"EndTime\": 1592850594.574518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850592.380888}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:54 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=291.797873396 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:54 INFO 140052404180800] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=350, train loss <loss>=3.03519434929\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:54 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:55 INFO 140052404180800] Epoch[351] Batch[0] avg_epoch_loss=3.052176\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=351, batch=0 train loss <loss>=3.05217623711\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:56 INFO 140052404180800] Epoch[351] Batch[5] avg_epoch_loss=3.011659\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=351, batch=5 train loss <loss>=3.01165874799\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:56 INFO 140052404180800] Epoch[351] Batch [5]#011Speed: 327.11 samples/sec#011loss=3.011659\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:56 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2191.8070316314697, \"sum\": 2191.8070316314697, \"min\": 2191.8070316314697}}, \"EndTime\": 1592850596.766851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850594.574599}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:56 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.593706927 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:56 INFO 140052404180800] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=351, train loss <loss>=3.00918858051\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:56 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:57 INFO 140052404180800] Epoch[352] Batch[0] avg_epoch_loss=3.047108\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=352, batch=0 train loss <loss>=3.04710769653\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:58 INFO 140052404180800] Epoch[352] Batch[5] avg_epoch_loss=3.065476\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=352, batch=5 train loss <loss>=3.06547617912\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:58 INFO 140052404180800] Epoch[352] Batch [5]#011Speed: 334.12 samples/sec#011loss=3.065476\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:58 INFO 140052404180800] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2222.364902496338, \"sum\": 2222.364902496338, \"min\": 2222.364902496338}}, \"EndTime\": 1592850598.989753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850596.766932}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:58 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.367577391 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:58 INFO 140052404180800] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=352, train loss <loss>=3.1274538517\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:58 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:59 INFO 140052404180800] Epoch[353] Batch[0] avg_epoch_loss=2.909692\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:29:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=353, batch=0 train loss <loss>=2.90969157219\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:00 INFO 140052404180800] Epoch[353] Batch[5] avg_epoch_loss=2.964030\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=353, batch=5 train loss <loss>=2.9640297095\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:00 INFO 140052404180800] Epoch[353] Batch [5]#011Speed: 331.24 samples/sec#011loss=2.964030\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:01 INFO 140052404180800] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2267.2269344329834, \"sum\": 2267.2269344329834, \"min\": 2267.2269344329834}}, \"EndTime\": 1592850601.25753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850598.989835}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:01 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.827400082 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:01 INFO 140052404180800] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=353, train loss <loss>=3.00206522942\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:01 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:01 INFO 140052404180800] Epoch[354] Batch[0] avg_epoch_loss=3.003723\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=354, batch=0 train loss <loss>=3.00372290611\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:02 INFO 140052404180800] Epoch[354] Batch[5] avg_epoch_loss=3.050964\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=354, batch=5 train loss <loss>=3.05096379916\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:02 INFO 140052404180800] Epoch[354] Batch [5]#011Speed: 331.12 samples/sec#011loss=3.050964\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:03 INFO 140052404180800] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2217.5819873809814, \"sum\": 2217.5819873809814, \"min\": 2217.5819873809814}}, \"EndTime\": 1592850603.475661, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850601.257609}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:03 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=269.197440775 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:03 INFO 140052404180800] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=354, train loss <loss>=3.11477687359\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:03 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:03 INFO 140052404180800] Epoch[355] Batch[0] avg_epoch_loss=3.008340\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=355, batch=0 train loss <loss>=3.0083398819\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:04 INFO 140052404180800] Epoch[355] Batch[5] avg_epoch_loss=3.005403\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=355, batch=5 train loss <loss>=3.00540296237\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:04 INFO 140052404180800] Epoch[355] Batch [5]#011Speed: 333.85 samples/sec#011loss=3.005403\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:05 INFO 140052404180800] Epoch[355] Batch[10] avg_epoch_loss=3.076644\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=355, batch=10 train loss <loss>=3.16213383675\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:05 INFO 140052404180800] Epoch[355] Batch [10]#011Speed: 332.09 samples/sec#011loss=3.162134\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:05 INFO 140052404180800] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2375.0739097595215, \"sum\": 2375.0739097595215, \"min\": 2375.0739097595215}}, \"EndTime\": 1592850605.851268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850603.475742}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:05 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.926172427 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:05 INFO 140052404180800] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=355, train loss <loss>=3.0766442689\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:05 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:06 INFO 140052404180800] Epoch[356] Batch[0] avg_epoch_loss=3.017073\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=356, batch=0 train loss <loss>=3.01707339287\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:07 INFO 140052404180800] Epoch[356] Batch[5] avg_epoch_loss=3.022120\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=356, batch=5 train loss <loss>=3.0221200784\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:07 INFO 140052404180800] Epoch[356] Batch [5]#011Speed: 334.91 samples/sec#011loss=3.022120\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:08 INFO 140052404180800] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2188.908100128174, \"sum\": 2188.908100128174, \"min\": 2188.908100128174}}, \"EndTime\": 1592850608.040708, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850605.85134}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:08 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.259462135 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:08 INFO 140052404180800] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=356, train loss <loss>=3.04654421806\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:08 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:08 INFO 140052404180800] Epoch[357] Batch[0] avg_epoch_loss=2.973068\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=357, batch=0 train loss <loss>=2.9730682373\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:09 INFO 140052404180800] Epoch[357] Batch[5] avg_epoch_loss=2.948694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=357, batch=5 train loss <loss>=2.94869395097\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:09 INFO 140052404180800] Epoch[357] Batch [5]#011Speed: 336.55 samples/sec#011loss=2.948694\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:10 INFO 140052404180800] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2167.910099029541, \"sum\": 2167.910099029541, \"min\": 2167.910099029541}}, \"EndTime\": 1592850610.209129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850608.040769}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:10 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=286.438455157 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:10 INFO 140052404180800] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=357, train loss <loss>=2.99105086327\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:10 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:10 INFO 140052404180800] Epoch[358] Batch[0] avg_epoch_loss=3.081744\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=358, batch=0 train loss <loss>=3.08174443245\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:11 INFO 140052404180800] Epoch[358] Batch[5] avg_epoch_loss=2.980431\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=358, batch=5 train loss <loss>=2.98043072224\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:11 INFO 140052404180800] Epoch[358] Batch [5]#011Speed: 335.62 samples/sec#011loss=2.980431\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:12 INFO 140052404180800] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2158.050060272217, \"sum\": 2158.050060272217, \"min\": 2158.050060272217}}, \"EndTime\": 1592850612.367756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850610.209192}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:12 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=295.536040504 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:12 INFO 140052404180800] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=358, train loss <loss>=2.97630393505\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:12 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:12 INFO 140052404180800] Epoch[359] Batch[0] avg_epoch_loss=2.968265\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=359, batch=0 train loss <loss>=2.96826481819\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:13 INFO 140052404180800] Epoch[359] Batch[5] avg_epoch_loss=2.970645\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=359, batch=5 train loss <loss>=2.97064526876\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:13 INFO 140052404180800] Epoch[359] Batch [5]#011Speed: 339.06 samples/sec#011loss=2.970645\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:14 INFO 140052404180800] Epoch[359] Batch[10] avg_epoch_loss=2.923805\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=359, batch=10 train loss <loss>=2.86759614944\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:14 INFO 140052404180800] Epoch[359] Batch [10]#011Speed: 329.65 samples/sec#011loss=2.867596\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:14 INFO 140052404180800] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2416.0311222076416, \"sum\": 2416.0311222076416, \"min\": 2416.0311222076416}}, \"EndTime\": 1592850614.786126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850612.368455}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:14 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.887345796 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:14 INFO 140052404180800] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=359, train loss <loss>=2.92380475998\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:14 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:15 INFO 140052404180800] Epoch[360] Batch[0] avg_epoch_loss=2.976239\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=360, batch=0 train loss <loss>=2.97623896599\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:16 INFO 140052404180800] Epoch[360] Batch[5] avg_epoch_loss=3.015699\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=360, batch=5 train loss <loss>=3.01569859187\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:16 INFO 140052404180800] Epoch[360] Batch [5]#011Speed: 337.87 samples/sec#011loss=3.015699\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:16 INFO 140052404180800] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2176.528215408325, \"sum\": 2176.528215408325, \"min\": 2176.528215408325}}, \"EndTime\": 1592850616.963173, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850614.786204}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:16 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=293.111384291 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:16 INFO 140052404180800] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=360, train loss <loss>=3.05176446438\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:16 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:17 INFO 140052404180800] Epoch[361] Batch[0] avg_epoch_loss=3.304502\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=361, batch=0 train loss <loss>=3.30450248718\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:18 INFO 140052404180800] Epoch[361] Batch[5] avg_epoch_loss=3.102168\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=361, batch=5 train loss <loss>=3.10216808319\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:18 INFO 140052404180800] Epoch[361] Batch [5]#011Speed: 330.68 samples/sec#011loss=3.102168\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:19 INFO 140052404180800] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2187.6609325408936, \"sum\": 2187.6609325408936, \"min\": 2187.6609325408936}}, \"EndTime\": 1592850619.151432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850616.963255}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:19 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=281.109981972 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:19 INFO 140052404180800] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=361, train loss <loss>=3.12297594547\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:19 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:19 INFO 140052404180800] Epoch[362] Batch[0] avg_epoch_loss=2.953093\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=362, batch=0 train loss <loss>=2.95309329033\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:20 INFO 140052404180800] Epoch[362] Batch[5] avg_epoch_loss=3.028097\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=362, batch=5 train loss <loss>=3.02809735139\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:20 INFO 140052404180800] Epoch[362] Batch [5]#011Speed: 338.27 samples/sec#011loss=3.028097\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] Epoch[362] Batch[10] avg_epoch_loss=3.045635\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=362, batch=10 train loss <loss>=3.06667919159\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] Epoch[362] Batch [10]#011Speed: 328.11 samples/sec#011loss=3.066679\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2381.5360069274902, \"sum\": 2381.5360069274902, \"min\": 2381.5360069274902}}, \"EndTime\": 1592850621.533547, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850619.151492}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.099438397 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=362, train loss <loss>=3.04563455148\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] Epoch[363] Batch[0] avg_epoch_loss=3.002544\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=363, batch=0 train loss <loss>=3.00254440308\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:22 INFO 140052404180800] Epoch[363] Batch[5] avg_epoch_loss=3.035522\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=363, batch=5 train loss <loss>=3.0355224212\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:22 INFO 140052404180800] Epoch[363] Batch [5]#011Speed: 319.26 samples/sec#011loss=3.035522\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:23 INFO 140052404180800] Epoch[363] Batch[10] avg_epoch_loss=2.948316\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=363, batch=10 train loss <loss>=2.84366884232\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:23 INFO 140052404180800] Epoch[363] Batch [10]#011Speed: 334.32 samples/sec#011loss=2.843669\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:23 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2418.437957763672, \"sum\": 2418.437957763672, \"min\": 2418.437957763672}}, \"EndTime\": 1592850623.952484, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850621.533605}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:23 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=265.034598191 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:23 INFO 140052404180800] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=363, train loss <loss>=2.94831624898\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:23 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:24 INFO 140052404180800] Epoch[364] Batch[0] avg_epoch_loss=3.054664\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=364, batch=0 train loss <loss>=3.05466389656\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:25 INFO 140052404180800] Epoch[364] Batch[5] avg_epoch_loss=3.001058\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=364, batch=5 train loss <loss>=3.00105754534\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:25 INFO 140052404180800] Epoch[364] Batch [5]#011Speed: 334.59 samples/sec#011loss=3.001058\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:26 INFO 140052404180800] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2205.9600353240967, \"sum\": 2205.9600353240967, \"min\": 2205.9600353240967}}, \"EndTime\": 1592850626.158953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850623.952562}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:26 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.588440329 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:26 INFO 140052404180800] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=364, train loss <loss>=3.00694992542\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:26 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:26 INFO 140052404180800] Epoch[365] Batch[0] avg_epoch_loss=3.041918\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=365, batch=0 train loss <loss>=3.0419178009\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:27 INFO 140052404180800] Epoch[365] Batch[5] avg_epoch_loss=3.051108\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=365, batch=5 train loss <loss>=3.05110788345\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:27 INFO 140052404180800] Epoch[365] Batch [5]#011Speed: 338.60 samples/sec#011loss=3.051108\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:28 INFO 140052404180800] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2169.1830158233643, \"sum\": 2169.1830158233643, \"min\": 2169.1830158233643}}, \"EndTime\": 1592850628.328661, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850626.159034}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:28 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=290.415710076 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:28 INFO 140052404180800] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=365, train loss <loss>=3.00447905064\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:28 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:28 INFO 140052404180800] Epoch[366] Batch[0] avg_epoch_loss=3.107553\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=366, batch=0 train loss <loss>=3.10755252838\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:29 INFO 140052404180800] Epoch[366] Batch[5] avg_epoch_loss=2.967926\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=366, batch=5 train loss <loss>=2.96792566776\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:29 INFO 140052404180800] Epoch[366] Batch [5]#011Speed: 339.51 samples/sec#011loss=2.967926\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:30 INFO 140052404180800] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2153.2068252563477, \"sum\": 2153.2068252563477, \"min\": 2153.2068252563477}}, \"EndTime\": 1592850630.482432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850628.328745}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:30 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.140159093 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:30 INFO 140052404180800] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=366, train loss <loss>=2.92432947159\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:30 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:30 INFO 140052404180800] Epoch[367] Batch[0] avg_epoch_loss=3.085747\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=367, batch=0 train loss <loss>=3.08574652672\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:31 INFO 140052404180800] Epoch[367] Batch[5] avg_epoch_loss=3.032577\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=367, batch=5 train loss <loss>=3.03257719676\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:31 INFO 140052404180800] Epoch[367] Batch [5]#011Speed: 338.33 samples/sec#011loss=3.032577\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:32 INFO 140052404180800] Epoch[367] Batch[10] avg_epoch_loss=2.970376\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=367, batch=10 train loss <loss>=2.89573411942\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:32 INFO 140052404180800] Epoch[367] Batch [10]#011Speed: 339.12 samples/sec#011loss=2.895734\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:32 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2369.385004043579, \"sum\": 2369.385004043579, \"min\": 2369.385004043579}}, \"EndTime\": 1592850632.8524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850630.482515}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:32 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.521433692 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:32 INFO 140052404180800] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=367, train loss <loss>=2.97037579797\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:32 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:33 INFO 140052404180800] Epoch[368] Batch[0] avg_epoch_loss=2.952764\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=368, batch=0 train loss <loss>=2.95276427269\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:34 INFO 140052404180800] Epoch[368] Batch[5] avg_epoch_loss=3.031492\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=368, batch=5 train loss <loss>=3.03149211407\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:34 INFO 140052404180800] Epoch[368] Batch [5]#011Speed: 339.92 samples/sec#011loss=3.031492\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] Epoch[368] Batch[10] avg_epoch_loss=3.068564\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=368, batch=10 train loss <loss>=3.11305003166\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] Epoch[368] Batch [10]#011Speed: 336.87 samples/sec#011loss=3.113050\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2376.0249614715576, \"sum\": 2376.0249614715576, \"min\": 2376.0249614715576}}, \"EndTime\": 1592850635.228934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850632.852477}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.919946479 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=368, train loss <loss>=3.06856389479\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] Epoch[369] Batch[0] avg_epoch_loss=3.044365\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=369, batch=0 train loss <loss>=3.04436540604\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:36 INFO 140052404180800] Epoch[369] Batch[5] avg_epoch_loss=2.990533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=369, batch=5 train loss <loss>=2.99053327243\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:36 INFO 140052404180800] Epoch[369] Batch [5]#011Speed: 340.20 samples/sec#011loss=2.990533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:37 INFO 140052404180800] Epoch[369] Batch[10] avg_epoch_loss=3.040445\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=369, batch=10 train loss <loss>=3.10033903122\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:37 INFO 140052404180800] Epoch[369] Batch [10]#011Speed: 334.31 samples/sec#011loss=3.100339\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:37 INFO 140052404180800] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2380.3210258483887, \"sum\": 2380.3210258483887, \"min\": 2380.3210258483887}}, \"EndTime\": 1592850637.609774, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850635.229011}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:37 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.621092551 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:37 INFO 140052404180800] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=369, train loss <loss>=3.04044498097\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:37 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:38 INFO 140052404180800] Epoch[370] Batch[0] avg_epoch_loss=3.104098\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=370, batch=0 train loss <loss>=3.10409784317\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:39 INFO 140052404180800] Epoch[370] Batch[5] avg_epoch_loss=3.034853\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=370, batch=5 train loss <loss>=3.03485337893\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:39 INFO 140052404180800] Epoch[370] Batch [5]#011Speed: 338.69 samples/sec#011loss=3.034853\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] Epoch[370] Batch[10] avg_epoch_loss=3.103811\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=370, batch=10 train loss <loss>=3.18655939102\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] Epoch[370] Batch [10]#011Speed: 336.42 samples/sec#011loss=3.186559\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2395.5330848693848, \"sum\": 2395.5330848693848, \"min\": 2395.5330848693848}}, \"EndTime\": 1592850640.005804, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850637.609852}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=276.752173553 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=370, train loss <loss>=3.10381065715\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] Epoch[371] Batch[0] avg_epoch_loss=3.154796\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=371, batch=0 train loss <loss>=3.15479564667\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:41 INFO 140052404180800] Epoch[371] Batch[5] avg_epoch_loss=3.004699\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=371, batch=5 train loss <loss>=3.00469863415\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:41 INFO 140052404180800] Epoch[371] Batch [5]#011Speed: 338.74 samples/sec#011loss=3.004699\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] Epoch[371] Batch[10] avg_epoch_loss=2.899298\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=371, batch=10 train loss <loss>=2.77281785011\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] Epoch[371] Batch [10]#011Speed: 336.81 samples/sec#011loss=2.772818\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2366.4278984069824, \"sum\": 2366.4278984069824, \"min\": 2366.4278984069824}}, \"EndTime\": 1592850642.372725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850640.005881}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.394809103 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=371, train loss <loss>=2.89929827777\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_ca8c7241-3f77-4792-ac1f-e531becf9278-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 68.45498085021973, \"sum\": 68.45498085021973, \"min\": 68.45498085021973}}, \"EndTime\": 1592850642.441731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850642.372802}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] Epoch[372] Batch[0] avg_epoch_loss=3.295591\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=372, batch=0 train loss <loss>=3.29559063911\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:43 INFO 140052404180800] Epoch[372] Batch[5] avg_epoch_loss=3.074493\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=372, batch=5 train loss <loss>=3.07449328899\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:43 INFO 140052404180800] Epoch[372] Batch [5]#011Speed: 339.64 samples/sec#011loss=3.074493\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:44 INFO 140052404180800] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2163.8059616088867, \"sum\": 2163.8059616088867, \"min\": 2163.8059616088867}}, \"EndTime\": 1592850644.605661, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850642.441794}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:44 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.962812533 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:44 INFO 140052404180800] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=372, train loss <loss>=2.98053543568\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:44 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:45 INFO 140052404180800] Epoch[373] Batch[0] avg_epoch_loss=2.988045\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=373, batch=0 train loss <loss>=2.98804521561\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] Epoch[373] Batch[5] avg_epoch_loss=3.042372\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=373, batch=5 train loss <loss>=3.04237159093\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] Epoch[373] Batch [5]#011Speed: 339.05 samples/sec#011loss=3.042372\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] Epoch[373] Batch[10] avg_epoch_loss=3.029363\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=373, batch=10 train loss <loss>=3.01375293732\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] Epoch[373] Batch [10]#011Speed: 334.66 samples/sec#011loss=3.013753\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2382.3981285095215, \"sum\": 2382.3981285095215, \"min\": 2382.3981285095215}}, \"EndTime\": 1592850646.988631, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850644.605746}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=288.351828663 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=373, train loss <loss>=3.02936311202\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:46 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:47 INFO 140052404180800] Epoch[374] Batch[0] avg_epoch_loss=2.971677\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:47 INFO 140052404180800] #quality_metric: host=algo-1, epoch=374, batch=0 train loss <loss>=2.97167682648\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:48 INFO 140052404180800] Epoch[374] Batch[5] avg_epoch_loss=3.004980\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:48 INFO 140052404180800] #quality_metric: host=algo-1, epoch=374, batch=5 train loss <loss>=3.00498016675\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:48 INFO 140052404180800] Epoch[374] Batch [5]#011Speed: 329.99 samples/sec#011loss=3.004980\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] Epoch[374] Batch[10] avg_epoch_loss=3.044498\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=374, batch=10 train loss <loss>=3.09191846848\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] Epoch[374] Batch [10]#011Speed: 327.97 samples/sec#011loss=3.091918\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2416.6080951690674, \"sum\": 2416.6080951690674, \"min\": 2416.6080951690674}}, \"EndTime\": 1592850649.405765, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850646.988702}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=275.994354627 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=374, train loss <loss>=3.04449757663\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] Epoch[375] Batch[0] avg_epoch_loss=3.084069\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:49 INFO 140052404180800] #quality_metric: host=algo-1, epoch=375, batch=0 train loss <loss>=3.08406925201\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:50 INFO 140052404180800] Epoch[375] Batch[5] avg_epoch_loss=2.999008\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:50 INFO 140052404180800] #quality_metric: host=algo-1, epoch=375, batch=5 train loss <loss>=2.99900813897\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:50 INFO 140052404180800] Epoch[375] Batch [5]#011Speed: 333.21 samples/sec#011loss=2.999008\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:51 INFO 140052404180800] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2207.8778743743896, \"sum\": 2207.8778743743896, \"min\": 2207.8778743743896}}, \"EndTime\": 1592850651.614162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850649.405836}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:51 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.329698844 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:51 INFO 140052404180800] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:51 INFO 140052404180800] #quality_metric: host=algo-1, epoch=375, train loss <loss>=3.01784374714\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:51 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:52 INFO 140052404180800] Epoch[376] Batch[0] avg_epoch_loss=3.038463\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:52 INFO 140052404180800] #quality_metric: host=algo-1, epoch=376, batch=0 train loss <loss>=3.03846311569\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:53 INFO 140052404180800] Epoch[376] Batch[5] avg_epoch_loss=3.033810\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=376, batch=5 train loss <loss>=3.03381049633\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:53 INFO 140052404180800] Epoch[376] Batch [5]#011Speed: 325.75 samples/sec#011loss=3.033810\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:53 INFO 140052404180800] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2206.2430381774902, \"sum\": 2206.2430381774902, \"min\": 2206.2430381774902}}, \"EndTime\": 1592850653.82092, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850651.614224}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:53 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=283.273390527 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:53 INFO 140052404180800] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:53 INFO 140052404180800] #quality_metric: host=algo-1, epoch=376, train loss <loss>=3.00113728046\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:53 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:54 INFO 140052404180800] Epoch[377] Batch[0] avg_epoch_loss=3.134764\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:54 INFO 140052404180800] #quality_metric: host=algo-1, epoch=377, batch=0 train loss <loss>=3.13476395607\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:55 INFO 140052404180800] Epoch[377] Batch[5] avg_epoch_loss=3.093332\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:55 INFO 140052404180800] #quality_metric: host=algo-1, epoch=377, batch=5 train loss <loss>=3.09333177408\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:55 INFO 140052404180800] Epoch[377] Batch [5]#011Speed: 333.14 samples/sec#011loss=3.093332\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] Epoch[377] Batch[10] avg_epoch_loss=2.981900\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=377, batch=10 train loss <loss>=2.84818248749\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] Epoch[377] Batch [10]#011Speed: 327.64 samples/sec#011loss=2.848182\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2381.3939094543457, \"sum\": 2381.3939094543457, \"min\": 2381.3939094543457}}, \"EndTime\": 1592850656.202906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850653.820995}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.932524049 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=377, train loss <loss>=2.98190028017\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] Epoch[378] Batch[0] avg_epoch_loss=3.003458\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:56 INFO 140052404180800] #quality_metric: host=algo-1, epoch=378, batch=0 train loss <loss>=3.00345778465\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:57 INFO 140052404180800] Epoch[378] Batch[5] avg_epoch_loss=2.977490\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:57 INFO 140052404180800] #quality_metric: host=algo-1, epoch=378, batch=5 train loss <loss>=2.97749010722\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:57 INFO 140052404180800] Epoch[378] Batch [5]#011Speed: 338.74 samples/sec#011loss=2.977490\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:58 INFO 140052404180800] Epoch[378] Batch[10] avg_epoch_loss=2.908033\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=378, batch=10 train loss <loss>=2.82468481064\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:58 INFO 140052404180800] Epoch[378] Batch [10]#011Speed: 337.75 samples/sec#011loss=2.824685\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:58 INFO 140052404180800] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2355.437994003296, \"sum\": 2355.437994003296, \"min\": 2355.437994003296}}, \"EndTime\": 1592850658.55893, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850656.20299}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:58 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=280.189475832 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:58 INFO 140052404180800] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:58 INFO 140052404180800] #quality_metric: host=algo-1, epoch=378, train loss <loss>=2.90803315423\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:58 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:59 INFO 140052404180800] Epoch[379] Batch[0] avg_epoch_loss=2.909452\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=379, batch=0 train loss <loss>=2.90945219994\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:59 INFO 140052404180800] Epoch[379] Batch[5] avg_epoch_loss=2.989811\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:59 INFO 140052404180800] #quality_metric: host=algo-1, epoch=379, batch=5 train loss <loss>=2.9898109436\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:30:59 INFO 140052404180800] Epoch[379] Batch [5]#011Speed: 336.31 samples/sec#011loss=2.989811\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:00 INFO 140052404180800] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2187.748908996582, \"sum\": 2187.748908996582, \"min\": 2187.748908996582}}, \"EndTime\": 1592850660.747204, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850658.559007}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:00 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.322488349 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:00 INFO 140052404180800] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:00 INFO 140052404180800] #quality_metric: host=algo-1, epoch=379, train loss <loss>=2.97583267689\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:00 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:01 INFO 140052404180800] Epoch[380] Batch[0] avg_epoch_loss=2.892141\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:01 INFO 140052404180800] #quality_metric: host=algo-1, epoch=380, batch=0 train loss <loss>=2.89214086533\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:02 INFO 140052404180800] Epoch[380] Batch[5] avg_epoch_loss=2.942885\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:02 INFO 140052404180800] #quality_metric: host=algo-1, epoch=380, batch=5 train loss <loss>=2.94288480282\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:02 INFO 140052404180800] Epoch[380] Batch [5]#011Speed: 323.73 samples/sec#011loss=2.942885\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:03 INFO 140052404180800] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2253.772974014282, \"sum\": 2253.772974014282, \"min\": 2253.772974014282}}, \"EndTime\": 1592850663.001519, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850660.747288}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:03 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=273.750644659 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:03 INFO 140052404180800] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=380, train loss <loss>=2.95408539772\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:03 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:03 INFO 140052404180800] Epoch[381] Batch[0] avg_epoch_loss=2.957531\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:03 INFO 140052404180800] #quality_metric: host=algo-1, epoch=381, batch=0 train loss <loss>=2.95753145218\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:04 INFO 140052404180800] Epoch[381] Batch[5] avg_epoch_loss=2.985067\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:04 INFO 140052404180800] #quality_metric: host=algo-1, epoch=381, batch=5 train loss <loss>=2.98506744703\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:04 INFO 140052404180800] Epoch[381] Batch [5]#011Speed: 337.50 samples/sec#011loss=2.985067\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:05 INFO 140052404180800] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2147.8629112243652, \"sum\": 2147.8629112243652, \"min\": 2147.8629112243652}}, \"EndTime\": 1592850665.149953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850663.001592}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:05 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=294.231564048 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:05 INFO 140052404180800] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=381, train loss <loss>=2.96799917221\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:05 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:05 INFO 140052404180800] Epoch[382] Batch[0] avg_epoch_loss=3.099116\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:05 INFO 140052404180800] #quality_metric: host=algo-1, epoch=382, batch=0 train loss <loss>=3.09911632538\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:06 INFO 140052404180800] Epoch[382] Batch[5] avg_epoch_loss=3.010180\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:06 INFO 140052404180800] #quality_metric: host=algo-1, epoch=382, batch=5 train loss <loss>=3.01017995675\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:06 INFO 140052404180800] Epoch[382] Batch [5]#011Speed: 334.16 samples/sec#011loss=3.010180\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:07 INFO 140052404180800] Epoch[382] Batch[10] avg_epoch_loss=3.054125\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=382, batch=10 train loss <loss>=3.10685801506\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:07 INFO 140052404180800] Epoch[382] Batch [10]#011Speed: 326.15 samples/sec#011loss=3.106858\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:07 INFO 140052404180800] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2398.756980895996, \"sum\": 2398.756980895996, \"min\": 2398.756980895996}}, \"EndTime\": 1592850667.54925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850665.150022}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:07 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=268.459840824 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:07 INFO 140052404180800] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:07 INFO 140052404180800] #quality_metric: host=algo-1, epoch=382, train loss <loss>=3.05412452871\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:07 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:08 INFO 140052404180800] Epoch[383] Batch[0] avg_epoch_loss=2.997028\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=383, batch=0 train loss <loss>=2.99702835083\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:08 INFO 140052404180800] Epoch[383] Batch[5] avg_epoch_loss=2.966299\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:08 INFO 140052404180800] #quality_metric: host=algo-1, epoch=383, batch=5 train loss <loss>=2.96629909674\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:08 INFO 140052404180800] Epoch[383] Batch [5]#011Speed: 335.01 samples/sec#011loss=2.966299\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:09 INFO 140052404180800] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2188.8229846954346, \"sum\": 2188.8229846954346, \"min\": 2188.8229846954346}}, \"EndTime\": 1592850669.738612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850667.549327}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:09 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=290.094616278 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:09 INFO 140052404180800] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:09 INFO 140052404180800] #quality_metric: host=algo-1, epoch=383, train loss <loss>=2.94543828964\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:09 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:10 INFO 140052404180800] Epoch[384] Batch[0] avg_epoch_loss=2.971930\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:10 INFO 140052404180800] #quality_metric: host=algo-1, epoch=384, batch=0 train loss <loss>=2.97192955017\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:11 INFO 140052404180800] Epoch[384] Batch[5] avg_epoch_loss=2.980742\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=384, batch=5 train loss <loss>=2.98074225585\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:11 INFO 140052404180800] Epoch[384] Batch [5]#011Speed: 334.02 samples/sec#011loss=2.980742\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:11 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2173.3150482177734, \"sum\": 2173.3150482177734, \"min\": 2173.3150482177734}}, \"EndTime\": 1592850671.912506, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850669.738693}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:11 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=293.084742297 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:11 INFO 140052404180800] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:11 INFO 140052404180800] #quality_metric: host=algo-1, epoch=384, train loss <loss>=2.96585085392\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:11 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:12 INFO 140052404180800] Epoch[385] Batch[0] avg_epoch_loss=3.013434\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:12 INFO 140052404180800] #quality_metric: host=algo-1, epoch=385, batch=0 train loss <loss>=3.01343369484\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:13 INFO 140052404180800] Epoch[385] Batch[5] avg_epoch_loss=2.959054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:13 INFO 140052404180800] #quality_metric: host=algo-1, epoch=385, batch=5 train loss <loss>=2.95905427138\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:13 INFO 140052404180800] Epoch[385] Batch [5]#011Speed: 305.38 samples/sec#011loss=2.959054\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:14 INFO 140052404180800] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2310.080051422119, \"sum\": 2310.080051422119, \"min\": 2310.080051422119}}, \"EndTime\": 1592850674.223161, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850671.912588}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:14 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=270.973499415 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:14 INFO 140052404180800] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=385, train loss <loss>=2.96178119183\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:14 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:14 INFO 140052404180800] Epoch[386] Batch[0] avg_epoch_loss=2.975690\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:14 INFO 140052404180800] #quality_metric: host=algo-1, epoch=386, batch=0 train loss <loss>=2.97569012642\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:15 INFO 140052404180800] Epoch[386] Batch[5] avg_epoch_loss=3.015081\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:15 INFO 140052404180800] #quality_metric: host=algo-1, epoch=386, batch=5 train loss <loss>=3.01508128643\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:15 INFO 140052404180800] Epoch[386] Batch [5]#011Speed: 333.69 samples/sec#011loss=3.015081\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:16 INFO 140052404180800] Epoch[386] Batch[10] avg_epoch_loss=3.103413\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=386, batch=10 train loss <loss>=3.20941119194\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:16 INFO 140052404180800] Epoch[386] Batch [10]#011Speed: 326.71 samples/sec#011loss=3.209411\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:16 INFO 140052404180800] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2376.161813735962, \"sum\": 2376.161813735962, \"min\": 2376.161813735962}}, \"EndTime\": 1592850676.599853, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850674.223231}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:16 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=271.434149384 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:16 INFO 140052404180800] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:16 INFO 140052404180800] #quality_metric: host=algo-1, epoch=386, train loss <loss>=3.10341306166\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:16 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:17 INFO 140052404180800] Epoch[387] Batch[0] avg_epoch_loss=2.996912\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=387, batch=0 train loss <loss>=2.99691152573\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:17 INFO 140052404180800] Epoch[387] Batch[5] avg_epoch_loss=3.017878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:17 INFO 140052404180800] #quality_metric: host=algo-1, epoch=387, batch=5 train loss <loss>=3.01787813505\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:17 INFO 140052404180800] Epoch[387] Batch [5]#011Speed: 337.16 samples/sec#011loss=3.017878\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:18 INFO 140052404180800] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2157.0239067077637, \"sum\": 2157.0239067077637, \"min\": 2157.0239067077637}}, \"EndTime\": 1592850678.757434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850676.599922}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:18 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=295.293898134 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:18 INFO 140052404180800] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:18 INFO 140052404180800] #quality_metric: host=algo-1, epoch=387, train loss <loss>=2.99364266396\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:18 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:19 INFO 140052404180800] Epoch[388] Batch[0] avg_epoch_loss=3.190844\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:19 INFO 140052404180800] #quality_metric: host=algo-1, epoch=388, batch=0 train loss <loss>=3.19084382057\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:20 INFO 140052404180800] Epoch[388] Batch[5] avg_epoch_loss=3.044107\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:20 INFO 140052404180800] #quality_metric: host=algo-1, epoch=388, batch=5 train loss <loss>=3.04410668214\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:20 INFO 140052404180800] Epoch[388] Batch [5]#011Speed: 341.23 samples/sec#011loss=3.044107\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] Epoch[388] Batch[10] avg_epoch_loss=3.098324\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=388, batch=10 train loss <loss>=3.16338415146\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] Epoch[388] Batch [10]#011Speed: 336.16 samples/sec#011loss=3.163384\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2339.0071392059326, \"sum\": 2339.0071392059326, \"min\": 2339.0071392059326}}, \"EndTime\": 1592850681.096998, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850678.757544}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.03502529 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=388, train loss <loss>=3.09832371365\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] Epoch[389] Batch[0] avg_epoch_loss=2.975556\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:21 INFO 140052404180800] #quality_metric: host=algo-1, epoch=389, batch=0 train loss <loss>=2.9755563736\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:22 INFO 140052404180800] Epoch[389] Batch[5] avg_epoch_loss=2.988978\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:22 INFO 140052404180800] #quality_metric: host=algo-1, epoch=389, batch=5 train loss <loss>=2.98897767067\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:22 INFO 140052404180800] Epoch[389] Batch [5]#011Speed: 337.14 samples/sec#011loss=2.988978\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:23 INFO 140052404180800] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2175.575017929077, \"sum\": 2175.575017929077, \"min\": 2175.575017929077}}, \"EndTime\": 1592850683.27312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850681.097073}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:23 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.424073339 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:23 INFO 140052404180800] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=389, train loss <loss>=2.9922090292\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:23 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:23 INFO 140052404180800] Epoch[390] Batch[0] avg_epoch_loss=2.960663\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:23 INFO 140052404180800] #quality_metric: host=algo-1, epoch=390, batch=0 train loss <loss>=2.96066331863\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:24 INFO 140052404180800] Epoch[390] Batch[5] avg_epoch_loss=3.012462\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:24 INFO 140052404180800] #quality_metric: host=algo-1, epoch=390, batch=5 train loss <loss>=3.01246162256\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:24 INFO 140052404180800] Epoch[390] Batch [5]#011Speed: 335.16 samples/sec#011loss=3.012462\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:25 INFO 140052404180800] Epoch[390] Batch[10] avg_epoch_loss=3.060009\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=390, batch=10 train loss <loss>=3.11706528664\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:25 INFO 140052404180800] Epoch[390] Batch [10]#011Speed: 332.02 samples/sec#011loss=3.117065\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:25 INFO 140052404180800] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2384.7761154174805, \"sum\": 2384.7761154174805, \"min\": 2384.7761154174805}}, \"EndTime\": 1592850685.658493, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850683.273219}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:25 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.550529561 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:25 INFO 140052404180800] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:25 INFO 140052404180800] #quality_metric: host=algo-1, epoch=390, train loss <loss>=3.06000874259\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:25 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:26 INFO 140052404180800] Epoch[391] Batch[0] avg_epoch_loss=2.946014\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:26 INFO 140052404180800] #quality_metric: host=algo-1, epoch=391, batch=0 train loss <loss>=2.94601416588\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:27 INFO 140052404180800] Epoch[391] Batch[5] avg_epoch_loss=2.971817\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=391, batch=5 train loss <loss>=2.9718165795\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:27 INFO 140052404180800] Epoch[391] Batch [5]#011Speed: 318.58 samples/sec#011loss=2.971817\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:27 INFO 140052404180800] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2239.7899627685547, \"sum\": 2239.7899627685547, \"min\": 2239.7899627685547}}, \"EndTime\": 1592850687.8988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850685.658562}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:27 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=266.530111256 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:27 INFO 140052404180800] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:27 INFO 140052404180800] #quality_metric: host=algo-1, epoch=391, train loss <loss>=2.91153929234\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:27 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:28 INFO 140052404180800] Epoch[392] Batch[0] avg_epoch_loss=2.984711\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:28 INFO 140052404180800] #quality_metric: host=algo-1, epoch=392, batch=0 train loss <loss>=2.9847111702\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:29 INFO 140052404180800] Epoch[392] Batch[5] avg_epoch_loss=2.990370\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:29 INFO 140052404180800] #quality_metric: host=algo-1, epoch=392, batch=5 train loss <loss>=2.99036987623\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:29 INFO 140052404180800] Epoch[392] Batch [5]#011Speed: 336.76 samples/sec#011loss=2.990370\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] Epoch[392] Batch[10] avg_epoch_loss=3.038545\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=392, batch=10 train loss <loss>=3.09635572433\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] Epoch[392] Batch [10]#011Speed: 333.86 samples/sec#011loss=3.096356\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2365.039110183716, \"sum\": 2365.039110183716, \"min\": 2365.039110183716}}, \"EndTime\": 1592850690.264414, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850687.898867}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.474511478 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=392, train loss <loss>=3.03854526173\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] Epoch[393] Batch[0] avg_epoch_loss=3.287737\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:30 INFO 140052404180800] #quality_metric: host=algo-1, epoch=393, batch=0 train loss <loss>=3.2877368927\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:31 INFO 140052404180800] Epoch[393] Batch[5] avg_epoch_loss=3.054804\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:31 INFO 140052404180800] #quality_metric: host=algo-1, epoch=393, batch=5 train loss <loss>=3.05480444431\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:31 INFO 140052404180800] Epoch[393] Batch [5]#011Speed: 338.99 samples/sec#011loss=3.054804\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:32 INFO 140052404180800] Epoch[393] Batch[10] avg_epoch_loss=3.010975\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=393, batch=10 train loss <loss>=2.95837903023\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:32 INFO 140052404180800] Epoch[393] Batch [10]#011Speed: 339.60 samples/sec#011loss=2.958379\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:32 INFO 140052404180800] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2357.4719429016113, \"sum\": 2357.4719429016113, \"min\": 2357.4719429016113}}, \"EndTime\": 1592850692.62242, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850690.264491}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:32 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.461986779 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:32 INFO 140052404180800] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:32 INFO 140052404180800] #quality_metric: host=algo-1, epoch=393, train loss <loss>=3.01097471064\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:32 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:33 INFO 140052404180800] Epoch[394] Batch[0] avg_epoch_loss=3.038419\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:33 INFO 140052404180800] #quality_metric: host=algo-1, epoch=394, batch=0 train loss <loss>=3.03841900826\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:34 INFO 140052404180800] Epoch[394] Batch[5] avg_epoch_loss=2.980617\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:34 INFO 140052404180800] #quality_metric: host=algo-1, epoch=394, batch=5 train loss <loss>=2.98061708609\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:34 INFO 140052404180800] Epoch[394] Batch [5]#011Speed: 338.04 samples/sec#011loss=2.980617\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] Epoch[394] Batch[10] avg_epoch_loss=2.891236\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=394, batch=10 train loss <loss>=2.78397789001\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] Epoch[394] Batch [10]#011Speed: 331.01 samples/sec#011loss=2.783978\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2393.1450843811035, \"sum\": 2393.1450843811035, \"min\": 2393.1450843811035}}, \"EndTime\": 1592850695.016103, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850692.622496}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.940402178 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=394, train loss <loss>=2.89123563333\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/state_9fc3b62d-5af5-40a5-aba9-a359ebbd5cca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 62.10803985595703, \"sum\": 62.10803985595703, \"min\": 62.10803985595703}}, \"EndTime\": 1592850695.078745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850695.016173}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] Epoch[395] Batch[0] avg_epoch_loss=3.022777\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:35 INFO 140052404180800] #quality_metric: host=algo-1, epoch=395, batch=0 train loss <loss>=3.02277731895\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:36 INFO 140052404180800] Epoch[395] Batch[5] avg_epoch_loss=2.969063\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:36 INFO 140052404180800] #quality_metric: host=algo-1, epoch=395, batch=5 train loss <loss>=2.96906312307\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:36 INFO 140052404180800] Epoch[395] Batch [5]#011Speed: 336.71 samples/sec#011loss=2.969063\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] Epoch[395] Batch[10] avg_epoch_loss=2.920674\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=395, batch=10 train loss <loss>=2.86260662079\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] Epoch[395] Batch [10]#011Speed: 333.87 samples/sec#011loss=2.862607\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2359.1458797454834, \"sum\": 2359.1458797454834, \"min\": 2359.1458797454834}}, \"EndTime\": 1592850697.438013, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850695.078813}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=272.543434093 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=395, train loss <loss>=2.92067380385\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] Epoch[396] Batch[0] avg_epoch_loss=2.889339\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:37 INFO 140052404180800] #quality_metric: host=algo-1, epoch=396, batch=0 train loss <loss>=2.88933897018\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:38 INFO 140052404180800] Epoch[396] Batch[5] avg_epoch_loss=3.025996\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:38 INFO 140052404180800] #quality_metric: host=algo-1, epoch=396, batch=5 train loss <loss>=3.02599585056\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:38 INFO 140052404180800] Epoch[396] Batch [5]#011Speed: 338.26 samples/sec#011loss=3.025996\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:39 INFO 140052404180800] Epoch[396] Batch[10] avg_epoch_loss=3.046285\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=396, batch=10 train loss <loss>=3.0706325531\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:39 INFO 140052404180800] Epoch[396] Batch [10]#011Speed: 336.97 samples/sec#011loss=3.070633\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:39 INFO 140052404180800] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2401.8449783325195, \"sum\": 2401.8449783325195, \"min\": 2401.8449783325195}}, \"EndTime\": 1592850699.840354, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850697.438089}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:39 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=274.776048202 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:39 INFO 140052404180800] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:39 INFO 140052404180800] #quality_metric: host=algo-1, epoch=396, train loss <loss>=3.04628526081\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:39 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:40 INFO 140052404180800] Epoch[397] Batch[0] avg_epoch_loss=3.230809\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:40 INFO 140052404180800] #quality_metric: host=algo-1, epoch=397, batch=0 train loss <loss>=3.23080921173\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:41 INFO 140052404180800] Epoch[397] Batch[5] avg_epoch_loss=3.120834\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:41 INFO 140052404180800] #quality_metric: host=algo-1, epoch=397, batch=5 train loss <loss>=3.12083375454\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:41 INFO 140052404180800] Epoch[397] Batch [5]#011Speed: 335.10 samples/sec#011loss=3.120834\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] Epoch[397] Batch[10] avg_epoch_loss=3.069792\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=397, batch=10 train loss <loss>=3.00854249001\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] Epoch[397] Batch [10]#011Speed: 326.83 samples/sec#011loss=3.008542\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2446.295976638794, \"sum\": 2446.295976638794, \"min\": 2446.295976638794}}, \"EndTime\": 1592850702.287158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850699.84043}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=279.593370111 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=397, train loss <loss>=3.06979227066\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] Epoch[398] Batch[0] avg_epoch_loss=3.137004\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:42 INFO 140052404180800] #quality_metric: host=algo-1, epoch=398, batch=0 train loss <loss>=3.13700437546\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:43 INFO 140052404180800] Epoch[398] Batch[5] avg_epoch_loss=3.041643\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:43 INFO 140052404180800] #quality_metric: host=algo-1, epoch=398, batch=5 train loss <loss>=3.04164342086\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:43 INFO 140052404180800] Epoch[398] Batch [5]#011Speed: 327.35 samples/sec#011loss=3.041643\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:44 INFO 140052404180800] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2201.0140419006348, \"sum\": 2201.0140419006348, \"min\": 2201.0140419006348}}, \"EndTime\": 1592850704.488672, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850702.287236}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:44 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=289.85047403 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:44 INFO 140052404180800] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=398, train loss <loss>=2.99709248543\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:44 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:44 INFO 140052404180800] Epoch[399] Batch[0] avg_epoch_loss=2.919950\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:44 INFO 140052404180800] #quality_metric: host=algo-1, epoch=399, batch=0 train loss <loss>=2.91994953156\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:45 INFO 140052404180800] Epoch[399] Batch[5] avg_epoch_loss=2.970300\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:45 INFO 140052404180800] #quality_metric: host=algo-1, epoch=399, batch=5 train loss <loss>=2.97030027707\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:45 INFO 140052404180800] Epoch[399] Batch [5]#011Speed: 339.43 samples/sec#011loss=2.970300\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] Epoch[399] Batch[10] avg_epoch_loss=2.953550\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=399, batch=10 train loss <loss>=2.93344883919\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] Epoch[399] Batch [10]#011Speed: 340.43 samples/sec#011loss=2.933449\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2350.486993789673, \"sum\": 2350.486993789673, \"min\": 2350.486993789673}}, \"EndTime\": 1592850706.839692, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850704.488755}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] #throughput_metric: host=algo-1, train throughput=285.459031227 records/second\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] #quality_metric: host=algo-1, epoch=399, train loss <loss>=2.95354962349\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] Final loss: 2.89123563333 (occurred at epoch 394)\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] #quality_metric: host=algo-1, train final_loss <loss>=2.89123563333\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 WARNING 140052404180800] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:46 INFO 140052404180800] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 838.770866394043, \"sum\": 838.770866394043, \"min\": 838.770866394043}}, \"EndTime\": 1592850707.679369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850706.839769}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:47 INFO 140052404180800] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1121.5529441833496, \"sum\": 1121.5529441833496, \"min\": 1121.5529441833496}}, \"EndTime\": 1592850707.962113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850707.679451}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:47 INFO 140052404180800] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:48 INFO 140052404180800] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 59.31901931762695, \"sum\": 59.31901931762695, \"min\": 59.31901931762695}}, \"EndTime\": 1592850708.021543, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850707.96218}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:48 INFO 140052404180800] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:48 INFO 140052404180800] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.04100799560546875, \"sum\": 0.04100799560546875, \"min\": 0.04100799560546875}}, \"EndTime\": 1592850708.022338, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850708.021599}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 3254.657983779907, \"sum\": 3254.657983779907, \"min\": 3254.657983779907}}, \"EndTime\": 1592850711.276952, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850708.022383}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, RMSE): 28.5498869537\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, mean_absolute_QuantileLoss): 23515.13667508496\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, mean_wQuantileLoss): 0.21420652089580858\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.1]): 0.16785776809350328\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.2]): 0.22296279923958318\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.3]): 0.2526127868922206\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.4]): 0.26603482736342515\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.5]): 0.2647427280104721\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.6]): 0.24869370820235429\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.7]): 0.218410535062383\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.8]): 0.1748379678178559\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #test_score (algo-1, wQuantileLoss[0.9]): 0.11170556738047956\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.214206520896\u001b[0m\n",
      "\u001b[34m[06/22/2020 18:31:51 INFO 140052404180800] #quality_metric: host=algo-1, test RMSE <loss>=28.5498869537\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 927699.159860611, \"sum\": 927699.159860611, \"min\": 927699.159860611}, \"setuptime\": {\"count\": 1, \"max\": 9.132862091064453, \"sum\": 9.132862091064453, \"min\": 9.132862091064453}}, \"EndTime\": 1592850711.340572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1592850711.277018}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-22 18:32:05 Uploading - Uploading generated training model\n",
      "2020-06-22 18:32:05 Completed - Training job completed\n",
      "Training seconds: 1005\n",
      "Billable seconds: 1005\n",
      "CPU times: user 3.16 s, sys: 187 ms, total: 3.35 s\n",
      "Wall time: 18min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log). You can find the definition of these metrics from our documentation. You can use these to optimize the parameters and tune your model or use SageMaker's Automated Model Tuning service to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create endpoint and predictor\n",
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.\n",
    "\n",
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using pandas.Series objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + pd.Timedelta(freq)\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        #prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        prediction_index = pd.date_range(prediction_time, periods = prediction_length, freq=freq)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-02 00:00:00</th>\n",
       "      <td>73.644539</td>\n",
       "      <td>79.010139</td>\n",
       "      <td>76.500031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 02:00:00</th>\n",
       "      <td>8.842995</td>\n",
       "      <td>12.199843</td>\n",
       "      <td>10.509267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 04:00:00</th>\n",
       "      <td>7.194129</td>\n",
       "      <td>11.202325</td>\n",
       "      <td>9.169115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 06:00:00</th>\n",
       "      <td>49.431465</td>\n",
       "      <td>80.791367</td>\n",
       "      <td>66.002594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 08:00:00</th>\n",
       "      <td>57.559685</td>\n",
       "      <td>296.591370</td>\n",
       "      <td>176.730148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0.1         0.9         0.5\n",
       "2019-07-02 00:00:00  73.644539   79.010139   76.500031\n",
       "2019-07-02 02:00:00   8.842995   12.199843   10.509267\n",
       "2019-07-02 04:00:00   7.194129   11.202325    9.169115\n",
       "2019-07-02 06:00:00  49.431465   80.791367   66.002594\n",
       "2019-07-02 08:00:00  57.559685  296.591370  176.730148"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries[1], quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a plotting function that queries the model and displays the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    #target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section = target_ts[forecast_date-pd.Timedelta(hours=plot_history):forecast_date+pd.Timedelta(hours=prediction_length)]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                #index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                index = pd.date_range(target_ts.index[0], periods = len(f), freq=target_ts.index.freq),\n",
    "                data=f\n",
    "            )\n",
    "            #feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')\n",
    "            feat_ts[forecast_date-pd.Timedelta(hours=plot_history):forecast_date+pd.Timedelta(hours=prediction_length)].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interact with the function previously defined, to look at the forecast of any customer at any point in (future) time. \n",
    "\n",
    "For each request, the predictions are obtained by calling our served model on the fly.\n",
    "\n",
    "Here we forecast the consumption of an office after week-end (note the lower week-end consumption). \n",
    "You can select any time series and any forecast date, just click on `Run Interact` to generate the predictions from our served endpoint and see the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a7369bff7149999e9fbfe4402580f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='vendor_id', max=1, style=SliderStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(\n",
    "    vendor_id=IntSlider(min=0, max=1, value=1, style=style), \n",
    "    forecast_day=IntSlider(min=0, max=100, value=51, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=1, max=20, value=1, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(vendor_id, forecast_day, confidence, history_weeks_plot, show_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=timeseries[vendor_id],\n",
    "        forecast_date=end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot * 12 * 7,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
